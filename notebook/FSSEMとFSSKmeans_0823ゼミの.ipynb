{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_10528\\1091100056.py\", line 10, in <module>\n",
      "    from lightning.pytorch import seed_everything\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\__init__.py\", line 18, in <module>\n",
      "    from lightning.fabric.fabric import Fabric  # noqa: E402\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\fabric\\__init__.py\", line 30, in <module>\n",
      "    from lightning.fabric.fabric import Fabric  # noqa: E402\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\fabric\\fabric.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "Seed set to 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.validation import check_array, check_X_y\n",
    "\n",
    "import japanize_matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # データ可視化ライブラリ\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "seed_everything(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(data:pd.DataFrame, metric: str, threshold: float =2) -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\NHANES_age_prediction.csv\"\n",
    ")\n",
    "df1 = df1.drop(columns=[\"SEQN\", \"age_group\"])\n",
    "\n",
    "obj1 = \"BMXBMI\"\n",
    "features_list1 = [\n",
    "    \"RIDAGEYR\",  # 年齢（連続変数）\n",
    "    \"RIAGENDR\",  # 性別（1:Male, 2:Female)\n",
    "    \"PAQ605\",  # 運動有無(1:日常的に運動する, 2:運動しない)\n",
    "    \"LBXGLU\",  # 断食後の血糖値（連続変数）\n",
    "    \"DIQ010\",  # 糖尿病の有無(0:なし、1:あり)\n",
    "    \"LBXGLT\",  # 口内の健康状態（連続変数）\n",
    "    \"LBXIN\",  # 血中インスリン濃度（連続変数）\n",
    "]\n",
    "df1 = df1[df1[\"PAQ605\"] != 7.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\OnlineNewsPopularity\\OnlinenewsPopularity.csv\"\n",
    "# )\n",
    "# df2 = df2.drop(columns=[\"url\"])\n",
    "# df2 = df2.drop(columns=[\" timedelta\"])\n",
    "\n",
    "# obj2 = \" shares\"\n",
    "# features_list2 = [col for col in list(df2.columns) if col != \" shares\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\USCensus1990.data.txt\",\n",
    "#     delimiter=\",\",\n",
    "# )\n",
    "\n",
    "# obj3 = \"iFertil\"\n",
    "# features_list3 = [col for col in list(df3.columns) if col != obj3]\n",
    "# features_list3_20 = features_list3[:20]\n",
    "# print(features_list3_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\aug_first_cpn_data_for_ab_test_sensibility_tsukuba.csv\"\n",
    "# )\n",
    "\n",
    "# obj4 = \"GMV\"\n",
    "# features_list4 = [\n",
    "#     \"hist_4_day_buy_num\",\n",
    "#     \"hist_4_day_gmv\",\n",
    "#     \"his_4_day_is_buy\",\n",
    "#     \"hist_30_day_buy_days\",\n",
    "#     \"hist_30_day_buy_num\",\n",
    "#     \"hist_30_day_gmv\",\n",
    "#     \"hist_30_day_buy_recency\",\n",
    "#     \"hist_30_day_pay_days\",\n",
    "#     \"hist_30_day_atpu\",\n",
    "#     \"hist_30_day_gpv\",\n",
    "#     \"hist_30_day_pay_recency\",\n",
    "#     \"hist_30_day_list_days\",\n",
    "#     \"hist_30_day_list_num\",\n",
    "#     \"hist_30_day_list_recency\",\n",
    "#     \"hist_30_day_like_count\",\n",
    "#     \"hist_30_day_like_count_not_deleted\",\n",
    "#     \"hist_30_day_like_recency\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1  # 選ぶ\n",
    "obj = obj1  # 選ぶ\n",
    "features_list = features_list1  # 選ぶ\n",
    "\n",
    "df = remove_outliers_zscore(df, obj)\n",
    "\n",
    "X = df[features_list]\n",
    "# 数値列の標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=features_list)\n",
    "\n",
    "y = df[obj]  # 目的変数\n",
    "\n",
    "# 行を詰める\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSSEM でクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapperクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "n_features_to_select = 7  # 選択したい特徴量の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features_to_select: int,\n",
    "        n_clusters: int,\n",
    "        criterion: str = \"ml\",\n",
    "        clustering_method: str = \"em\",\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.n_features_to_select = n_features_to_select  # 特徴量数\n",
    "        self.n_clusters = n_clusters  # クラスタ数\n",
    "        self.criterion = criterion  # 特徴量選択基準\n",
    "        self.clustering_method = clustering_method  # クラスタリング手法\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def FSS(self, X: pd.DataFrame, y: pd.DataFrame) -> \"Wrapper\":\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        n_features = X.shape[1]  # 総特徴量数\n",
    "        self.selected_features_ = []  # ここに選択した特徴量を入れる\n",
    "\n",
    "        # 選ばれた特徴量と残っている特徴量の初期化\n",
    "        current_features = []\n",
    "        remaining_features = list(range(n_features))\n",
    "        best_score = -np.inf\n",
    "\n",
    "        while (\n",
    "            len(current_features) < self.n_features_to_select\n",
    "        ):  # これだと特徴量が５個選ばれるまで続く\n",
    "            # print(current_features)\n",
    "            # best_score = -np.inf  # best score初期化（-∞）　#上に書く\n",
    "            best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "            for feature in remaining_features:\n",
    "                temp_features = tuple(\n",
    "                    current_features + [feature]\n",
    "                )  # 特徴量をひとつ加え、score計算\n",
    "\n",
    "                score = self.CRIT(X[:, temp_features])\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "\n",
    "            if best_feature is not None:\n",
    "                current_features.append(\n",
    "                    best_feature\n",
    "                )  # best feature をcurrent features に追加\n",
    "                remaining_features.remove(\n",
    "                    best_feature\n",
    "                )  # best feature をremaining features から取り除く\n",
    "                self.selected_features_ = current_features\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # 選ばれた特徴量サブセットでクラスタリング\n",
    "        final_features = X[:, self.selected_features_]\n",
    "        if self.clustering_method == \"em\":\n",
    "            self.final_model_ = GaussianMixture(\n",
    "                n_components=self.n_clusters, random_state=self.random_state\n",
    "            )\n",
    "        elif self.clustering_method == \"kmeans\":\n",
    "            self.final_model_ = KMeans(\n",
    "                n_clusters=self.n_clusters, random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
    "\n",
    "        self.final_model_.fit(final_features)\n",
    "        self.final_cluster_assignments_ = self.final_model_.predict(final_features)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def CRIT(self, X: pd.DataFrame) -> float:\n",
    "        if self.clustering_method == \"em\":\n",
    "            em = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                # init_params=\"kmeans\",\n",
    "            )\n",
    "            em.fit(X)\n",
    "            labels = em.predict(X)\n",
    "\n",
    "            if self.criterion == \"tr\":\n",
    "                means = em.means_  # 平均ベクトル\n",
    "                covariances = em.covariances_  # 共分散行列\n",
    "                weights = em.weights_  # 混合比率\n",
    "                overall_mean = np.sum(\n",
    "                    weights[:, np.newaxis] * means, axis=0\n",
    "                )  # 標本平均 #np.newaxisを使って1次元配列から2次元配列にする\n",
    "\n",
    "                S_W = np.sum(weights[:, np.newaxis, np.newaxis] * covariances, axis=0)\n",
    "                S_B = np.sum(\n",
    "                    weights[:, np.newaxis, np.newaxis]\n",
    "                    * np.einsum(\n",
    "                        \"...i,...j->...ij\", means - overall_mean, means - overall_mean\n",
    "                    ),\n",
    "                    axis=0,\n",
    "                )\n",
    "                score = np.trace(np.linalg.solve(S_W, S_B))\n",
    "\n",
    "            elif self.criterion == \"ml\":\n",
    "                score = em.score(X)\n",
    "\n",
    "        if self.clustering_method == \"kmeans\":\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "            kmeans.fit(X)\n",
    "            labels = kmeans.predict(X)\n",
    "            if self.criterion == \"tr\":\n",
    "                labels = kmeans.labels_\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "                sw_i_list = []\n",
    "                for i in range(self.n_clusters):\n",
    "                    cluster_points = X[labels == i]\n",
    "\n",
    "                    if cluster_points.shape[0] <= 2:\n",
    "                        # データポイントが1つの場合はゼロ行列を使用\n",
    "                        sw_i = np.zeros((X.shape[1], X.shape[1])) + 1e-7\n",
    "                    else:\n",
    "                        # 共分散行列を計算し、スカラー値ではなく2次元行列になることを保証\n",
    "                        sw_i = np.cov(cluster_points, rowvar=False) * np.sum(\n",
    "                            labels\n",
    "                            == i  # データ数を重みに使う代わりにデータの割合を使う\n",
    "                        )\n",
    "                        if np.isscalar(sw_i):  # スカラー値のとき\n",
    "                            sw_i = np.array([[sw_i]])\n",
    "                    sw_i_list.append(sw_i)\n",
    "\n",
    "                # 全クラスターの S_W を合計\n",
    "                S_W = np.sum(sw_i_list, axis=0)\n",
    "\n",
    "                # クラスター間散布行列 S_B を計算\n",
    "                overall_mean = np.mean(X, axis=0)\n",
    "                S_B = sum(\n",
    "                    np.sum(labels == i)  # 割合にする\n",
    "                    * np.outer(\n",
    "                        cluster_centers[i] - overall_mean,\n",
    "                        cluster_centers[i] - overall_mean,\n",
    "                    )\n",
    "                    # *(cluster_centers[i] - overall_mean) @ (cluster_centers[i] - overall_mean).T\n",
    "                    for i in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "                # 散乱分離性を計算\n",
    "                score = np.trace(np.linalg.solve(S_W, S_B))\n",
    "\n",
    "            elif self.criterion == \"ml\":\n",
    "                score = -kmeans.score(X)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_feature_index_out(self) -> NDArray:\n",
    "        return np.array(self.selected_features_)  # 選択された特徴量のインデックス\n",
    "\n",
    "    def get_final_cluster_assignments(self) -> NDArray:\n",
    "        return self.final_cluster_assignments_  # 最終的なクラスタリング結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapperクラス確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "clusters = 5\n",
    "n_features_to_select = 5  # 選択したい特徴量の数\n",
    "\n",
    "fssem_tr = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"tr\",\n",
    "    clustering_method=\"em\",\n",
    "    random_state=0,\n",
    ")\n",
    "fssem_ml = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"ml\",\n",
    "    clustering_method=\"em\",\n",
    "    random_state=0,\n",
    ")\n",
    "fsskmeans_tr = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"tr\",\n",
    "    clustering_method=\"kmeans\",\n",
    "    random_state=0,\n",
    ")\n",
    "fsskmeans_ml = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"ml\",\n",
    "    clustering_method=\"kmeans\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "instance_dict = {\n",
    "    \"fssem_tr\": fssem_tr,\n",
    "    \"fssem_ml\": fssem_ml,\n",
    "    \"fsskmeans_tr\": fsskmeans_tr,\n",
    "    \"fsskmeans_ml\": fsskmeans_ml,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1]\n",
      "[4 2 1]\n",
      "[4 2]\n",
      "[3 6 0 1 4]\n"
     ]
    }
   ],
   "source": [
    "selected_features_index_dict = {}\n",
    "cluster_label_dict = {}\n",
    "cluster_size_dict = {}\n",
    "for name, instance in instance_dict.items():\n",
    "    instance.FSS(X_scaled, y)\n",
    "    selected_features_index = instance.get_feature_index_out()\n",
    "    selected_features_index_dict[name] = selected_features_index\n",
    "    cluster_label = instance.get_final_cluster_assignments()\n",
    "    cluster_label_dict[name] = cluster_label\n",
    "    cluster_size = np.unique(cluster_label, return_counts=True)[1]\n",
    "    cluster_size_dict[name] = cluster_size\n",
    "\n",
    "    print(selected_features_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 層化抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "    # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        random_state: int,\n",
    "        criterion: str,\n",
    "        clustering_method: str,\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.H = H\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.clustering_method = clustering_method\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"標本配分を解く\n",
    "\n",
    "        Args:\n",
    "            X (NDArray): データ (N x M)\n",
    "            y (NDArray): 目的変数 (N)\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "        Note:\n",
    "            M: 特徴量数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"em\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"em\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_ml\"]\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_ml\"]\n",
    "        # インスタンス変数として設定\n",
    "        print(cluster_size)\n",
    "        self.cluster_label = cluster_label\n",
    "        self.N = cluster_size\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAllocation(BaseAllocation):\n",
    "    # 抽象メゾッドを具象化\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        # cluster_labelのすべての要素は0（すべてのデータを同じクラスタに属させている）\n",
    "        cluster_label = np.zeros(\n",
    "            X.shape[0]\n",
    "        )  # cluster_label = [0,0,0,,...(要素数：データ数）]\n",
    "        # クラスタサイズ＝データ数\n",
    "        cluster_size = np.array([len(cluster_label)])  # cluster_size=[データ数]\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalAllocation(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "        n: NDArray = np.round(self.N / self.N.sum() * self.n_samples).astype(int)\n",
    "\n",
    "        if n.sum() > self.n_samples:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n[np.argmax(n)] -= n.sum() - self.n_samples\n",
    "\n",
    "        for i in range(len(n)):  # nの要素で2より小さいものがあれば2にする\n",
    "            if n[i] == 0:\n",
    "                n[i] = 2\n",
    "                n[np.argmax(n)] -= 2\n",
    "            if n[i] == 1:\n",
    "                n[i] = 2\n",
    "                n[np.argmax(n)] -= 1\n",
    "\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostStratification(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAllocation(BaseAllocation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        m: NDArray,  # 標本サイズ下限\n",
    "        M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "        random_state: int = 0,\n",
    "        criterion: str = \"ml\",\n",
    "        clustering_method: str=\"kmeans\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_samples, H, random_state, criterion, clustering_method\n",
    "        )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "        self.m = m  # 各クラスタの最小標本サイズ (H, )\n",
    "        self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array([np.var(y[self.cluster_label == h]) for h in range(self.H)])\n",
    "        d = (self.N**2) * S  # (H, )\n",
    "        n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "        # 制約チェック\n",
    "        self._check_constraints(n)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "        M = self.M.copy() if self.M is not None else self.N.copy()\n",
    "        I = np.arange(self.H)  # noqa #クラスタのインデックス\n",
    "        while (n.sum() != self.n_samples) and len(I) != 0:\n",
    "            delta = np.zeros(self.H)\n",
    "            delta[I] = (d / (n + 1) - d / n)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n[h_star] + 1 <= M[h_star]:\n",
    "                n[h_star] = n[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _check_constraints(self, n: NDArray):\n",
    "        assert (\n",
    "            n.sum() <= self.n_samples\n",
    "        ), f\"Total sample size is over than {self.n_samples}\"\n",
    "        assert np.all(n >= self.m), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_y_mean(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    \"\"\"実際にサンプリングを行って目的変数の平均を推定\n",
    "\n",
    "    Args:\n",
    "        n (NDArray): 各クラスタの標本数 (H, )\n",
    "        cluster_label (NDArray): クラスタラベル (N, )\n",
    "        y (NDArray): 目的変数 (N, )\n",
    "\n",
    "    Returns:\n",
    "        NDArray: 推定された目的変数の平均\n",
    "\n",
    "    Note:\n",
    "        N: データ数\n",
    "        H: クラスタ数\n",
    "    \"\"\"\n",
    "    # cluster_labelからユニークなクラスタラベルを取得し、母集団の各クラスタのサイズNを取得\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]  # クラスタサイズ (H, )\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    for h in range(n.shape[0]):  # n.shape[0]:層の数\n",
    "        y_cluster = y[cluster_label == h]\n",
    "        # クラスタ内でランダム n_h サンプリング\n",
    "        sample: NDArray = np.random.choice(y_cluster, n[h], replace=False)\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_post(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    indices = np.arange(N.sum())\n",
    "    y_array = np.array(y.tolist())\n",
    "    n_indices = np.random.choice(indices, n[0], replace=False)\n",
    "    n_label = np.array([cluster_label[i] for i in n_indices])\n",
    "    n_new = np.unique(n_label)\n",
    "    for h in n_new:\n",
    "        index = np.where(n_label == h)[0]\n",
    "        sample = y_array[n_indices[index]]\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[976 560 160 463  10]\n",
      "[976 560 160 463  10]\n",
      "[976 560 160 463  10]\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 1000  # 標本サイズ\n",
    "H = clusters  # クラスタ数が多すぎるとpropotionalがうまくいかない\n",
    "N_TRIALS = 1000  # 試行回数\n",
    "m_VALUE = 2  # 各クラスタの最小標本数\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "CRITERION = \"ml\"\n",
    "CLUSTERING_METHOD = \"kmeans\"\n",
    "\n",
    "# 戦略を定義\n",
    "policies: list[BaseAllocation] = [\n",
    "    RandomAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    ProportionalAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    PostStratification(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    OptimalAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        m=np.full(H, m_VALUE),\n",
    "        M=None,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# それぞれの戦略で各クラスタの標本数を求解\n",
    "allocations: list[dict] = []  # 各戦略の実行結果が辞書形式で追加される\n",
    "for policy in policies:\n",
    "    # policyを用いてXをクラスタリング\n",
    "    cluster_label, _ = policy.clustering(X_scaled)\n",
    "    n = policy.solve(X_scaled, y)\n",
    "    allocations.append(\n",
    "        {\n",
    "            \"policy\": policy.__class__.__name__,\n",
    "            \"n\": n,\n",
    "            \"cluster_label\": cluster_label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 各戦略の標本数に基づいて目的変数の平均を推定\n",
    "y_hats = []\n",
    "for random_state in range(N_TRIALS):\n",
    "    for allocation in allocations:\n",
    "        if allocation[\"policy\"] == \"PostStratification\":\n",
    "            y_hat = estimate_y_mean_post(\n",
    "                allocation[\"n\"], allocation[\"cluster_label\"], y\n",
    "            )\n",
    "        else:\n",
    "            y_hat = estimate_y_mean(allocation[\"n\"], allocation[\"cluster_label\"], y)\n",
    "        y_hats.append(\n",
    "            {\n",
    "                \"policy\": allocation[\"policy\"],\n",
    "                \"y_hat\": y_hat,\n",
    "                \"random_state\": random_state,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[criteion : ml , clustering_method: kmeans のときの各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation                NaN\n",
      "ProportionalAllocation    -3.551617\n",
      "PostStratification         7.815010\n",
      "OptimalAllocation         15.271639\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_hat_df = pd.DataFrame(y_hats)\n",
    "y_hat_df[\"error\"] = (\n",
    "    y_hat_df[\"y_hat\"] - y.mean()\n",
    ")  # 真の平均からの誤差をerrorカラムに追加\n",
    "\n",
    "# random_allocationの誤差分散\n",
    "random_allocation_std = y_hat_df[y_hat_df[\"policy\"] == \"RandomAllocation\"][\n",
    "    \"error\"\n",
    "].var()\n",
    "# random_allocation以外の誤差分散\n",
    "non_random_allocation_std = (\n",
    "    y_hat_df[y_hat_df[\"policy\"] != \"RandomAllocation\"].groupby(\"policy\")[\"error\"].var()\n",
    ")\n",
    "\n",
    "# 削減率\n",
    "reduction_rate = (1 - non_random_allocation_std / random_allocation_std) * 100\n",
    "\n",
    "## policyの順番をpoliciesの順番に調整\n",
    "reduction_rate = reduction_rate.reindex(\n",
    "    [policy.__class__.__name__ for policy in policies]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[criteion :\",\n",
    "    CRITERION,\n",
    "    \", clustering_method:\",\n",
    "    CLUSTERING_METHOD,\n",
    "    \"のときの各手法の誤差分散削減率]\",\n",
    ")\n",
    "print(reduction_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[criterion: ml , clustering_method: kmeans  のときの各手法のvar]\n",
      "RandomAllocation 0.015422291865375368\n",
      "ProportionalAllocation 0.01597003253677609\n",
      "PostStratification 0.014217038153814181\n",
      "OptimalAllocation 0.013067055122034868\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"[criterion:\",\n",
    "    CRITERION,\n",
    "    \", clustering_method:\",\n",
    "    CLUSTERING_METHOD,\n",
    "    \" のときの各手法のvar]\",\n",
    ")\n",
    "\n",
    "policy_name_list = [policy.__class__.__name__ for policy in policies]\n",
    "for i in range(len(policy_name_list)):\n",
    "    var = y_hat_df[y_hat_df[\"policy\"] == policy_name_list[i]][\"error\"].var()\n",
    "    print(policy_name_list[i], var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
