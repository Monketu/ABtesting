{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.validation import check_array, check_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RIDAGEYR  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN\n",
      "0         61.0       2.0     2.0    35.7   110.0     2.0   150.0  14.91\n",
      "1         26.0       2.0     2.0    20.3    89.0     2.0    80.0   3.85\n",
      "2         16.0       1.0     2.0    23.2    89.0     2.0    68.0   6.14\n",
      "3         32.0       1.0     2.0    28.9   104.0     2.0    84.0  16.15\n",
      "4         38.0       2.0     1.0    35.9   103.0     2.0    81.0  10.92\n",
      "...        ...       ...     ...     ...     ...     ...     ...    ...\n",
      "2165      38.0       2.0     2.0    33.5   100.0     2.0    73.0   6.53\n",
      "2166      61.0       1.0     2.0    30.0    93.0     2.0   208.0  13.02\n",
      "2167      34.0       1.0     2.0    23.7   103.0     2.0   124.0  21.41\n",
      "2168      60.0       2.0     2.0    27.4    90.0     2.0   108.0   4.99\n",
      "2169      26.0       1.0     2.0    24.5   108.0     2.0   108.0   3.76\n",
      "\n",
      "[2170 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\NHANES_age_prediction.csv\"\n",
    ")\n",
    "df = df.drop(columns=[\"SEQN\", \"age_group\"])\n",
    "\n",
    "obj = \"BMXBMI\"\n",
    "features_list = [\n",
    "    \"RIDAGEYR\",  # 年齢（連続変数）\n",
    "    \"RIAGENDR\",  # 性別（1:Male, 2:Female)\n",
    "    \"PAQ605\",  # 運動有無(1:日常的に運動する, 2:運動しない)\n",
    "    \"LBXGLU\",  # 断食後の血糖値（連続変数）\n",
    "    \"DIQ010\",  # 糖尿病の有無(0:なし、1:あり)\n",
    "    \"LBXGLT\",  # 口内の健康状態（連続変数）\n",
    "    \"LBXIN\",  # 血中インスリン濃度（連続変数）\n",
    "]\n",
    "\n",
    "\n",
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(data, metric, threshold=2):\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data\n",
    "\n",
    "\n",
    "df = remove_outliers_zscore(df, obj)\n",
    "\n",
    "# process_features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X = df[features_list]\n",
    "# 数値列の標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=features_list)\n",
    "y = df[obj]  # 目的変数\n",
    "\n",
    "# 行を詰める\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features_to_select):\n",
    "        self.n_features_to_select = n_features_to_select  # 選択する特徴量の数\n",
    "\n",
    "    def FSSEM(self, X, y):\n",
    "        X, y = check_X_y(X, y)  # データの整合性確認\n",
    "\n",
    "        self.n_features_ = X.shape[1]  # 特徴量の総数\n",
    "        self.selected_features_ = []  # 特徴量サブセット初期化\n",
    "\n",
    "        # 現在選択されている特徴量のインデックスを格納する用\n",
    "        current_features = []\n",
    "        # 選択されていない特徴量のインデックスを格納する用\n",
    "        remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "        # SFS\n",
    "        while len(current_features) < self.n_features_to_select:\n",
    "            best_score = -np.inf  # 最良スコアの初期化（マイナス無限大）\n",
    "            best_feature = None\n",
    "\n",
    "            # 追加すべき特徴量を1つ決める\n",
    "            for feature in remaining_features:\n",
    "                temp_features = current_features + [\n",
    "                    feature\n",
    "                ]  # 現在の特徴量リストに新しい特徴量を加えた仮のリストを作成\n",
    "                print(\"X:\", X[:, temp_features])\n",
    "                score = self.evaluate_subset(\n",
    "                    X[:, temp_features], y\n",
    "                )  # 仮のリストに基づいて評価\n",
    "                print(score)\n",
    "                if score > best_score:  # 評価スコアが最良スコアよりも大きいときは更新\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "\n",
    "            # current_featuresとremaining_featuresの更新\n",
    "            if best_feature is not None:\n",
    "                current_features.append(best_feature)\n",
    "                remaining_features.remove(best_feature)\n",
    "                self.selected_features_ = current_features\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_array(X)  # Xを検証\n",
    "        return X[:, self.selected_features_]  # 選択された特徴量だけを抽出して返す\n",
    "\n",
    "    def evaluate_subset(self, X_subset, y):  # EMクラスタリング\n",
    "        n_clusters = 3\n",
    "\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_clusters, random_state=42\n",
    "        )  # n_componensはクラスタ数\n",
    "        gmm.fit(X_subset)  # ガウスモデルをX_subsetデータで訓練\n",
    "        labels = gmm.predict(X_subset)  # 各データが属するクラスタのインデックス\n",
    "        print(\"labels:\", labels)\n",
    "        responsibilities = gmm.predict_proba(\n",
    "            X_subset\n",
    "        )  # (d, k)各データが各クラスタに属する確率\n",
    "        print(\"respon:\", responsibilities)\n",
    "        # 各データポイントに対するクラスタ確率の平均を計算\n",
    "\n",
    "        # 各データポイントに対するクラスタ確率の平均を計算\n",
    "        mean_responsibilities = responsibilities.mean(axis=0)\n",
    "        print(\"mean:\", mean_responsibilities)\n",
    "\n",
    "        covariances = gmm.covariances_  # covariances[j]はクラスタjに対応する共分散行列\n",
    "        print(\"covariances\", covariances)\n",
    "        # クラスごとの平均を計算\n",
    "        class_means = np.array(\n",
    "            [X_subset[labels == label].mean(axis=0) for label in np.unique(labels)]\n",
    "        )\n",
    "        print(\"class_means\", class_means)\n",
    "        # 全体の平均を計算\n",
    "        overall_mean = X_subset.mean(axis=0)\n",
    "        print(\"overallmean\", overall_mean)\n",
    "\n",
    "        # 各クラスタの共分散行列の計算\n",
    "        S_W = np.sum(\n",
    "            [mean_responsibilities[j] * covariances[j] for j in range(n_clusters)],\n",
    "            axis=0,\n",
    "        )\n",
    "        print(\"Sw:\", S_W)\n",
    "\n",
    "        # クラスタ間散布行列 (S_B) を計算するためのベクトル化\n",
    "        # (X - class_means[j]) の計算を一度に行うために、np.newaxis を使用\n",
    "        deviations = [class_means[j] - overall_mean for j in range(n_clusters)]\n",
    "        responsibility_matrix = responsibilities[\n",
    "            :, :, np.newaxis\n",
    "        ]  # 各データポイントに対するクラスタ確率\n",
    "\n",
    "        # クラスタ間散布行列 (S_B) の計算\n",
    "        S_B = np.sum(\n",
    "            [\n",
    "                responsibility_matrix[:, j, np.newaxis]\n",
    "                * (deviations[j] @ deviations[j].T)\n",
    "                for j in range(n_clusters)\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        print(\"Sb:\", S_B)\n",
    "        # 散乱識別力\n",
    "        scatter_discriminability = np.trace(np.linalg.inv(S_W).dot(S_B))\n",
    "        return scatter_discriminability\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        # 選択された特徴量のインデックスを返す\n",
    "        return np.array(self.selected_features_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 0.94929614]\n",
      " [-0.77321245]\n",
      " [-1.26535776]\n",
      " ...\n",
      " [-0.3794962 ]\n",
      " [ 0.90008161]\n",
      " [-0.77321245]]\n",
      "labels: [1 2 0 ... 2 1 2]\n",
      "respon: [[1.79750335e-25 8.50095223e-01 1.49904777e-01]\n",
      " [2.75245447e-01 3.17439495e-04 7.24437113e-01]\n",
      " [9.47385203e-01 1.21630303e-06 5.26135811e-02]\n",
      " ...\n",
      " [4.21398360e-04 4.25482820e-03 9.95323773e-01]\n",
      " [2.50166461e-24 8.14983510e-01 1.85016490e-01]\n",
      " [2.75245447e-01 3.17439495e-04 7.24437113e-01]]\n",
      "mean: [0.25473293 0.30228542 0.44298165]\n",
      "covariances [[[0.03979333]]\n",
      "\n",
      " [[0.23394824]]\n",
      "\n",
      " [[0.26042752]]]\n",
      "class_means [[-1.20781462]\n",
      " [ 1.25019485]\n",
      " [-0.11197105]]\n",
      "overallmean [1.5144056e-16]\n",
      "Sw: [[0.19622043]]\n",
      "Sb: [[[1.33056735]]\n",
      "\n",
      " [[0.4111113 ]]\n",
      "\n",
      " [[1.38272237]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.01974387]]\n",
      "\n",
      " [[1.27612841]]\n",
      "\n",
      " [[0.4111113 ]]]\n",
      "[6.78098285]\n",
      "X: [[ 0.99907877]\n",
      " [ 0.99907877]\n",
      " [-1.00092208]\n",
      " ...\n",
      " [-1.00092208]\n",
      " [ 0.99907877]\n",
      " [-1.00092208]]\n",
      "labels: [1 1 0 ... 0 1 0]\n",
      "respon: [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "mean: [0.49953917 0.50046083 0.        ]\n",
      "covariances [[[1.e-06]]\n",
      "\n",
      " [[1.e-06]]\n",
      "\n",
      " [[1.e-06]]]\n",
      "class_means [[-1.00092208]\n",
      " [ 0.99907877]]\n",
      "overallmean [2.66862825e-16]\n",
      "Sw: [[1.e-06]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m fssem \u001b[38;5;241m=\u001b[39m Wrapper(n_features_to_select\u001b[38;5;241m=\u001b[39mn_features_to_select)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# FSSEM\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfssem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFSSEM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 選択された特徴量\u001b[39;00m\n\u001b[0;32m      7\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m fssem\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected features indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 33\u001b[0m, in \u001b[0;36mWrapper.FSSEM\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     29\u001b[0m temp_features \u001b[38;5;241m=\u001b[39m current_features \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m     30\u001b[0m     feature\n\u001b[0;32m     31\u001b[0m ]  \u001b[38;5;66;03m# 現在の特徴量リストに新しい特徴量を加えた仮のリストを作成\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X[:, temp_features])\n\u001b[1;32m---> 33\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 仮のリストに基づいて評価\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:  \u001b[38;5;66;03m# 評価スコアが最良スコアよりも大きいときは更新\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 94\u001b[0m, in \u001b[0;36mWrapper.evaluate_subset\u001b[1;34m(self, X_subset, y)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSw:\u001b[39m\u001b[38;5;124m\"\u001b[39m, S_W)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# クラスタ間散布行列 (S_B) を計算するためのベクトル化\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# (X - class_means[j]) の計算を一度に行うために、np.newaxis を使用\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m deviations \u001b[38;5;241m=\u001b[39m [\u001b[43mclass_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m overall_mean \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_clusters)]\n\u001b[0;32m     95\u001b[0m responsibility_matrix \u001b[38;5;241m=\u001b[39m responsibilities[\n\u001b[0;32m     96\u001b[0m     :, :, np\u001b[38;5;241m.\u001b[39mnewaxis\n\u001b[0;32m     97\u001b[0m ]  \u001b[38;5;66;03m# 各データポイントに対するクラスタ確率\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# クラスタ間散布行列 (S_B) の計算\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# 特徴量選択のためのインスタンスを作成\n",
    "n_features_to_select = 3  # 選択したい特徴量の数\n",
    "fssem = Wrapper(n_features_to_select=n_features_to_select)\n",
    "\n",
    "# FSSEM\n",
    "fssem.FSSEM(X_scaled, y)  # 選択された特徴量\n",
    "selected_features = fssem.get_feature_names_out()\n",
    "print(f\"Selected features indices: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
