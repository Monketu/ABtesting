{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量的データ用（For quantitaive data）\n",
    "[Methods]  \n",
    "- Clustering methods : Kmeans, GMM, FSSEM, FSS-Kmeans    \n",
    "- Allocation methods : random, proportional allocation, post stratification, optimal allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 3  # クラスタ数\n",
    "n_features_to_select = 17  # 選択される最大の特徴量数\n",
    "\n",
    "N_SAMPLES = 100  # 標本サイズ\n",
    "data_size = 3000 #データサイズ（df2)\n",
    "H = clusters\n",
    "N_TRIALS = 1000  # 試行回数\n",
    "m_VALUE = 2  # 各クラスタの最小標本数(最適標本配分)\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "ALLOCATION_LIST = [\"random\", \"proportional\", \"post\", \"optimal\"]\n",
    "CRITERION_LIST = [\"ml\", \"none\"]\n",
    "CLUSTERING_METHOD_LIST = [\"kmeans\"] #[\"gmm\", \"kmeans\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### ライブラリのインポート＆その他の設定（Importing Libraries & Other Settings）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 8\n"
     ]
    }
   ],
   "source": [
    "# 基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn関連\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "# 抽象基底クラス (ABC)\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# タイピングのサポート\n",
    "from typing import Optional\n",
    "\n",
    "# シード設定\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(8)\n",
    "\n",
    "# 可視化の設定\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### データの前処理（Data Preprocessing）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 外れ値を除去する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(\n",
    "    data: pd.DataFrame, metric: str, threshold: float = 2\n",
    ") -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### メルカリデータ（df1：全部, df2：一部）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1(all the data)\n",
    "df1 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\aug_first_cpn_data_for_ab_test_sensibility_tsukuba.csv\"\n",
    ")\n",
    "obj1 = \"GMV\"\n",
    "features_list1 = [\n",
    "    \"hist_4_day_buy_num\",\n",
    "    \"hist_4_day_gmv\",\n",
    "    \"his_4_day_is_buy\",\n",
    "    \"hist_30_day_buy_days\",\n",
    "    \"hist_30_day_buy_num\",\n",
    "    \"hist_30_day_gmv\",\n",
    "    \"hist_30_day_buy_recency\",\n",
    "    \"hist_30_day_pay_days\",\n",
    "    \"hist_30_day_atpu\",\n",
    "    \"hist_30_day_gpv\",\n",
    "    \"hist_30_day_pay_recency\",\n",
    "    \"hist_30_day_list_days\",\n",
    "    \"hist_30_day_list_num\",\n",
    "    \"hist_30_day_list_recency\",\n",
    "    \"hist_30_day_like_count\",\n",
    "    \"hist_30_day_like_count_not_deleted\",\n",
    "    \"hist_30_day_like_recency\",\n",
    "]\n",
    "\n",
    "# df2(subset of the data)\n",
    "df2 = df1.iloc[:data_size]\n",
    "obj2 = obj1\n",
    "features_list2 = features_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 外れ値除去と標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2  # choose from (df1, df2)\n",
    "obj = obj2  # choose from (obj1, obj2)\n",
    "features_list = features_list2  # choose from (features_list1, features_list2)\n",
    "\n",
    "# 外れ値除去\n",
    "df = remove_outliers_zscore(df, obj)\n",
    "\n",
    "# 標準化\n",
    "X = df[features_list]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=features_list)\n",
    "\n",
    "# 目的変数\n",
    "y = df[obj]\n",
    "\n",
    "# 行を詰める\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### 提案手法のクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Allocation_in_Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Allocation_in_Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features_to_select: int,\n",
    "        n_clusters: int,\n",
    "        allocation_method_list: list[str] = [\"random\"],\n",
    "        clustering_method: str = \"gmm\",\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.n_features_to_select = n_features_to_select  # 特徴量数\n",
    "        self.n_clusters = n_clusters  # クラスタ数\n",
    "        self.allocation_method_list = allocation_method_list  # 特徴量選択基準\n",
    "        self.clustering_method = clustering_method  # クラスタリング手法\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fss(self, X: pd.DataFrame, y: pd.DataFrame) -> \"Allocation_in_Wrapper\":\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        n_features = X.shape[1]  # 総特徴量数\n",
    "        self.selected_features_ = []  # ここに選択した特徴量を入れる\n",
    "\n",
    "        # 選ばれた特徴量と残っている特徴量の初期化\n",
    "        current_features = []\n",
    "        remaining_features = list(range(n_features))\n",
    "        best_score = -np.inf\n",
    "\n",
    "        while len(current_features) < self.n_features_to_select:\n",
    "            best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "            for feature in remaining_features:\n",
    "                temp_features = tuple(\n",
    "                    current_features + [feature]\n",
    "                )  # 特徴量をひとつ加え、score計算\n",
    "\n",
    "                score = self.crit(X[:, temp_features])\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "\n",
    "            if best_feature is not None:\n",
    "                current_features.append(\n",
    "                    best_feature\n",
    "                )  # best feature をcurrent features に追加\n",
    "                remaining_features.remove(\n",
    "                    best_feature\n",
    "                )  # best feature をremaining features から取り除く\n",
    "                self.selected_features_ = current_features\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # 選ばれた特徴量サブセットでクラスタリング\n",
    "        final_features = X[:, self.selected_features_]\n",
    "        if self.clustering_method == \"gmm\":\n",
    "            self.final_model_ = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                init_params=\"kmeans\",\n",
    "            )\n",
    "        elif self.clustering_method == \"kmeans\":\n",
    "            self.final_model_ = KMeans(\n",
    "                n_clusters=self.n_clusters, random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
    "\n",
    "        self.final_model_.fit(final_features)\n",
    "        self.final_cluster_assignments_ = self.final_model_.predict(final_features)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def crit(self, X: pd.DataFrame) -> float:\n",
    "        # クラスタリング手法がGMMの場合\n",
    "        if self.clustering_method == \"gmm\":\n",
    "            em = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                init_params=\"kmeans\",\n",
    "            )\n",
    "            em.fit(X)\n",
    "            self.cluster_label = em.predict(X)\n",
    "            self.cluster_size = np.bincount(self.cluster_label, minlength=clusters)\n",
    "\n",
    "        # クラスタリング手法がKMEANSの場合\n",
    "        if self.clustering_method == \"kmeans\":\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "            kmeans.fit(X)\n",
    "            self.cluster_label = kmeans.predict(X)\n",
    "            self.cluster_size = np.bincount(self.cluster_label, minlength=clusters)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        score = 5\n",
    "\n",
    "        return score\n",
    "    \n",
    "\n",
    "    # 各ポリシーの生成を行う関数\n",
    "    def create_policies(\n",
    "        outer_self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        random_state: int,\n",
    "        m_value: int,\n",
    "    ) -> dict[list[str] : list[BaseAllocation]]:\n",
    "        policies_dict = {}\n",
    "        if \"random\" in outer_self.allocation_method_list:\n",
    "            policies_dict[\"random\"]=outer_self.RandomAllocation(\n",
    "                n_samples=n_samples,\n",
    "                H=H,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        if \"proportional\" in outer_self.allocation_method_list:\n",
    "            policies_dict[\"proportional\"]=outer_self.ProportionalAllocation(\n",
    "                n_samples=n_samples,\n",
    "                H=H,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        if \"post\" in outer_self.allocation_method_list:\n",
    "            policies_dict[\"post\"] = outer_self.PostStratification(\n",
    "                n_samples=n_samples,\n",
    "                H=H,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        if \"optimal\" in outer_self.allocation_method_list:\n",
    "            policies_dict[\"optimal\"] = outer_self.OptimalAllocation(\n",
    "                n_samples=n_samples,\n",
    "                H=H,\n",
    "                random_state=random_state,\n",
    "                m=np.full(H, m_value),\n",
    "                M=None,\n",
    "            )\n",
    "        return policies_dict\n",
    "\n",
    "    class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "        # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "        def __init__(\n",
    "            self,\n",
    "            outer_self,\n",
    "            n_samples: int,\n",
    "            H: int,\n",
    "            random_state: int,\n",
    "        ):\n",
    "            self.outer_self = outer_self\n",
    "            self.n_samples = n_samples\n",
    "            self.H = H\n",
    "            self.random_state = random_state\n",
    "\n",
    "        @abstractmethod\n",
    "        def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "            \"\"\"標本配分を解く\n",
    "\n",
    "            Args:\n",
    "                X (NDArray): データ (N x M)\n",
    "                y (NDArray): 目的変数 (N)\n",
    "\n",
    "            Raises:\n",
    "                NotImplementedError: _description_\n",
    "\n",
    "            Returns:\n",
    "                NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "            Note:\n",
    "                M: 特徴量数\n",
    "                H: クラスタ数\n",
    "            \"\"\"\n",
    "            # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "            cluster_label = self.outer_self.cluster_label\n",
    "            cluster_size = self.outer_self.cluster_size\n",
    "            # インスタンス変数として設定\n",
    "            self.cluster_label = cluster_label\n",
    "            self.N = cluster_size\n",
    "            return cluster_label, cluster_size\n",
    "    \n",
    "    class RandomAllocation(BaseAllocation):\n",
    "        # 抽象メゾッドを具象化\n",
    "        def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "            \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "            n = np.array([self.n_samples])\n",
    "            return n\n",
    "\n",
    "        def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "            # cluster_labelのすべての要素は0（すべてのデータを同じクラスタに属させている）\n",
    "            cluster_label = np.zeros(\n",
    "                X.shape[0]\n",
    "            )  # cluster_label = [0,0,0,,...(要素数：データ数）]\n",
    "            # クラスタサイズ＝データ数\n",
    "            cluster_size = np.array([len(cluster_label)])  # cluster_size=[データ数]\n",
    "            return cluster_label, cluster_size\n",
    "        \n",
    "    class ProportionalAllocation(BaseAllocation):\n",
    "        def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "            \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "            n: NDArray = np.round(self.N / self.N.sum() * self.n_samples).astype(int)\n",
    "\n",
    "            if n.sum() > self.n_samples:\n",
    "                # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "                n[np.argmax(n)] -= n.sum() - self.n_samples\n",
    "\n",
    "            for i in range(\n",
    "                len(n)\n",
    "            ):  # nの要素でm_VALUEより小さいものがあれば要素数が最も大きい層から持ってくる\n",
    "                if n[i] < m_VALUE:\n",
    "                    delta = m_VALUE - n[i]\n",
    "                    n[i] = m_VALUE\n",
    "                    n[np.argmax(n)] -= delta\n",
    "\n",
    "            return n\n",
    "\n",
    "    class PostStratification(BaseAllocation):\n",
    "        def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "            \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "            n = np.array([self.n_samples])\n",
    "\n",
    "            return n  # （例）n=[標本サイズ]\n",
    "        \n",
    "    class OptimalAllocation(BaseAllocation):\n",
    "        def __init__(\n",
    "            self,\n",
    "            n_samples: int,\n",
    "            H: int,\n",
    "            m: NDArray,  # 標本サイズ下限\n",
    "            M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "            random_state: int = 0,\n",
    "\n",
    "        ):\n",
    "            super().__init__(\n",
    "                n_samples, H, random_state\n",
    "            )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "            self.m = m  # 各クラスタの最小標本サイズ (H, )\n",
    "            self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "        def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "            # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "            S = np.array([np.var(y[self.cluster_label == h]) for h in range(self.H)])\n",
    "            d = (self.N**2) * S  # (H, )\n",
    "            n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "            # 制約チェック\n",
    "            self._check_constraints(n)\n",
    "\n",
    "            return n\n",
    "\n",
    "        def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "            print(n)\n",
    "            print(d)\n",
    "            M = self.M.copy() if self.M is not None else self.N.copy()\n",
    "            I = np.arange(self.H)  # noqa #クラスタのインデックス\n",
    "            while (n.sum() != self.n_samples) and len(I) != 0:\n",
    "                delta = np.zeros(self.H)\n",
    "                delta[I] = (d / (n + 1) - d / n)[I]\n",
    "                h_star = np.argmin(delta[I])\n",
    "                h_star = I[h_star]\n",
    "\n",
    "                if n[h_star] + 1 <= M[h_star]:\n",
    "                    n[h_star] = n[h_star] + 1\n",
    "                else:\n",
    "                    # Iの要素h_starを削除\n",
    "                    I_ = I.tolist()\n",
    "                    I_ = [i for i in I_ if i != h_star]\n",
    "                    I = np.array(I_)  # noqa\n",
    "\n",
    "            return n\n",
    "\n",
    "        def _check_constraints(self, n: NDArray):\n",
    "            assert (\n",
    "                n.sum() <= self.n_samples\n",
    "            ), f\"Total sample size is over than {self.n_samples}\"\n",
    "            assert np.all(n >= self.m), \"Minimum sample size constraint is not satisfied\"\n",
    "            if self.M is not None:\n",
    "                assert np.all(\n",
    "                    n <= self.M\n",
    "                ), \"Maximum sample size constraint is not satisfied\"\n",
    "\n",
    "    def estimate_y_mean(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"実際にサンプリングを行って目的変数の平均を推定\n",
    "\n",
    "        Args:\n",
    "            n (NDArray): 各クラスタの標本数 (H, )\n",
    "            cluster_label (NDArray): クラスタラベル (N, )\n",
    "            y (NDArray): 目的変数 (N, )\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 推定された目的変数の平均\n",
    "\n",
    "        Note:\n",
    "            N: データ数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # cluster_labelからユニークなクラスタラベルを取得し、母集団の各クラスタのサイズNを取得\n",
    "        N = np.unique(cluster_label, return_counts=True)[1]  # クラスタサイズ (H, )\n",
    "        weights = N / N.sum()\n",
    "        y_hat = 0\n",
    "        for h in range(n.shape[0]):  # n.shape[0]:層の数\n",
    "            y_cluster = y[cluster_label == h]\n",
    "            # クラスタ内でランダム n_h サンプリング\n",
    "            sample: NDArray = np.random.choice(y_cluster, n[h], replace=False)\n",
    "            y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "            y_hat += y_sample_mean * weights[h]\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def estimate_y_mean_post(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "        N = np.unique(cluster_label, return_counts=True)[1]\n",
    "        weights = N / N.sum()\n",
    "        y_hat = 0\n",
    "        indices = np.arange(N.sum())\n",
    "        y_array = np.array(y.tolist())\n",
    "        n_indices = np.random.choice(indices, n[0], replace=False)\n",
    "        n_label = np.array([cluster_label[i] for i in n_indices])\n",
    "        n_new = np.unique(n_label)\n",
    "        for h in n_new:\n",
    "            index = np.where(n_label == h)[0]\n",
    "            sample = y_array[n_indices[index]]\n",
    "            y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "            y_hat += y_sample_mean * weights[h]\n",
    "        return y_hat             \n",
    "    \n",
    "\n",
    "    def get_feature_index_out(self) -> NDArray:\n",
    "        return np.array(self.selected_features_)  # 選択された特徴量のインデックス\n",
    "\n",
    "    def get_final_cluster_assignments(self) -> NDArray:\n",
    "        return self.final_cluster_assignments_  # 最終的なクラスタリング結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper classでクラスタリングしたとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ fssem_tr ]\n",
      "選択された特徴量のインデックス :  [ 2  6  0  4 13 11  7 10  3 16  8  1]\n",
      "選択された特徴量の数 :  12\n",
      "各層のクラスタサイズ :  [1529 1431]\n",
      "\n",
      "[ fssem_ml ]\n",
      "選択された特徴量のインデックス :  [2 0 1]\n",
      "選択された特徴量の数 :  3\n",
      "各層のクラスタサイズ :  [1529 1431]\n",
      "\n",
      "[ fsskmeans_tr ]\n",
      "選択された特徴量のインデックス :  [ 2  6 14 13  7 10  1  8  9]\n",
      "選択された特徴量の数 :  9\n",
      "各層のクラスタサイズ :  [1529 1431]\n",
      "\n",
      "[ fsskmeans_ml ]\n",
      "選択された特徴量のインデックス :  [ 5 14  9 15  7 12  2  1 10 16  3  4 13  8  6 11  0]\n",
      "選択された特徴量の数 :  17\n",
      "各層のクラスタサイズ :  [ 810 2150]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrapper法を実装するための関数\n",
    "def process_wrapper(\n",
    "    name: str, instance: \"Allocation_in_Wrapper\", X_scaled: pd.DataFrame, y: pd.DataFrame\n",
    "):\n",
    "    instance.fss(X_scaled, y)\n",
    "    selected_features_index = instance.get_feature_index_out()\n",
    "    cluster_label = instance.get_final_cluster_assignments()\n",
    "    cluster_size = np.unique(cluster_label, return_counts=True)[1]\n",
    "\n",
    "    return selected_features_index, cluster_label, cluster_size\n",
    "\n",
    "\n",
    "# Wrapperインスタンスのリスト\n",
    "instances = [\n",
    "    (\n",
    "        \"fssem_tr\",\n",
    "        Wrapper(\n",
    "            n_features_to_select=n_features_to_select,\n",
    "            n_clusters=clusters,\n",
    "            criterion=\"tr\",\n",
    "            clustering_method=\"gmm\",\n",
    "            random_state=0,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"fssem_ml\",\n",
    "        Wrapper(\n",
    "            n_features_to_select=n_features_to_select,\n",
    "            n_clusters=clusters,\n",
    "            criterion=\"ml\",\n",
    "            clustering_method=\"gmm\",\n",
    "            random_state=0,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"fsskmeans_tr\",\n",
    "        Wrapper(\n",
    "            n_features_to_select=n_features_to_select,\n",
    "            n_clusters=clusters,\n",
    "            criterion=\"tr\",\n",
    "            clustering_method=\"kmeans\",\n",
    "            random_state=0,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"fsskmeans_ml\",\n",
    "        Wrapper(\n",
    "            n_features_to_select=n_features_to_select,\n",
    "            n_clusters=clusters,\n",
    "            criterion=\"ml\",\n",
    "            clustering_method=\"kmeans\",\n",
    "            random_state=0,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 辞書の初期化\n",
    "selected_features_index_dict = {}\n",
    "cluster_label_dict = {}\n",
    "cluster_size_dict = {}\n",
    "\n",
    "# 各インスタンスに対して処理を実行\n",
    "for name, instance in instances:\n",
    "    selected_features_index, cluster_label, cluster_size = process_wrapper(\n",
    "        name, instance, X_scaled, y\n",
    "    )\n",
    "    selected_features_index_dict[name] = selected_features_index\n",
    "    cluster_label_dict[name] = cluster_label\n",
    "    cluster_size_dict[name] = cluster_size\n",
    "    print(\"[\", name, \"]\")\n",
    "    print(\"選択された特徴量のインデックス : \", selected_features_index)\n",
    "    print(\"選択された特徴量の数 : \", len(selected_features_index))\n",
    "    print(\"各層のクラスタサイズ : \", cluster_size)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kmeans, GMMでクラスタリングしたとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ kmeans ]\n",
      "各層のクラスタサイズ :  [ 810 2150]\n",
      "\n",
      "[ gmm ]\n",
      "各層のクラスタサイズ :  [ 766 2194]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-meansクラスタリングの適用\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=0)\n",
    "kmeans_cluster = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "gmm = GaussianMixture(n_components=clusters, random_state=0, init_params=\"kmeans\")\n",
    "gmm_cluster = gmm.fit_predict(X_scaled)\n",
    "\n",
    "kmeans_size = np.bincount(kmeans_cluster, minlength=clusters)\n",
    "gmm_size = np.bincount(gmm_cluster, minlength=clusters)\n",
    "\n",
    "cluster_label_dict[\"kmeans\"] = kmeans_cluster\n",
    "cluster_label_dict[\"gmm\"] = gmm_cluster\n",
    "cluster_size_dict[\"kmeans\"] = kmeans_size\n",
    "cluster_size_dict[\"gmm\"] = gmm_size\n",
    "\n",
    "methods = [\"kmeans\", \"gmm\"]\n",
    "for name in methods:\n",
    "    print(\"[\", name, \"]\")\n",
    "    print(\"各層のクラスタサイズ : \", cluster_size_dict[name])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### 標本配分（Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基底クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "    # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        random_state: int,\n",
    "        criterion: str,\n",
    "        clustering_method: str,\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.H = H\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.clustering_method = clustering_method\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"標本配分を解く\n",
    "\n",
    "        Args:\n",
    "            X (NDArray): データ (N x M)\n",
    "            y (NDArray): 目的変数 (N)\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "        Note:\n",
    "            M: 特徴量数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"gmm\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"gmm\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_ml\"]\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_ml\"]\n",
    "        if self.criterion == \"none\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"kmeans\"]\n",
    "            cluster_size = cluster_size_dict[\"kmeans\"]\n",
    "        if self.criterion == \"none\" and self.clustering_method == \"gmm\":\n",
    "            cluster_label = cluster_label_dict[\"gmm\"]\n",
    "            cluster_size = cluster_size_dict[\"gmm\"]\n",
    "\n",
    "        # インスタンス変数として設定\n",
    "        self.cluster_label = cluster_label\n",
    "        self.N = cluster_size\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 単純無作為抽出のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAllocation(BaseAllocation):\n",
    "    # 抽象メゾッドを具象化\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "        return n\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        # cluster_labelのすべての要素は0（すべてのデータを同じクラスタに属させている）\n",
    "        cluster_label = np.zeros(\n",
    "            X.shape[0]\n",
    "        )  # cluster_label = [0,0,0,,...(要素数：データ数）]\n",
    "        # クラスタサイズ＝データ数\n",
    "        cluster_size = np.array([len(cluster_label)])  # cluster_size=[データ数]\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 比例配分のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalAllocation(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "        n: NDArray = np.round(self.N / self.N.sum() * self.n_samples).astype(int)\n",
    "\n",
    "        if n.sum() > self.n_samples:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n[np.argmax(n)] -= n.sum() - self.n_samples\n",
    "\n",
    "        for i in range(\n",
    "            len(n)\n",
    "        ):  # nの要素でm_VALUEより小さいものがあれば要素数が最も大きい層から持ってくる\n",
    "            if n[i] < m_VALUE:\n",
    "                delta = m_VALUE - n[i]\n",
    "                n[i] = m_VALUE\n",
    "                n[np.argmax(n)] -= delta\n",
    "\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 事後層化のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostStratification(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最適標本配分のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAllocation(BaseAllocation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        m: NDArray,  # 標本サイズ下限\n",
    "        M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "        random_state: int = 0,\n",
    "        criterion: str = \"tr\",\n",
    "        clustering_method: str = \"kmeans\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_samples, H, random_state, criterion, clustering_method\n",
    "        )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "        self.m = m  # 各クラスタの最小標本サイズ (H, )\n",
    "        self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array([np.var(y[self.cluster_label == h]) for h in range(self.H)])\n",
    "        d = (self.N**2) * S  # (H, )\n",
    "        n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "        # 制約チェック\n",
    "        self._check_constraints(n)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "        print(n)\n",
    "        print(d)\n",
    "        M = self.M.copy() if self.M is not None else self.N.copy()\n",
    "        I = np.arange(self.H)  # noqa #クラスタのインデックス\n",
    "        while (n.sum() != self.n_samples) and len(I) != 0:\n",
    "            delta = np.zeros(self.H)\n",
    "            delta[I] = (d / (n + 1) - d / n)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n[h_star] + 1 <= M[h_star]:\n",
    "                n[h_star] = n[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _check_constraints(self, n: NDArray):\n",
    "        assert (\n",
    "            n.sum() <= self.n_samples\n",
    "        ), f\"Total sample size is over than {self.n_samples}\"\n",
    "        assert np.all(n >= self.m), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 母平均の推定値を計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_y_mean(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    \"\"\"実際にサンプリングを行って目的変数の平均を推定\n",
    "\n",
    "    Args:\n",
    "        n (NDArray): 各クラスタの標本数 (H, )\n",
    "        cluster_label (NDArray): クラスタラベル (N, )\n",
    "        y (NDArray): 目的変数 (N, )\n",
    "\n",
    "    Returns:\n",
    "        NDArray: 推定された目的変数の平均\n",
    "\n",
    "    Note:\n",
    "        N: データ数\n",
    "        H: クラスタ数\n",
    "    \"\"\"\n",
    "    # cluster_labelからユニークなクラスタラベルを取得し、母集団の各クラスタのサイズNを取得\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]  # クラスタサイズ (H, )\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    for h in range(n.shape[0]):  # n.shape[0]:層の数\n",
    "        y_cluster = y[cluster_label == h]\n",
    "        # クラスタ内でランダム n_h サンプリング\n",
    "        sample: NDArray = np.random.choice(y_cluster, n[h], replace=False)\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_post(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    indices = np.arange(N.sum())\n",
    "    y_array = np.array(y.tolist())\n",
    "    n_indices = np.random.choice(indices, n[0], replace=False)\n",
    "    n_label = np.array([cluster_label[i] for i in n_indices])\n",
    "    n_new = np.unique(n_label)\n",
    "    for h in n_new:\n",
    "        index = np.where(n_label == h)[0]\n",
    "        sample = y_array[n_indices[index]]\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各ポリシーを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ポリシーの生成を行う関数\n",
    "def create_policies(\n",
    "    criterion_list: list[str],\n",
    "    clustering_method_list: list[str],\n",
    "    n_samples: int,\n",
    "    H: int,\n",
    "    random_state: int,\n",
    "    m_value: int,\n",
    ") -> dict[list[str] : list[BaseAllocation]]:\n",
    "    policies_dict = {}\n",
    "    for criterion in criterion_list:\n",
    "        for clustering_method in clustering_method_list:\n",
    "            policies: list[BaseAllocation] = [\n",
    "                RandomAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                ProportionalAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                PostStratification(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                OptimalAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    m=np.full(H, m_value),\n",
    "                    M=None,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "            ]\n",
    "            policies_dict[(criterion, clustering_method)] = policies\n",
    "    return policies_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### 母平均の推定と分散の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 母平均の推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n",
      "[5.61260709e+13 2.23524856e+13]\n",
      "[2 2]\n",
      "[2.24750549e+13 4.79357132e+13]\n",
      "[2 2]\n",
      "[5.61260709e+13 2.23524856e+13]\n",
      "[2 2]\n",
      "[5.61260709e+13 2.23524856e+13]\n",
      "[2 2]\n",
      "[1.46310681e+13 7.42317620e+13]\n",
      "[2 2]\n",
      "[2.24750549e+13 4.79357132e+13]\n"
     ]
    }
   ],
   "source": [
    "policies_dict = create_policies(\n",
    "    CRITERION_LIST, CLUSTERING_METHOD_LIST, N_SAMPLES, H, RANDOM_STATE, m_VALUE\n",
    ")\n",
    "allocations_dict = {}\n",
    "for method, policies in policies_dict.items():\n",
    "    # それぞれの戦略で各クラスタの標本数を求解\n",
    "    allocations: list[dict] = []  # 各戦略の実行結果が辞書形式で追加される\n",
    "    for policy in policies:\n",
    "        # policyを用いてXをクラスタリング\n",
    "        cluster_label, _ = policy.clustering(X_scaled)\n",
    "        n = policy.solve(X_scaled, y)\n",
    "        allocations.append(\n",
    "            {\n",
    "                \"policy\": policy.__class__.__name__,\n",
    "                \"n\": n,\n",
    "                \"cluster_label\": cluster_label,\n",
    "            }\n",
    "        )\n",
    "    allocations_dict[method] = allocations\n",
    "\n",
    "# 各戦略の標本数に基づいて目的変数の平均を推定\n",
    "y_hats_dict = {}\n",
    "for method, allocations in allocations_dict.items():\n",
    "    y_hats = []\n",
    "    for random_state in range(N_TRIALS):\n",
    "        for allocation in allocations:\n",
    "            if allocation[\"policy\"] == \"PostStratification\":\n",
    "                y_hat = estimate_y_mean_post(\n",
    "                    allocation[\"n\"], allocation[\"cluster_label\"], y\n",
    "                )\n",
    "            else:\n",
    "                y_hat = estimate_y_mean(allocation[\"n\"], allocation[\"cluster_label\"], y)\n",
    "            y_hats.append(\n",
    "                {\n",
    "                    \"policy\": allocation[\"policy\"],\n",
    "                    \"y_hat\": y_hat,\n",
    "                    \"random_state\": random_state,\n",
    "                }\n",
    "            )\n",
    "    y_hats_dict[method] = y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(clustering_method: gmm , criterion: ml )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation               NaN\n",
      "ProportionalAllocation    6.247977\n",
      "PostStratification        3.460032\n",
      "OptimalAllocation         8.776575\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 174605.50959078077\n",
      "ProportionalAllocation 163696.19675981626\n",
      "PostStratification 168564.10234339567\n",
      "OptimalAllocation 159281.12677647927\n",
      "\n",
      "(clustering_method: kmeans , criterion: ml )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation                NaN\n",
      "ProportionalAllocation     3.376753\n",
      "PostStratification        -9.727350\n",
      "OptimalAllocation         10.446356\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 161337.10977551303\n",
      "ProportionalAllocation 155889.1537394893\n",
      "PostStratification 177030.93570985756\n",
      "OptimalAllocation 144483.2615712785\n",
      "\n",
      "(clustering_method: gmm , criterion: tr )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation                NaN\n",
      "ProportionalAllocation     1.405573\n",
      "PostStratification         3.555973\n",
      "OptimalAllocation         12.857874\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 178506.6372767006\n",
      "ProportionalAllocation 175997.59608725403\n",
      "PostStratification 172158.98866738362\n",
      "OptimalAllocation 155554.47858046525\n",
      "\n",
      "(clustering_method: kmeans , criterion: tr )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation               NaN\n",
      "ProportionalAllocation   -2.698324\n",
      "PostStratification        1.533684\n",
      "OptimalAllocation         6.989264\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 178787.5761314065\n",
      "ProportionalAllocation 183611.84344715954\n",
      "PostStratification 176045.53964333647\n",
      "OptimalAllocation 166291.6411240775\n",
      "\n",
      "(clustering_method: gmm , criterion: none )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation               NaN\n",
      "ProportionalAllocation   -2.870478\n",
      "PostStratification       -3.374533\n",
      "OptimalAllocation        -3.674138\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 166569.469976864\n",
      "ProportionalAllocation 171350.81017646237\n",
      "PostStratification 172190.41177915884\n",
      "OptimalAllocation 172689.461650649\n",
      "\n",
      "(clustering_method: kmeans , criterion: none )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation                NaN\n",
      "ProportionalAllocation     6.880282\n",
      "PostStratification         5.421683\n",
      "OptimalAllocation         19.186911\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 173350.2795042142\n",
      "ProportionalAllocation 161423.29110425853\n",
      "PostStratification 163951.777688637\n",
      "OptimalAllocation 140089.71480372982\n"
     ]
    }
   ],
   "source": [
    "for method, y_hats in y_hats_dict.items():\n",
    "    y_hat_df = pd.DataFrame(y_hats)\n",
    "    y_hat_df[\"error\"] = (\n",
    "        y_hat_df[\"y_hat\"] - y.mean()\n",
    "    )  # 真の平均からの誤差をerrorカラムに追加\n",
    "\n",
    "    # random_allocationの誤差分散\n",
    "    random_allocation_std = y_hat_df[y_hat_df[\"policy\"] == \"RandomAllocation\"][\n",
    "        \"error\"\n",
    "    ].var()\n",
    "    # random_allocation以外の誤差分散\n",
    "    non_random_allocation_std = (\n",
    "        y_hat_df[y_hat_df[\"policy\"] != \"RandomAllocation\"]\n",
    "        .groupby(\"policy\")[\"error\"]\n",
    "        .var()\n",
    "    )\n",
    "\n",
    "    # 削減率\n",
    "    reduction_rate = (1 - non_random_allocation_std / random_allocation_std) * 100\n",
    "\n",
    "    ## policyの順番をpoliciesの順番に調整\n",
    "    reduction_rate = reduction_rate.reindex(\n",
    "        [policy.__class__.__name__ for policy in policies]\n",
    "    )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"(clustering_method:\", method[1], \", criterion:\", method[0], \")\")\n",
    "    print(\"[各手法の誤差分散削減率]\")\n",
    "    print(reduction_rate)\n",
    "\n",
    "    print(\"[各手法の分散]\")\n",
    "    policy_name_list = [policy.__class__.__name__ for policy in policies]\n",
    "    for i in range(len(policy_name_list)):\n",
    "        var = y_hat_df[y_hat_df[\"policy\"] == policy_name_list[i]][\"error\"].var()\n",
    "        print(policy_name_list[i], var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
