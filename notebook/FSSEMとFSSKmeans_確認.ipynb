{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\HaruMomozu\\.rye\\py\\cpython@3.12.4\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_19988\\1091100056.py\", line 10, in <module>\n",
      "    from lightning.pytorch import seed_everything\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\__init__.py\", line 18, in <module>\n",
      "    from lightning.fabric.fabric import Fabric  # noqa: E402\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\fabric\\__init__.py\", line 30, in <module>\n",
      "    from lightning.fabric.fabric import Fabric  # noqa: E402\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\lightning\\fabric\\fabric.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "Seed set to 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.validation import check_array, check_X_y\n",
    "\n",
    "import japanize_matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # データ可視化ライブラリ\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "seed_everything(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(data, metric, threshold=2):\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\NHANES_age_prediction.csv\"\n",
    ")\n",
    "df1 = df1.drop(columns=[\"SEQN\", \"age_group\"])\n",
    "\n",
    "obj1 = \"BMXBMI\"\n",
    "features_list1 = [\n",
    "    \"RIDAGEYR\",  # 年齢（連続変数）\n",
    "    \"RIAGENDR\",  # 性別（1:Male, 2:Female)\n",
    "    \"PAQ605\",  # 運動有無(1:日常的に運動する, 2:運動しない)\n",
    "    \"LBXGLU\",  # 断食後の血糖値（連続変数）\n",
    "    \"DIQ010\",  # 糖尿病の有無(0:なし、1:あり)\n",
    "    \"LBXGLT\",  # 口内の健康状態（連続変数）\n",
    "    \"LBXIN\",  # 血中インスリン濃度（連続変数）\n",
    "]\n",
    "df1 = df1[df1[\"PAQ605\"] != 7.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\OnlineNewsPopularity\\OnlinenewsPopularity.csv\"\n",
    ")\n",
    "df2 = df2.drop(columns=[\"url\"])\n",
    "df2 = df2.drop(columns=[\" timedelta\"])\n",
    "\n",
    "obj2 = \" shares\"\n",
    "features_list2 = [col for col in list(df2.columns) if col != \" shares\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\USCensus1990.data.txt\",\n",
    "#     delimiter=\",\",\n",
    "# )\n",
    "\n",
    "# obj3 = \"iFertil\"\n",
    "# features_list3 = [col for col in list(df3.columns) if col != obj3]\n",
    "# features_list3_20 = features_list3[:20]\n",
    "# print(features_list3_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\aug_first_cpn_data_for_ab_test_sensibility_tsukuba.csv\"\n",
    "# )\n",
    "\n",
    "# obj4 = \"GMV\"\n",
    "# features_list4 = [\n",
    "#     \"hist_4_day_buy_num\",\n",
    "#     \"hist_4_day_gmv\",\n",
    "#     \"his_4_day_is_buy\",\n",
    "#     \"hist_30_day_buy_days\",\n",
    "#     \"hist_30_day_buy_num\",\n",
    "#     \"hist_30_day_gmv\",\n",
    "#     \"hist_30_day_buy_recency\",\n",
    "#     \"hist_30_day_pay_days\",\n",
    "#     \"hist_30_day_atpu\",\n",
    "#     \"hist_30_day_gpv\",\n",
    "#     \"hist_30_day_pay_recency\",\n",
    "#     \"hist_30_day_list_days\",\n",
    "#     \"hist_30_day_list_num\",\n",
    "#     \"hist_30_day_list_recency\",\n",
    "#     \"hist_30_day_like_count\",\n",
    "#     \"hist_30_day_like_count_not_deleted\",\n",
    "#     \"hist_30_day_like_recency\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2  # 選ぶ\n",
    "obj = obj2  # 選ぶ\n",
    "features_list = features_list2  # 選ぶ\n",
    "\n",
    "df = remove_outliers_zscore(df, obj)\n",
    "\n",
    "X = df[features_list]\n",
    "# 数値列の標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=features_list)\n",
    "\n",
    "y = df[obj]  # 目的変数\n",
    "\n",
    "# 行を詰める\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSSEM でクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapperクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features_to_select,\n",
    "        n_clusters,\n",
    "        criterion=\"ml\",\n",
    "        clustering_method=\"em\",\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_features_to_select = n_features_to_select  # 特徴量数\n",
    "        self.n_clusters = n_clusters  # クラスタ数\n",
    "        self.criterion = criterion  # 特徴量選択基準\n",
    "        self.clustering_method = clustering_method  # クラスタリング手法\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def FSS(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        n_features = X.shape[1]  # 総特徴量数\n",
    "        self.selected_features_ = []  # ここに選択した特徴量を入れる\n",
    "\n",
    "        # 選ばれた特徴量と残っている特徴量の初期化\n",
    "        current_features = []\n",
    "        remaining_features = list(range(n_features))\n",
    "\n",
    "\n",
    "        while len(current_features) < self.n_features_to_select:\n",
    "            # print(current_features)\n",
    "            best_score = -np.inf  # best score初期化（-∞）\n",
    "            best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "            for feature in remaining_features:\n",
    "                temp_features = tuple(\n",
    "                    current_features + [feature]\n",
    "                )  # 特徴量をひとつ加え、score計算\n",
    "\n",
    "                score = self.CRIT(X[:, temp_features])\n",
    "\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "\n",
    "            if best_feature is not None:\n",
    "                current_features.append(\n",
    "                    best_feature\n",
    "                )  # best feature をcurrent features に追加\n",
    "                remaining_features.remove(\n",
    "                    best_feature\n",
    "                )  # best feature をremaining features から取り除く\n",
    "                self.selected_features_ = current_features\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        final_features = X[:, self.selected_features_]\n",
    "        if self.clustering_method == \"em\":\n",
    "            self.final_model_ = GaussianMixture(\n",
    "                n_components=self.n_clusters, random_state=self.random_state\n",
    "            )\n",
    "        elif self.clustering_method == \"kmeans\":\n",
    "            self.final_model_ = KMeans(\n",
    "                n_clusters=self.n_clusters, random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
    "\n",
    "        self.final_model_.fit(final_features)\n",
    "        self.final_cluster_assignments_ = self.final_model_.predict(final_features)\n",
    "        return self\n",
    "\n",
    "    def CRIT(self, X):\n",
    "        if self.clustering_method == \"em\":\n",
    "            em = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                init_params=\"kmeans\",\n",
    "            )\n",
    "            em.fit(X)\n",
    "            labels = em.predict(X)\n",
    "        if self.clustering_method == \"kmeans\":\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "            kmeans.fit(X)\n",
    "            labels = kmeans.predict(X)\n",
    "\n",
    "        #　共分散行列が特異値行列になるのを回避する\n",
    "        overall_cov_mean = np.mean(np.var(X, axis=0))\n",
    "        identity_matrix = np.eye(X.shape[1])\n",
    "        avoid_singular = overall_cov_mean * identity_matrix\n",
    "        cluster_num = len(np.unique(labels))\n",
    "\n",
    "        if self.criterion == \"ml\":\n",
    "            log_likelihood = 0.0\n",
    "        if self.criterion == \"tr\":\n",
    "            # 全体の平均ベクトル\n",
    "            overall_mean = np.mean(X, axis=0)\n",
    "            # クラスター内分散行列（S_W）の計算\n",
    "            S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "            S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "\n",
    "        for cluster in range(cluster_num):\n",
    "            # クラスタに属するデータポイントを抽出\n",
    "            cluster_points = X[labels == cluster]\n",
    "            # クラスタの平均ベクトル（μ）を計算\n",
    "            mean_vector = np.mean(cluster_points, axis=0)\n",
    "            # クラスタの共分散行列（Σ）を計算\n",
    "            cov_matrix = np.cov(cluster_points, rowvar=False)\n",
    "            if X.shape[1] == 1:\n",
    "                cov_matrix = np.array([[cov_matrix]])\n",
    "            cov_matrix += avoid_singular\n",
    "\n",
    "            if self.criterion == \"ml\":\n",
    "                log_likelihood += np.sum(\n",
    "                    multivariate_normal.logpdf(\n",
    "                        cluster_points, mean=mean_vector, cov=cov_matrix\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if self.criterion == \"tr\":\n",
    "                S_W += cov_matrix * (cluster_points.shape[0] - 1)\n",
    "                n_k = cluster_points.shape[0]\n",
    "                mean_diff = (mean_vector - overall_mean).reshape(-1, 1)\n",
    "                S_B += n_k * np.dot(mean_diff, mean_diff.T)\n",
    "\n",
    "            if self.criterion == \"ml\":\n",
    "                score = log_likelihood\n",
    "            if self.criterion == \"tr\":\n",
    "                score = np.trace(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_array(X)\n",
    "        return X[:, self.selected_features_]\n",
    "\n",
    "    def get_feature_index_out(self):\n",
    "        return np.array(self.selected_features_)\n",
    "\n",
    "    def get_final_cluster_assignments(self):\n",
    "        return self.final_cluster_assignments_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapperクラス確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "clusters = 5\n",
    "n_features_to_select = 5  # 選択したい特徴量の数\n",
    "\n",
    "fssem_tr = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"tr\",\n",
    "    clustering_method=\"em\",\n",
    "    random_state=0,\n",
    ")\n",
    "fssem_ml = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"ml\",\n",
    "    clustering_method=\"em\",\n",
    "    random_state=0,\n",
    ")\n",
    "fsskmeans_tr = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"tr\",\n",
    "    clustering_method=\"kmeans\",\n",
    "    random_state=0,\n",
    ")\n",
    "fsskmeans_ml = Wrapper(\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    n_clusters=clusters,\n",
    "    criterion=\"ml\",\n",
    "    clustering_method=\"kmeans\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "instance_dict = {\n",
    "    \"fssem_tr\": fssem_tr,\n",
    "    \"fssem_ml\": fssem_ml,\n",
    "    \"fsskmeans_tr\": fsskmeans_tr,\n",
    "    \"fsskmeans_ml\": fsskmeans_ml,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_19988\\3095469497.py:116: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(cluster_points, rowvar=False)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_19988\\3095469497.py:116: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(cluster_points, rowvar=False)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_19988\\3095469497.py:116: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(cluster_points, rowvar=False)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2773: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape () doesn't match the broadcast shape (2,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m cluster_size_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, instance \u001b[38;5;129;01min\u001b[39;00m instance_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 5\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFSS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     selected_features_index \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mget_feature_index_out()\n\u001b[0;32m      7\u001b[0m     selected_features_index_dict[name] \u001b[38;5;241m=\u001b[39m selected_features_index\n",
      "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mWrapper.FSS\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     43\u001b[0m     score \u001b[38;5;241m=\u001b[39m cluster_cache[temp_features]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCRIT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     cluster_cache[temp_features] \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n",
      "Cell \u001b[1;32mIn[25], line 119\u001b[0m, in \u001b[0;36mWrapper.CRIT\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    118\u001b[0m     cov_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[cov_matrix]])\n\u001b[1;32m--> 119\u001b[0m \u001b[43mcov_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mavoid_singular\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    122\u001b[0m     log_likelihood \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    123\u001b[0m         multivariate_normal\u001b[38;5;241m.\u001b[39mlogpdf(\n\u001b[0;32m    124\u001b[0m             cluster_points, mean\u001b[38;5;241m=\u001b[39mmean_vector, cov\u001b[38;5;241m=\u001b[39mcov_matrix\n\u001b[0;32m    125\u001b[0m         )\n\u001b[0;32m    126\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape () doesn't match the broadcast shape (2,2)"
     ]
    }
   ],
   "source": [
    "selected_features_index_dict = {}\n",
    "cluster_label_dict = {}\n",
    "cluster_size_dict = {}\n",
    "for name, instance in instance_dict.items():\n",
    "    instance.FSS(X_scaled, y)\n",
    "    selected_features_index = instance.get_feature_index_out()\n",
    "    selected_features_index_dict[name] = selected_features_index\n",
    "    cluster_label = instance.get_final_cluster_assignments()\n",
    "    cluster_label_dict[name] = cluster_label\n",
    "    cluster_size = np.unique(cluster_label, return_counts=True)[1]\n",
    "    cluster_size_dict[name] = cluster_size\n",
    "\n",
    "    print(selected_features_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 層化抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "    # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        random_state: int,\n",
    "        criterion: str,\n",
    "        clustering_method,\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.H = H\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.clustering_method = clustering_method\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"標本配分を解く\n",
    "\n",
    "        Args:\n",
    "            X (NDArray): データ (N x M)\n",
    "            y (NDArray): 目的変数 (N)\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "        Note:\n",
    "            M: 特徴量数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"em\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"em\":\n",
    "            cluster_label = cluster_label_dict[\"fssem_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fssem_ml\"]\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"kmeans\":\n",
    "            cluster_label = cluster_label_dict[\"fsskmeans_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskmeans_ml\"]\n",
    "        # インスタンス変数として設定\n",
    "        print(cluster_size)\n",
    "        self.cluster_label = cluster_label\n",
    "        self.N = cluster_size\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAllocation(BaseAllocation):\n",
    "    # 抽象メゾッドを具象化\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        # cluster_labelのすべての要素は0（すべてのデータを同じクラスタに属させている）\n",
    "        cluster_label = np.zeros(\n",
    "            X.shape[0]\n",
    "        )  # cluster_label = [0,0,0,,...(要素数：データ数）]\n",
    "        # クラスタサイズ＝データ数\n",
    "        cluster_size = np.array([len(cluster_label)])  # cluster_size=[データ数]\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalAllocation(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "        n: NDArray = np.round(self.N / self.N.sum() * self.n_samples).astype(int)\n",
    "\n",
    "        if n.sum() > self.n_samples:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n[np.argmax(n)] -= n.sum() - self.n_samples\n",
    "\n",
    "        for i in range(len(n)):  # nの要素で2より小さいものがあれば2にする\n",
    "            if n[i] == 0:\n",
    "                n[i] = 2\n",
    "                n[np.argmax(n)] -= 2\n",
    "            if n[i] == 1:\n",
    "                n[i] = 2\n",
    "                n[np.argmax(n)] -= 1\n",
    "\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostStratification(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAllocation(BaseAllocation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        m: NDArray,  # 標本サイズ下限\n",
    "        M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "        random_state: int = 0,\n",
    "        criterion: Optional[str] = None,\n",
    "        clustering_method: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_samples, H, random_state, criterion, clustering_method\n",
    "        )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "        self.m = m  # 各クラスタの最小標本サイズ (H, )\n",
    "        self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array([np.var(y[self.cluster_label == h]) for h in range(self.H)])\n",
    "        d = (self.N**2) * S  # (H, )\n",
    "        n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "        # 制約チェック\n",
    "        self._check_constraints(n)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "        M = self.M.copy() if self.M is not None else self.N.copy()\n",
    "        I = np.arange(self.H)  # noqa #クラスタのインデックス\n",
    "        while (n.sum() != self.n_samples) and len(I) != 0:\n",
    "            delta = np.zeros(self.H)\n",
    "            delta[I] = (d / (n + 1) - d / n)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n[h_star] + 1 <= M[h_star]:\n",
    "                n[h_star] = n[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _check_constraints(self, n: NDArray):\n",
    "        assert (\n",
    "            n.sum() <= self.n_samples\n",
    "        ), f\"Total sample size is over than {self.n_samples}\"\n",
    "        assert np.all(n >= self.m), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_y_mean(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    \"\"\"実際にサンプリングを行って目的変数の平均を推定\n",
    "\n",
    "    Args:\n",
    "        n (NDArray): 各クラスタの標本数 (H, )\n",
    "        cluster_label (NDArray): クラスタラベル (N, )\n",
    "        y (NDArray): 目的変数 (N, )\n",
    "\n",
    "    Returns:\n",
    "        NDArray: 推定された目的変数の平均\n",
    "\n",
    "    Note:\n",
    "        N: データ数\n",
    "        H: クラスタ数\n",
    "    \"\"\"\n",
    "    # cluster_labelからユニークなクラスタラベルを取得し、母集団の各クラスタのサイズNを取得\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]  # クラスタサイズ (H, )\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    for h in range(n.shape[0]):  # n.shape[0]:層の数\n",
    "        y_cluster = y[cluster_label == h]\n",
    "        # クラスタ内でランダム n_h サンプリング\n",
    "        sample: NDArray = np.random.choice(y_cluster, n[h], replace=False)\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_post(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    indices = np.arange(N.sum())\n",
    "    y_array = np.array(y.tolist())\n",
    "    n_indices = np.random.choice(indices, n[0], replace=False)\n",
    "    n_label = np.array([cluster_label[i] for i in n_indices])\n",
    "    n_new = np.unique(n_label, return_counts=True)[1]\n",
    "    for h in range(n_new.shape[0]):\n",
    "        index = np.where(n_label == h)[0]\n",
    "        sample = y_array[n_indices[index]]\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[379 924 789  51  26]\n",
      "[379 924 789  51  26]\n",
      "[379 924 789  51  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaruMomozu\\AppData\\Local\\Temp\\ipykernel_19988\\2208833593.py:42: RuntimeWarning: Mean of empty slice.\n",
      "  y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
      "c:\\Users\\HaruMomozu\\Desktop\\momozu\\ABtesting\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 100  # 標本サイズ\n",
    "H = clusters  # クラスタ数が多すぎるとpropotionalがうまくいかない\n",
    "N_TRIALS = 1000  # 試行回数\n",
    "m_VALUE = 2  # 各クラスタの最小標本数\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "CRITERION = \"ml\"\n",
    "CLUSTERING_METHOD = \"em\"\n",
    "\n",
    "# 戦略を定義\n",
    "policies: list[BaseAllocation] = [\n",
    "    RandomAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    ProportionalAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    PostStratification(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "    OptimalAllocation(\n",
    "        n_samples=N_SAMPLES,\n",
    "        H=H,\n",
    "        random_state=RANDOM_STATE,\n",
    "        m=np.full(H, m_VALUE),\n",
    "        M=None,\n",
    "        criterion=CRITERION,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# それぞれの戦略で各クラスタの標本数を求解\n",
    "allocations: list[dict] = []  # 各戦略の実行結果が辞書形式で追加される\n",
    "for policy in policies:\n",
    "    # policyを用いてXをクラスタリング(_は戻り値の２番目を無視)\n",
    "    cluster_label, _ = policy.clustering(X_scaled)\n",
    "    n = policy.solve(X_scaled, y)\n",
    "    allocations.append(\n",
    "        {\n",
    "            \"policy\": policy.__class__.__name__,\n",
    "            \"n\": n,\n",
    "            \"cluster_label\": cluster_label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 各戦略の標本数に基づいて目的変数の平均を推定\n",
    "y_hats = []\n",
    "for random_state in range(N_TRIALS):\n",
    "    for allocation in allocations:\n",
    "        if allocation[\"policy\"] == \"PostStratification\":\n",
    "            y_hat = estimate_y_mean_post(\n",
    "                allocation[\"n\"], allocation[\"cluster_label\"], y\n",
    "            )\n",
    "        else:\n",
    "            y_hat = estimate_y_mean(allocation[\"n\"], allocation[\"cluster_label\"], y)\n",
    "        y_hats.append(\n",
    "            {\n",
    "                \"policy\": allocation[\"policy\"],\n",
    "                \"y_hat\": y_hat,\n",
    "                \"random_state\": random_state,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱ひげ図で誤差の分布を比較\n",
    "y_hat_df = pd.DataFrame(y_hats)\n",
    "y_hat_df[\"error\"] = (\n",
    "    y_hat_df[\"y_hat\"] - y.mean()\n",
    ")  # 真の平均からの誤差をerrorカラムに追加\n",
    "\n",
    "# 棒グラフでRandomAllocationに対する誤差分散削減率を比較\n",
    "# random_allocationの誤差分散\n",
    "random_allocation_std = y_hat_df[y_hat_df[\"policy\"] == \"RandomAllocation\"][\n",
    "    \"error\"\n",
    "].var()\n",
    "# random_allocation以外の誤差分散\n",
    "non_random_allocation_std = (\n",
    "    y_hat_df[y_hat_df[\"policy\"] != \"RandomAllocation\"].groupby(\"policy\")[\"error\"].var()\n",
    ")\n",
    "reduction_rate = (\n",
    "    1 - non_random_allocation_std / random_allocation_std\n",
    ") * 100  # 削減率 (%)\n",
    "## policyの順番をpoliciesの順番に調整\n",
    "reduction_rate = reduction_rate.reindex(\n",
    "    [policy.__class__.__name__ for policy in policies]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[criterion: ml clustering_method: em ]\n",
      "RandomAllocation 0.29864462893993976\n",
      "ProportionalAllocation 0.2905039877746467\n",
      "PostStratification 0.3612216027560774\n",
      "OptimalAllocation 0.30419235339427\n"
     ]
    }
   ],
   "source": [
    "print(\"[criterion:\", CRITERION, \"clustering_method:\", CLUSTERING_METHOD, \"]\")\n",
    "\n",
    "policy_name_list = [policy.__class__.__name__ for policy in policies]\n",
    "for i in range(len(policy_name_list)):\n",
    "    var = y_hat_df[y_hat_df[\"policy\"] == policy_name_list[i]][\"error\"].var()\n",
    "    print(policy_name_list[i], var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml, em(0820ゼミで発表したの)  \n",
    "[criterion: ml clustering_method: em ]  \n",
    "RandomAllocation 0.29525629549449456  \n",
    "ProportionalAllocation 0.29054056185640814  \n",
    "PostStratification 0.3103888973475933  \n",
    "OptimalAllocation 0.28627511216128887  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
