{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# １．ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn関連\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "# タイピングのサポート\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# 可視化の設定\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import itertools\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# ２．実験設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 実験データの設定 ###\n",
    "TARGET = \"PM_US Post\"\n",
    "THRESHOLD = 2  # 外れ値除外の閾値\n",
    "\n",
    "### 実験設定 ###\n",
    "N_TRIALS = 10000  # 試行回数（標本平均を求める回数）\n",
    "SAMPLE_SIZE = 10000  # 標本サイズ\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "\n",
    "### シード ###\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# ３．データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\BeijingPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Chengdu = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ChengduPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Guangzhou = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\GuangzhouPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Shanghai = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ShanghaiPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Shenyang = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ShenyangPM20100101_20151231.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Beijing = pd.read_csv(\n",
    "#     R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\BeijingPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Chengdu = pd.read_csv(\n",
    "#     R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ChengduPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Guangzhou = pd.read_csv(\n",
    "#     R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\GuangzhouPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Shanghai = pd.read_csv(\n",
    "#     R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ShanghaiPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Shenyang = pd.read_csv(\n",
    "#     R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ShenyangPM20100101_20151231.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing[\"city\"] = \"Beijing\"\n",
    "df_Chengdu[\"city\"] = \"Chengdu\"\n",
    "df_Guangzhou[\"city\"] = \"Guangzhou\"\n",
    "df_Shanghai[\"city\"] = \"Shanghai\"\n",
    "df_Shenyang[\"city\"] = \"Shenyang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(\n",
    "    [df_Beijing, df_Chengdu, df_Guangzhou, df_Shanghai, df_Shenyang],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PM_US Post', 'DEWP', 'TEMP', 'HUMI', 'PRES', 'Iws', 'precipitation', 'Iprec', 'city', 'season', 'cbwd']\n"
     ]
    }
   ],
   "source": [
    "features_list_con = [\n",
    "    \"DEWP\",\n",
    "    \"TEMP\",\n",
    "    \"HUMI\",\n",
    "    \"PRES\",\n",
    "    \"Iws\",\n",
    "    \"precipitation\",\n",
    "    \"Iprec\",\n",
    "]\n",
    "features_list_cat = [\"city\", \"season\", \"cbwd\"]\n",
    "\n",
    "features_list = features_list_con + features_list_cat\n",
    "variables_list = [TARGET] + features_list_con + features_list_cat\n",
    "\n",
    "print(variables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           No  year  month  day  hour  PM_Dongsi  PM_Dongsihuan  \\\n",
      "0           1  2010      1    1     0        NaN            NaN   \n",
      "1           2  2010      1    1     1        NaN            NaN   \n",
      "2           3  2010      1    1     2        NaN            NaN   \n",
      "3           4  2010      1    1     3        NaN            NaN   \n",
      "4           5  2010      1    1     4        NaN            NaN   \n",
      "...       ...   ...    ...  ...   ...        ...            ...   \n",
      "262915  52580  2015     12   31    19        NaN            NaN   \n",
      "262916  52581  2015     12   31    20        NaN            NaN   \n",
      "262917  52582  2015     12   31    21        NaN            NaN   \n",
      "262918  52583  2015     12   31    22        NaN            NaN   \n",
      "262919  52584  2015     12   31    23        NaN            NaN   \n",
      "\n",
      "        PM_Nongzhanguan  PM_US Post  DEWP  ...  city_Shenyang  season_1.0  \\\n",
      "0                   NaN         NaN -21.0  ...          False       False   \n",
      "1                   NaN         NaN -21.0  ...          False       False   \n",
      "2                   NaN         NaN -21.0  ...          False       False   \n",
      "3                   NaN         NaN -21.0  ...          False       False   \n",
      "4                   NaN         NaN -20.0  ...          False       False   \n",
      "...                 ...         ...   ...  ...            ...         ...   \n",
      "262915              NaN       166.0 -10.0  ...           True       False   \n",
      "262916              NaN       259.0 -10.0  ...           True       False   \n",
      "262917              NaN       368.0 -10.0  ...           True       False   \n",
      "262918              NaN       319.0 -10.0  ...           True       False   \n",
      "262919              NaN       275.0  -9.0  ...           True       False   \n",
      "\n",
      "        season_2.0  season_3.0  season_4.0  cbwd_NE  cbwd_NW  cbwd_SE  \\\n",
      "0            False       False        True    False     True    False   \n",
      "1            False       False        True    False     True    False   \n",
      "2            False       False        True    False     True    False   \n",
      "3            False       False        True    False     True    False   \n",
      "4            False       False        True    False     True    False   \n",
      "...            ...         ...         ...      ...      ...      ...   \n",
      "262915       False       False        True    False    False     True   \n",
      "262916       False       False        True    False    False     True   \n",
      "262917       False       False        True    False    False     True   \n",
      "262918       False       False        True    False    False     True   \n",
      "262919       False       False        True    False    False     True   \n",
      "\n",
      "        cbwd_SW  cbwd_cv  \n",
      "0         False    False  \n",
      "1         False    False  \n",
      "2         False    False  \n",
      "3         False    False  \n",
      "4         False    False  \n",
      "...         ...      ...  \n",
      "262915    False    False  \n",
      "262916    False    False  \n",
      "262917    False    False  \n",
      "262918    False    False  \n",
      "262919    False    False  \n",
      "\n",
      "[262920 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "df_all_encoded = pd.get_dummies(df_all, columns=features_list_cat)\n",
    "print(df_all_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_COLUMNS_LIST: ['DEWP', 'TEMP', 'HUMI', 'PRES', 'Iws', 'precipitation', 'Iprec', 'city_Beijing', 'city_Chengdu', 'city_Guangzhou', 'city_Shanghai', 'city_Shenyang', 'season_1.0', 'season_2.0', 'season_3.0', 'season_4.0', 'cbwd_NE', 'cbwd_NW', 'cbwd_SE', 'cbwd_SW', 'cbwd_cv']\n",
      "columns_list: ['PM_US Post', 'DEWP', 'TEMP', 'HUMI', 'PRES', 'Iws', 'precipitation', 'Iprec', 'city_Beijing', 'city_Chengdu', 'city_Guangzhou', 'city_Shanghai', 'city_Shenyang', 'season_1.0', 'season_2.0', 'season_3.0', 'season_4.0', 'cbwd_NE', 'cbwd_NW', 'cbwd_SE', 'cbwd_SW', 'cbwd_cv']\n"
     ]
    }
   ],
   "source": [
    "cat_columns_list = []\n",
    "for f in features_list_cat:\n",
    "    col_list = [col for col in df_all_encoded.columns if col.startswith(f)]\n",
    "    cat_columns_list = cat_columns_list + col_list\n",
    "\n",
    "X_COLUMNS_LIST = features_list_con + cat_columns_list\n",
    "columns_list = [TARGET] + features_list_con + cat_columns_list\n",
    "\n",
    "print(\"X_COLUMNS_LIST:\", X_COLUMNS_LIST)\n",
    "print(\"columns_list:\", columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PM_US Post  DEWP  TEMP   HUMI    PRES     Iws  precipitation  Iprec  \\\n",
      "35064         53.0 -20.0   7.0  12.00  1014.0  143.48            0.0    0.0   \n",
      "35065         65.0 -20.0   7.0  12.00  1013.0  147.50            0.0    0.0   \n",
      "35066         70.0 -20.0   6.0  13.00  1013.0  151.52            0.0    0.0   \n",
      "35067         79.0 -20.0   6.0  13.00  1013.0  153.31            0.0    0.0   \n",
      "35068         92.0 -18.0   3.0  19.00  1012.0    0.89            0.0    0.0   \n",
      "...            ...   ...   ...    ...     ...     ...            ...    ...   \n",
      "254155        38.0 -20.0 -10.0  43.68  1026.0   51.00            0.0    0.0   \n",
      "254156        36.0 -21.0 -10.0  40.04  1026.0   55.00            0.0    0.0   \n",
      "254157        31.0 -21.0 -11.0  43.36  1027.0   59.00            0.0    0.0   \n",
      "254158        30.0 -21.0 -11.0  43.36  1027.0   64.00            0.0    0.0   \n",
      "254159        26.0 -21.0 -12.0  46.98  1027.0   68.00            0.0    0.0   \n",
      "\n",
      "        city_Beijing  city_Chengdu  ...  city_Shenyang  season_1.0  \\\n",
      "35064           True         False  ...          False       False   \n",
      "35065           True         False  ...          False       False   \n",
      "35066           True         False  ...          False       False   \n",
      "35067           True         False  ...          False       False   \n",
      "35068           True         False  ...          False       False   \n",
      "...              ...           ...  ...            ...         ...   \n",
      "254155         False         False  ...           True       False   \n",
      "254156         False         False  ...           True       False   \n",
      "254157         False         False  ...           True       False   \n",
      "254158         False         False  ...           True       False   \n",
      "254159         False         False  ...           True       False   \n",
      "\n",
      "        season_2.0  season_3.0  season_4.0  cbwd_NE  cbwd_NW  cbwd_SE  \\\n",
      "35064        False       False        True    False     True    False   \n",
      "35065        False       False        True    False     True    False   \n",
      "35066        False       False        True    False     True    False   \n",
      "35067        False       False        True    False     True    False   \n",
      "35068        False       False        True    False    False    False   \n",
      "...            ...         ...         ...      ...      ...      ...   \n",
      "254155       False       False        True    False     True    False   \n",
      "254156       False       False        True    False     True    False   \n",
      "254157       False       False        True    False     True    False   \n",
      "254158       False       False        True    False     True    False   \n",
      "254159       False       False        True    False     True    False   \n",
      "\n",
      "        cbwd_SW  cbwd_cv  \n",
      "35064     False    False  \n",
      "35065     False    False  \n",
      "35066     False    False  \n",
      "35067     False    False  \n",
      "35068     False     True  \n",
      "...         ...      ...  \n",
      "254155    False    False  \n",
      "254156    False    False  \n",
      "254157    False    False  \n",
      "254158    False    False  \n",
      "254159    False    False  \n",
      "\n",
      "[43800 rows x 22 columns]\n",
      "        PM_US Post  DEWP  TEMP   HUMI    PRES    Iws  precipitation  Iprec  \\\n",
      "43824         22.0 -21.0  -6.0  29.00  1034.0   0.89            0.0    0.0   \n",
      "43825          9.0 -22.0  -4.0  23.00  1034.0   4.92            0.0    0.0   \n",
      "43826          9.0 -21.0  -5.0  27.00  1034.0   8.94            0.0    0.0   \n",
      "43827         13.0 -21.0  -6.0  29.00  1035.0  12.96            0.0    0.0   \n",
      "43828         10.0 -21.0  -5.0  27.00  1034.0  16.98            0.0    0.0   \n",
      "...            ...   ...   ...    ...     ...    ...            ...    ...   \n",
      "262915       166.0 -10.0  -9.0  92.42  1031.0   2.00            0.0    0.0   \n",
      "262916       259.0 -10.0  -7.0  79.10  1030.0   5.00            0.0    0.0   \n",
      "262917       368.0 -10.0  -7.0  79.10  1030.0   8.00            0.0    0.0   \n",
      "262918       319.0 -10.0  -7.0  79.10  1028.0  11.00            NaN    NaN   \n",
      "262919       275.0  -9.0  -6.0  79.26  1028.0  12.00            0.0    0.0   \n",
      "\n",
      "        city_Beijing  city_Chengdu  ...  city_Shenyang  season_1.0  \\\n",
      "43824           True         False  ...          False       False   \n",
      "43825           True         False  ...          False       False   \n",
      "43826           True         False  ...          False       False   \n",
      "43827           True         False  ...          False       False   \n",
      "43828           True         False  ...          False       False   \n",
      "...              ...           ...  ...            ...         ...   \n",
      "262915         False         False  ...           True       False   \n",
      "262916         False         False  ...           True       False   \n",
      "262917         False         False  ...           True       False   \n",
      "262918         False         False  ...           True       False   \n",
      "262919         False         False  ...           True       False   \n",
      "\n",
      "        season_2.0  season_3.0  season_4.0  cbwd_NE  cbwd_NW  cbwd_SE  \\\n",
      "43824        False       False        True    False    False     True   \n",
      "43825        False       False        True    False     True    False   \n",
      "43826        False       False        True    False     True    False   \n",
      "43827        False       False        True    False     True    False   \n",
      "43828        False       False        True    False     True    False   \n",
      "...            ...         ...         ...      ...      ...      ...   \n",
      "262915       False       False        True    False    False     True   \n",
      "262916       False       False        True    False    False     True   \n",
      "262917       False       False        True    False    False     True   \n",
      "262918       False       False        True    False    False     True   \n",
      "262919       False       False        True    False    False     True   \n",
      "\n",
      "        cbwd_SW  cbwd_cv  \n",
      "43824     False    False  \n",
      "43825     False    False  \n",
      "43826     False    False  \n",
      "43827     False    False  \n",
      "43828     False    False  \n",
      "...         ...      ...  \n",
      "262915    False    False  \n",
      "262916    False    False  \n",
      "262917    False    False  \n",
      "262918    False    False  \n",
      "262919    False    False  \n",
      "\n",
      "[43800 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "df_2014 = df_all_encoded[df_all[\"year\"] == 2014][columns_list]\n",
    "df_2015 = df_all_encoded[df_all[\"year\"] == 2015][columns_list]\n",
    "print(df_2014)\n",
    "print(df_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 欠損値除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # 各列に対して、pd.to_numericを使用して数値に変換（エラーがあればNaNにする）\n",
    "    df_numeric = data.apply(lambda col: pd.to_numeric(col, errors=\"coerce\"))\n",
    "\n",
    "    # 数値に変換できなかった行を抽出（NaNを含む行）\n",
    "    df_excluded = data[df_numeric.isna().any(axis=1)]\n",
    "\n",
    "    # NaNを含む行を削除\n",
    "    df_clean = df_numeric.dropna()\n",
    "\n",
    "    return df_excluded, df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ数（訓練）： 43800\n",
      "全データ数（テスト）： 43800\n",
      "欠損値除去後のデータ数（訓練）： 40334\n",
      "欠損値除去後のデータ数（テスト）： 39098\n"
     ]
    }
   ],
   "source": [
    "_, df_2014_clean = remove_nan(df_2014)\n",
    "_, df_2015_clean = remove_nan(df_2015)\n",
    "\n",
    "print(\"全データ数（訓練）：\", len(df_2014))\n",
    "print(\"全データ数（テスト）：\", len(df_2015))\n",
    "print(\"欠損値除去後のデータ数（訓練）：\", len(df_2014_clean))\n",
    "print(\"欠損値除去後のデータ数（テスト）：\", len(df_2015_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 外れ値除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(\n",
    "    data: pd.DataFrame, metric: str, threshold: float\n",
    ") -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欠損値と外れ値除外後のデータ数（訓練）: 38559\n",
      "欠損値と外れ値除外後のデータ数（テスト）: 37237\n"
     ]
    }
   ],
   "source": [
    "df_train = remove_outliers_zscore(\n",
    "    data=df_2014_clean, metric=TARGET, threshold=THRESHOLD\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)  # 行を詰める\n",
    "df_test = remove_outliers_zscore(data=df_2015_clean, metric=TARGET, threshold=THRESHOLD)\n",
    "df_test = df_test.reset_index(drop=True)  # 行を詰める\n",
    "\n",
    "print(\"欠損値と外れ値除外後のデータ数（訓練）:\", len(df_train))\n",
    "print(\"欠損値と外れ値除外後のデータ数（テスト）:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 X, y に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[X_COLUMNS_LIST].to_numpy()\n",
    "y_train = df_train[TARGET].to_numpy()\n",
    "\n",
    "X_test = df_test[X_COLUMNS_LIST].to_numpy()\n",
    "y_test = df_test[TARGET].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "# 4. 共変量の選択（訓練データ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM_US Postと最も相関の高い変数: TEMP, 相関係数: -0.226\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train)\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "correlations = X_train_df.corrwith(y_train_series)\n",
    "\n",
    "# 最も相関が高い変数の選択\n",
    "most_correlated_var_index = correlations.abs().idxmax()  # 絶対値が最大の変数を取得\n",
    "max_correlation = correlations[most_correlated_var_index]\n",
    "\n",
    "print(\n",
    "    f\"{TARGET}と最も相関の高い変数: {X_COLUMNS_LIST[most_correlated_var_index]}, 相関係数: {round(max_correlation, 3)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "# 5．CUPED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 αの計算（訓練データ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha :  -9.673673177441103\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_cov = scaler.fit_transform(X_train[:, [most_correlated_var_index]])\n",
    "alpha = np.cov(y_train, scaled_cov[:, 0])[0, 1] / np.var(scaled_cov[:, 0])\n",
    "\n",
    "print(\"alpha : \", alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 CUPEDの実施（テストデータ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_var_cuped(\n",
    "    X: NDArray,\n",
    "    y: NDArray,\n",
    "    alpha: float,\n",
    "    most_correlated_var_index: int,\n",
    "    sample_size: int,\n",
    "    n_trials: int,\n",
    ") -> float:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_cov = scaler.fit_transform(X[:, [most_correlated_var_index]])\n",
    "    y_cuped = y - alpha * scaled_cov[:, 0]\n",
    "\n",
    "    sample_means = np.zeros(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        rng = np.random.RandomState(i)\n",
    "        sample = rng.choice(y_cuped, sample_size, replace=False)\n",
    "        sample_means[i] = sample.mean()\n",
    "\n",
    "    # 標本分散を計算\n",
    "    variance = np.var(sample_means)\n",
    "\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# 6. COSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_var_coss(\n",
    "    X: NDArray,\n",
    "    y: NDArray,\n",
    "    most_correlated_var_index: int,\n",
    "    sample_size: int,\n",
    "    n_trials: int,\n",
    ") -> float:\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    sample_means_t = np.zeros(n_trials)\n",
    "    sample_means_c = np.zeros(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        rng = np.random.RandomState(i)\n",
    "        rows = rng.choice(X.index, sample_size * 2, replace=False)\n",
    "\n",
    "        sample_X = X.iloc[rows, most_correlated_var_index]\n",
    "        sample_X_sorted = sample_X.sort_values(ascending=False)\n",
    "        # print(\"rows:\", rows)\n",
    "        # print(\"sample_X_sorted:\", sample_X_sorted.values)\n",
    "\n",
    "        rows_t = sample_X_sorted.index[::2]\n",
    "        rows_c = sample_X_sorted.index[1::2]\n",
    "        # print(\"rows_t:\", rows_t)\n",
    "        # print(\"rows_c:\", rows_c)\n",
    "\n",
    "        sample_t = y.loc[rows_t].to_numpy()\n",
    "        sample_c = y.loc[rows_c].to_numpy()\n",
    "\n",
    "        sample_means_t[i] = sample_t.mean()\n",
    "        sample_means_c[i] = sample_c.mean()\n",
    "\n",
    "    variance_t = np.var(sample_means_t)\n",
    "    variance_c = np.var(sample_means_c)\n",
    "\n",
    "    return variance_t, variance_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# 7. 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_var_random(y: NDArray, n_trials: int, sample_size) -> float:\n",
    "    y_hats = []\n",
    "    for i in range(n_trials):\n",
    "        rng = np.random.RandomState(i)\n",
    "        sample = rng.choice(y, sample_size, replace=False)\n",
    "        y_hat_random = sample.mean()\n",
    "        y_hats.append(y_hat_random)\n",
    "    var_random = np.array(y_hats).var()\n",
    "\n",
    "    return var_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_RANDOM_TRAIN = cauculate_var_random(\n",
    "    y=y_train, n_trials=N_TRIALS, sample_size=SAMPLE_SIZE\n",
    ")\n",
    "VAR_RANDOM_TEST = cauculate_var_random(\n",
    "    y=y_test, n_trials=N_TRIALS, sample_size=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "var_cuped_train = cauculate_var_cuped(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    alpha=alpha,\n",
    "    most_correlated_var_index=most_correlated_var_index,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    n_trials=N_TRIALS,\n",
    ")\n",
    "\n",
    "var_cuped_test = cauculate_var_cuped(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    alpha=alpha,\n",
    "    most_correlated_var_index=most_correlated_var_index,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    n_trials=N_TRIALS,\n",
    ")\n",
    "\n",
    "var_coss_train_t, var_coss_train_c = cauculate_var_coss(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    most_correlated_var_index=most_correlated_var_index,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    n_trials=N_TRIALS,\n",
    ")\n",
    "\n",
    "var_coss_test_t, var_coss_test_c = cauculate_var_coss(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    most_correlated_var_index=most_correlated_var_index,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    n_trials=N_TRIALS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ数（訓練）： 43800\n",
      "全データ数（テスト）： 43800\n",
      "欠損値除去後のデータ数（訓練）： 40334\n",
      "欠損値除去後のデータ数（テスト）： 39098\n",
      "欠損値と外れ値除外後のデータ数（訓練）: 38559\n",
      "欠損値と外れ値除外後のデータ数（テスト）: 37237\n"
     ]
    }
   ],
   "source": [
    "print(\"全データ数（訓練）：\", len(df_2014))\n",
    "print(\"全データ数（テスト）：\", len(df_2015))\n",
    "print(\"欠損値除去後のデータ数（訓練）：\", len(df_2014_clean))\n",
    "print(\"欠損値除去後のデータ数（テスト）：\", len(df_2015_clean))\n",
    "print(\"欠損値と外れ値除外後のデータ数（訓練）:\", len(df_train))\n",
    "print(\"欠損値と外れ値除外後のデータ数（テスト）:\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_random_train = 0.1361919773905536\n",
      "var_random_test = 0.11073560986541511\n"
     ]
    }
   ],
   "source": [
    "print(\"var_random_train =\", VAR_RANDOM_TRAIN)\n",
    "print(\"var_random_test =\", VAR_RANDOM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUPEDの分散削減率 :  8.410437248072212\n",
      "COSSの分散削減率(test) :  8.11564399714234\n",
      "COSSの分散削減率(control) :  7.775600319189113\n"
     ]
    }
   ],
   "source": [
    "# CUPED の分散削減率\n",
    "reduction_rate_cuped = (1 - var_cuped_test / VAR_RANDOM_TEST) * 100\n",
    "print(\"CUPEDの分散削減率 : \", reduction_rate_cuped)\n",
    "\n",
    "reduction_rate_coss_t = (1 - var_coss_test_t / VAR_RANDOM_TEST) * 100\n",
    "reduction_rate_coss_c = (1 - var_coss_test_c / VAR_RANDOM_TEST) * 100\n",
    "print(\"COSSの分散削減率(test) : \", reduction_rate_coss_t)\n",
    "print(\"COSSの分散削減率(control) : \", reduction_rate_coss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cuped_train = 0.1289392297039616\n",
      "var_cuped_test = 0.10142226088641432\n"
     ]
    }
   ],
   "source": [
    "print(\"var_cuped_train =\", var_cuped_train)\n",
    "print(\"var_cuped_test =\", var_cuped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_coss_train = 0.13113642173877987\n",
      "var_coss_test = 0.1017487019906736\n"
     ]
    }
   ],
   "source": [
    "print(\"var_coss_train =\", var_coss_train_t)\n",
    "print(\"var_coss_test =\", var_coss_test_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
