{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量的データ用（For quantitaive data）\n",
    "[Methods]  \n",
    "- Clustering methods : Kmeans, GMM, FSSEM, FSS-Kmeans    \n",
    "- Allocation methods : random, proportional allocation, post stratification, optimal allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### ライブラリのインポート＆その他の設定（Importing Libraries & Other Settings）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn関連\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "# 抽象基底クラス (ABC)\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# タイピングのサポート\n",
    "from typing import Optional\n",
    "\n",
    "# シード設定\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "# seed_everything(8)\n",
    "\n",
    "# 可視化の設定\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import itertools\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### 実験設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2  # 外れ値除外の閾値\n",
    "\n",
    "SAMPLE_SIZE = 100  # 標本サイズ\n",
    "DATA_SIZE = 100000  # データサイズ(df2)\n",
    "N_TRIALS = 100000  # 試行回数（標本平均を求める回数）\n",
    "N_EXPERIMENT = 3  # 分散を求める回数\n",
    "m_VALUE = 2  # 各クラスタの最小標本数(最適標本配分)\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "\n",
    "ALLOCATION_METHODS = [\n",
    "    \"Proportional\",\n",
    "    \"Post\",\n",
    "    \"Optimal\",\n",
    "]  # Randomはいれない\n",
    "CLUSTERING_METHOD = \"kmeans\"  # \"gmm\" or \"kmeans\" or \"xmeans\"\n",
    "\n",
    "if CLUSTERING_METHOD == \"kmeans\":\n",
    "    CLUSTERING_METHOD_NAME = \"Proposal-Kmeans\"\n",
    "if CLUSTERING_METHOD == \"xmeans\":\n",
    "    CLUSTERING_METHOD_NAME == \"Proposal-Xmeans\"\n",
    "if CLUSTERING_METHOD == \"gmm\":\n",
    "    CLUSTERING_METHOD_NAME == \"Proposal-GMM\"\n",
    "\n",
    "# クラスタを固定する場合(gmm, kmeans)\n",
    "N_CLUSTERS = 10  # クラスタ数\n",
    "# クラスタ数を固定しない場合（xmeans)\n",
    "K_MIN = 2\n",
    "K_MAX = 10\n",
    "\n",
    "\n",
    "MAXIMUM_FEATURES_TO_SELECT = 17  # 選択される最大の特徴量数\n",
    "\n",
    "SELECTING_FEATURES = \"all_features\"  # 特徴量をmaxになるまで選ぶ⇒\"all_features\" or 基準値が最大になるまで⇒\"not_all_features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### データの前処理（Data Preprocessing）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 外れ値を除去する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(\n",
    "    data: pd.DataFrame, metric: str, threshold: float = 2\n",
    ") -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### メルカリデータ（df1：全部, df2：一部）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1(all the data)\n",
    "df1 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\aug_first_cpn_data_for_ab_test_sensibility_tsukuba.csv\"\n",
    ")\n",
    "obj1 = \"GMV\"\n",
    "features_list1 = [\n",
    "    \"hist_4_day_buy_num\",\n",
    "    \"hist_4_day_gmv\",\n",
    "    \"his_4_day_is_buy\",\n",
    "    \"hist_30_day_buy_days\",\n",
    "    \"hist_30_day_buy_num\",\n",
    "    \"hist_30_day_gmv\",\n",
    "    \"hist_30_day_buy_recency\",\n",
    "    \"hist_30_day_pay_days\",\n",
    "    \"hist_30_day_atpu\",\n",
    "    \"hist_30_day_gpv\",\n",
    "    \"hist_30_day_pay_recency\",\n",
    "    \"hist_30_day_list_days\",\n",
    "    \"hist_30_day_list_num\",\n",
    "    \"hist_30_day_list_recency\",\n",
    "    \"hist_30_day_like_count\",\n",
    "    \"hist_30_day_like_count_not_deleted\",\n",
    "    \"hist_30_day_like_recency\",\n",
    "]\n",
    "\n",
    "# df2(subset of the data)\n",
    "df2 = df1.iloc[:DATA_SIZE]\n",
    "obj2 = obj1\n",
    "features_list2 = features_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 外れ値除去と標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2  # choose from (df1, df2)\n",
    "obj = obj2  # choose from (obj1, obj2)\n",
    "features_list = features_list2  # choose from (features_list1, features_list2)\n",
    "\n",
    "# 外れ値除去\n",
    "df = remove_outliers_zscore(data=df, metric=obj, threshold=THRESHOLD)\n",
    "\n",
    "# 標準化\n",
    "X = df[features_list]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=features_list)\n",
    "\n",
    "# 目的変数\n",
    "y = df[obj]\n",
    "\n",
    "# 行を詰める\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"外れ値除外前のデータ数:\", DATA_SIZE)\n",
    "print(\"外れ値除外後のデータ数:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### 単純無作為抽出（Random）の誤差分散を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STD_LIST = []\n",
    "for exp in range(N_EXPERIMENT):\n",
    "    y_hats = []\n",
    "    np.random.seed(exp)\n",
    "    for random_state in range(N_TRIALS):\n",
    "        sample = np.random.choice(y, SAMPLE_SIZE, replace=False)\n",
    "        y_hat_random = sample.mean()\n",
    "        y_hats.append(y_hat_random)\n",
    "    errors = np.array(y_hats) - y.mean()\n",
    "    random_std = errors.var()\n",
    "    RANDOM_STD_LIST.append(random_std)\n",
    "print(RANDOM_STD_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### 提案手法でクラスタリングと特徴量選択と標本配分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Allocation_in_Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Allocation_in_Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        maximum_features_to_select: int,\n",
    "        n_clusters: int,\n",
    "        clustering_method: str = \"kmeans\",\n",
    "        allocation_methods: list[str] = [\"Proportional\"],\n",
    "        sample_size: int = 10,\n",
    "        n_trials: int = 100,\n",
    "        m_value=2,\n",
    "        M: Optional[NDArray] = None,\n",
    "        random_state: int = 0,\n",
    "        selecting_features: str = \"all_features\",\n",
    "        k_min: int = 2,\n",
    "        k_max: int = 10,\n",
    "        random_allocation_std: float = 0.1,\n",
    "    ):\n",
    "        self.maximum_features_to_select = maximum_features_to_select\n",
    "        self.n_clusters = n_clusters\n",
    "        self.clustering_method = clustering_method\n",
    "        self.allocation_methods = allocation_methods\n",
    "        self.sample_size = sample_size\n",
    "        self.n_trials = n_trials\n",
    "        self.m_value = m_value\n",
    "        self.M = M\n",
    "        self.random_state = random_state\n",
    "        self.selecting_features = selecting_features\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.random_allocation_std = random_allocation_std\n",
    "\n",
    "    def fss(self, X: pd.DataFrame, y: pd.DataFrame) -> \"Allocation_in_Wrapper\":\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_all_features = X.shape[1]  # 総特徴量数\n",
    "        self.final_selected_features_dict = {}  # キー：標本配分手法, 値：最終的に選ばれた特徴量のリスト\n",
    "        self.final_cluster_assignments_dict = {}  # キー：標本配分手法, 値：最終的なクラスタラベル\n",
    "        self.features_score_dict_dict = {}  # キー：標本配分手法, 値：features_score_dict\n",
    "        self.features_error_variance_dict_dict = {}  # キー：標本配分手法, 値：features_error_variance_dict\n",
    "        self.final_n_clusters_dict = {}  # キー：標本配分手法, 値：最終的なクラスタ数\n",
    "        self.final_score_dict = {}\n",
    "        self.final_error_variance_dict = {}\n",
    "\n",
    "        # 特徴量選択\n",
    "        for allocation_method in self.allocation_methods:\n",
    "            print(\"[\", allocation_method, \"]\")\n",
    "\n",
    "            features_score_dict = {}\n",
    "            features_error_variance_dict = {}\n",
    "\n",
    "            # 選ばれた特徴量と残っている特徴量の初期化\n",
    "            current_features = []\n",
    "            remaining_features = list(range(n_all_features))\n",
    "\n",
    "            if self.selecting_features == \"not_all_features\":\n",
    "                best_score = -np.inf\n",
    "\n",
    "            while len(current_features) < self.maximum_features_to_select:\n",
    "                if self.selecting_features == \"all_features\":\n",
    "                    best_score = -np.inf\n",
    "\n",
    "                best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "                for feature in remaining_features:\n",
    "                    temp_features = current_features + [\n",
    "                        feature\n",
    "                    ]  # 特徴量をひとつ加え、score計算\n",
    "                    score, error_variance, n_clusters = self.crit(\n",
    "                        X[:, temp_features], y, allocation_method\n",
    "                    )\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_error_variance = error_variance\n",
    "                        best_feature = feature\n",
    "                        best_labels = self.N_cluster_label\n",
    "                        best_N_cluster_size = self.N_cluster_size\n",
    "                        best_n_clusters = n_clusters\n",
    "                        best_n_cluster_size = self.n_cluster_size\n",
    "\n",
    "                if best_feature is not None:\n",
    "                    current_features.append(best_feature)\n",
    "                    num_of_features = len(current_features)\n",
    "                    print(\n",
    "                        \"num_of_features:\",\n",
    "                        num_of_features,\n",
    "                        \"current_features:\",\n",
    "                        current_features,\n",
    "                        \", score:\",\n",
    "                        best_score,\n",
    "                        \"best_n_clusters:\",\n",
    "                        best_n_clusters,\n",
    "                        \"best_N_cluster_size:\",\n",
    "                        best_N_cluster_size,\n",
    "                        \"best_n_cluster_size:\",\n",
    "                        best_n_cluster_size,\n",
    "                    )\n",
    "\n",
    "                    features_score_dict[str(num_of_features)] = best_score  # 確認用\n",
    "                    features_error_variance_dict[str(num_of_features)] = (\n",
    "                        best_error_variance\n",
    "                    )\n",
    "\n",
    "                    remaining_features.remove(best_feature)\n",
    "                    self.selected_features_ = current_features\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            self.final_cluster_assignments_dict[allocation_method] = best_labels\n",
    "            self.final_n_clusters_dict[allocation_method] = best_n_clusters\n",
    "            self.final_selected_features_dict[allocation_method] = (\n",
    "                self.selected_features_\n",
    "            )\n",
    "            self.features_score_dict_dict[allocation_method] = features_score_dict\n",
    "            self.features_error_variance_dict_dict[allocation_method] = (\n",
    "                features_error_variance_dict\n",
    "            )\n",
    "            self.final_score_dict[allocation_method] = best_score\n",
    "            self.final_error_variance_dict[allocation_method] = best_error_variance\n",
    "\n",
    "        return self\n",
    "\n",
    "    def crit(self, X: pd.DataFrame, y: pd.DataFrame, allocation_method: str) -> float:\n",
    "        # クラスタリング手法がGMMの場合\n",
    "        if self.clustering_method == \"gmm\":\n",
    "            model = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                init_params=\"kmeans\",\n",
    "            )\n",
    "        # クラスタリング手法がKMEANSの場合\n",
    "        if self.clustering_method == \"kmeans\":\n",
    "            model = KMeans(\n",
    "                n_clusters=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        if self.clustering_method == \"xmeans\":\n",
    "            model = self.xmeans(X)\n",
    "\n",
    "        model.fit(X)\n",
    "        self.N_cluster_label = model.predict(X)\n",
    "        self.N_cluster_size = np.bincount(self.N_cluster_label)\n",
    "        n_clusters = len(np.unique(self.N_cluster_label))\n",
    "\n",
    "        error_variance_reduction_rate, error_variance = self.cauculate_reduction_rate(\n",
    "            X, y, allocation_method\n",
    "        )\n",
    "        score = error_variance_reduction_rate\n",
    "\n",
    "        return score, error_variance, n_clusters\n",
    "\n",
    "    def compute_bic(self, X: pd.DataFrame, kmeans):\n",
    "        # クラスタリング結果の取得\n",
    "        labels = kmeans.labels_\n",
    "        clusters = kmeans.cluster_centers_\n",
    "        n_clusters = len(clusters)\n",
    "        n = len(X)\n",
    "        m = X.shape[1]\n",
    "        # クラスター内の分散の合計\n",
    "        variance = np.sum(\n",
    "            [\n",
    "                np.linalg.norm(X[labels == i] - clusters[i], axis=1).sum()\n",
    "                for i in range(n_clusters)\n",
    "            ]\n",
    "        )\n",
    "        # BIC の計算\n",
    "        bic = np.log(n) * n_clusters * m + n * np.log(variance / n)\n",
    "        return bic\n",
    "\n",
    "    def xmeans(self, X: pd.DataFrame):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=self.k_min, random_state=self.random_state\n",
    "        )  # 初期クラスター数で KMeans を実行\n",
    "        kmeans.fit(X)\n",
    "        best_bic = self.compute_bic(X, kmeans)\n",
    "        best_kmeans = kmeans\n",
    "        for k in range(self.k_min + 1, self.k_max + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.random_state)\n",
    "            kmeans.fit(X)\n",
    "            # BIC を計算\n",
    "            current_bic = self.compute_bic(X, kmeans)\n",
    "            # BIC が改善される場合、クラスタ数を更新\n",
    "            if current_bic < best_bic:\n",
    "                best_bic = current_bic\n",
    "                best_kmeans = kmeans\n",
    "        # 最適なクラスタリング結果を返す\n",
    "        return best_kmeans\n",
    "\n",
    "    def cauculate_reduction_rate(\n",
    "        self, X: pd.DataFrame, y: pd.DataFrame, allocation_method: str\n",
    "    ) -> float:\n",
    "        # 各戦略の標本数に基づいて目的変数の平均を推定\n",
    "        y_hats = []\n",
    "        for random_state in range(self.n_trials):\n",
    "            if allocation_method == \"Post\":\n",
    "                y_hat = self.estimate_y_mean_post(X, y)\n",
    "            else:\n",
    "                y_hat = self.estimate_y_mean_other(X, y, allocation_method)\n",
    "            y_hats.append(\n",
    "                {\n",
    "                    \"policy\": allocation_method,\n",
    "                    \"y_hat\": y_hat,\n",
    "                    \"random_state\": random_state,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        y_hat_df = pd.DataFrame(y_hats)\n",
    "        y_hat_df[\"error\"] = (\n",
    "            y_hat_df[\"y_hat\"] - y.mean()\n",
    "        )  # 真の平均からの誤差をerrorカラムに追加\n",
    "\n",
    "        non_random_allocation_std = y_hat_df[\"error\"].var()\n",
    "\n",
    "        # 削減率\n",
    "        reduction_rate = (\n",
    "            1 - non_random_allocation_std / self.random_allocation_std\n",
    "        ) * 100\n",
    "\n",
    "        return reduction_rate, non_random_allocation_std\n",
    "\n",
    "    def estimate_y_mean_post(self, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "        n_cluster_size = np.array([self.sample_size])\n",
    "        weights = self.N_cluster_size / self.N_cluster_size.sum()\n",
    "        y_hat = 0\n",
    "        indices = np.arange(self.N_cluster_size.sum())\n",
    "        y_array = np.array(y.tolist())\n",
    "        n_indices = np.random.choice(indices, n_cluster_size[0], replace=False)\n",
    "        n_label = np.array([self.N_cluster_label[i] for i in n_indices])\n",
    "        n_new_labels = np.unique(n_label)\n",
    "        self.n_cluster_size = np.bincount(n_label)\n",
    "        for h in n_new_labels:\n",
    "            index = np.where(n_label == h)[0]\n",
    "            sample = y_array[n_indices[index]]\n",
    "            y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "            y_hat += y_sample_mean * weights[h]\n",
    "        return y_hat\n",
    "\n",
    "    def estimate_y_mean_other(\n",
    "        self, X: pd.DataFrame, y: pd.DataFrame, allocation_method: str\n",
    "    ) -> float:\n",
    "        if allocation_method == \"Proportional\":\n",
    "            self.n_cluster_size = self.ProportionalAllocation(X, y)\n",
    "        if allocation_method == \"Optimal\":\n",
    "            self.n_cluster_size = self.OptimalAllocation(X, y)\n",
    "\n",
    "        weights = self.N_cluster_size / self.N_cluster_size.sum()\n",
    "        y_hat = 0\n",
    "        for h in range(self.n_cluster_size.shape[0]):\n",
    "            if self.n_cluster_size[h] != 0:\n",
    "                y_cluster = y[self.N_cluster_label == h]\n",
    "                if self.n_cluster_size[h] <= len(y_cluster):\n",
    "                    sample: NDArray = np.random.choice(\n",
    "                        y_cluster, self.n_cluster_size[h], replace=False\n",
    "                    )\n",
    "                    y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "                    y_hat += y_sample_mean * weights[h]\n",
    "        return y_hat\n",
    "\n",
    "    def ProportionalAllocation(self, X: pd.DataFrame, y: pd.DataFrame) -> NDArray:\n",
    "        n_cluster_size: NDArray = np.round(\n",
    "            self.N_cluster_size / self.N_cluster_size.sum() * self.sample_size\n",
    "        ).astype(int)\n",
    "\n",
    "        if n_cluster_size.sum() > self.sample_size:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n_cluster_size[np.argmax(n_cluster_size)] -= (\n",
    "                n_cluster_size.sum() - self.sample_size\n",
    "            )\n",
    "        if n_cluster_size.sum() < self.sample_size:\n",
    "            # nの合計がn_samplesより小さい場合は一番標本数が多いクラスタにたす\n",
    "            n_cluster_size[np.argmax(n_cluster_size)] += (\n",
    "                -n_cluster_size.sum() + self.sample_size\n",
    "            )\n",
    "        return n_cluster_size\n",
    "\n",
    "    def OptimalAllocation(\n",
    "        self, X: pd.DataFrame, y: pd.DataFrame\n",
    "    ) -> NDArray:  # たぶんあってる\n",
    "        n_cluster = len(np.unique(self.N_cluster_label))\n",
    "        self.m = np.full(n_cluster, self.m_value)\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array([np.var(y[self.N_cluster_label == h]) for h in range(n_cluster)])\n",
    "        d = (self.N_cluster_size**2) * S\n",
    "\n",
    "        n_cluster_size = self.m.copy()  # 初期値\n",
    "\n",
    "        M = self.M.copy() if self.M is not None else self.N_cluster_size.copy()\n",
    "        I = np.arange(n_cluster)  # noqa #クラスタのインデックス   ###############self.n_clusters\n",
    "        while (n_cluster_size.sum() != self.sample_size) and len(I) != 0:\n",
    "            delta = np.zeros(n_cluster)  ############self.n_clusters\n",
    "            delta[I] = (d / (n_cluster_size + 1) - d / n_cluster_size)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n_cluster_size[h_star] + 1 <= M[h_star]:\n",
    "                n_cluster_size[h_star] = n_cluster_size[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        # 制約チェック\n",
    "        assert (\n",
    "            n_cluster_size.sum() <= self.sample_size\n",
    "        ), f\"Total sample size is over than {self.sample_size}\"\n",
    "        assert np.all(\n",
    "            n_cluster_size >= self.m\n",
    "        ), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n_cluster_size <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\"\n",
    "\n",
    "        return n_cluster_size\n",
    "\n",
    "    def get_final_selected_features_dict(self) -> dict[NDArray]:\n",
    "        return self.final_selected_features_dict  # 選択された特徴量のインデックス\n",
    "\n",
    "    def get_final_cluster_assignments(self) -> dict[NDArray]:\n",
    "        return self.final_cluster_assignments_dict  # 最終的なクラスタリング結果\n",
    "\n",
    "    def get_final_n_clusters_dict(self) -> dict:\n",
    "        return self.final_n_clusters_dict\n",
    "\n",
    "    def get_final_score_dict(self) -> dict:\n",
    "        return self.final_score_dict\n",
    "\n",
    "    def get_final_error_variance_dict(self) -> dict:\n",
    "        return self.final_error_variance_dict\n",
    "\n",
    "    def get_features_score_dict_dict(self) -> dict[dict]:\n",
    "        return self.features_score_dict_dict\n",
    "\n",
    "    def get_features_error_variance_dict_dict(self) -> dict[dict]:\n",
    "        return self.features_error_variance_dict_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reduction_rate_dict = {}\n",
    "all_reduction_rate_dict[\"Random\"] = [0 for i in range(N_EXPERIMENT)]\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    all_reduction_rate_dict[allocation_method] = []\n",
    "\n",
    "all_error_variance_dict = {}\n",
    "all_error_variance_dict[\"Random\"] = RANDOM_STD_LIST\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    all_error_variance_dict[allocation_method] = []\n",
    "\n",
    "features_score_dict_dict_list = []\n",
    "features_error_variance_dict_dict_list = []\n",
    "\n",
    "for exp in range(N_EXPERIMENT):\n",
    "    np.random.seed(exp)\n",
    "    ins = Allocation_in_Wrapper(\n",
    "        maximum_features_to_select=MAXIMUM_FEATURES_TO_SELECT,\n",
    "        n_clusters=N_CLUSTERS,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "        allocation_methods=ALLOCATION_METHODS,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        n_trials=N_TRIALS,\n",
    "        m_value=m_VALUE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        selecting_features=SELECTING_FEATURES,\n",
    "        k_min=K_MIN,\n",
    "        k_max=K_MAX,\n",
    "        random_allocation_std=RANDOM_STD_LIST[exp],\n",
    "    )\n",
    "\n",
    "    ins.fss(X_scaled, y)\n",
    "    selected_features_index = ins.get_final_selected_features_dict()\n",
    "    cluster_label = ins.get_final_cluster_assignments()\n",
    "    cluster_size = np.unique(cluster_label, return_counts=True)[1]\n",
    "    features_score_dict_dict = ins.get_features_score_dict_dict()\n",
    "    features_error_variance_dict_dict = ins.get_features_error_variance_dict_dict()\n",
    "    features_score_dict_dict_list.append(features_score_dict_dict)\n",
    "    features_error_variance_dict_dict_list.append(features_error_variance_dict_dict)\n",
    "\n",
    "    final_score_dict = ins.get_final_score_dict()\n",
    "    final_error_variance_dict = ins.get_final_error_variance_dict()\n",
    "\n",
    "    for allocation_method in ALLOCATION_METHODS:\n",
    "        all_reduction_rate_dict[allocation_method].append(\n",
    "            final_score_dict[allocation_method]\n",
    "        )\n",
    "        all_error_variance_dict[allocation_method].append(\n",
    "            final_error_variance_dict[allocation_method]\n",
    "        )\n",
    "\n",
    "\n",
    "mean_reduction_rate_dict = {}\n",
    "mean_reduction_rate_dict[\"Random\"] = 0\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    mean_reduction_rate_dict[allocation_method] = sum(\n",
    "        all_reduction_rate_dict[allocation_method]\n",
    "    ) / len(all_reduction_rate_dict[allocation_method])\n",
    "\n",
    "mean_error_variance_dict = {}\n",
    "mean_error_variance_dict[\"Random\"] = sum(RANDOM_STD_LIST) / N_EXPERIMENT\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    mean_error_variance_dict[allocation_method] = sum(\n",
    "        all_error_variance_dict[allocation_method]\n",
    "    ) / len(all_error_variance_dict[allocation_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差分散削減率の推移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in range(N_EXPERIMENT):\n",
    "    # グラフ作成\n",
    "    fig, axs = plt.subplots(\n",
    "        1, len(ALLOCATION_METHODS), figsize=(8 * len(ALLOCATION_METHODS), 5)\n",
    "    )  # サブプロットを作成\n",
    "\n",
    "    plot_idx = 0\n",
    "    for allocation, dict in features_score_dict_dict_list[exp].items():\n",
    "        if len(features_score_dict_dict_list[exp]) == 1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[plot_idx]\n",
    "        bars = ax.bar(dict.keys(), dict.values())\n",
    "        for key, value in dict.items():\n",
    "            rounded_value = round(value, 2)\n",
    "            ax.text(key, value + 0.3, str(rounded_value), ha=\"center\", fontsize=18)\n",
    "        ax.set_title(f\"{allocation}\", fontsize=40)\n",
    "        ax.set_xlabel(\"Number of features\", fontsize=30)\n",
    "        ax.set_ylabel(\"EV reduction rate\", fontsize=30)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=25)\n",
    "        ax.set_ylim([0, 40])  # Y軸の範囲を0から20に設定\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # グラフ作成\n",
    "    fig, axs = plt.subplots(\n",
    "        1, len(ALLOCATION_METHODS), figsize=(8 * len(ALLOCATION_METHODS), 5)\n",
    "    )  # サブプロットを作成\n",
    "\n",
    "    plot_idx = 0\n",
    "    for allocation, dict in features_error_variance_dict_dict_list[exp].items():\n",
    "        if len(features_error_variance_dict_dict_list[exp]) == 1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[plot_idx]\n",
    "        bars = ax.bar(dict.keys(), dict.values())\n",
    "        for key, value in dict.items():\n",
    "            rounded_value = round(value, 2)\n",
    "            # ax.text(key, value + 10, str(rounded_value), ha=\"center\", fontsize=15)\n",
    "        random_bar = ax.bar(\"ran\", RANDOM_STD_LIST[exp], color=\"blue\")\n",
    "        # ax.text(\"random\", random_std + 0.3, str(round(random_std, 2)), ha=\"center\")\n",
    "        ax.set_title(f\"{allocation}\", fontsize=40)\n",
    "        ax.set_xlabel(\"Number of features\", fontsize=30)\n",
    "        ax.set_ylabel(\"EV\", fontsize=30)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=25)\n",
    "        ax.set_ylim([0, 300000])  # Y軸の範囲を0から20に設定\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 箱ひげ図"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 誤差分散削減率の箱ひげ図\n",
    "fig, axs = plt.subplots(1, 1, figsize=(8, 5))  # サブプロットを作成\n",
    "\n",
    "ax = axs\n",
    "allo_data = list(all_reduction_rate_dict.values())  # allo1, allo2, allo3 のデータリスト\n",
    "allo_labels = list(all_reduction_rate_dict.keys())  # alloのラベルリスト\n",
    "ax.boxplot(allo_data, labels=allo_labels)\n",
    "# グラフタイトルを設定\n",
    "ax.set_title(\n",
    "    f\"{CLUSTERING_METHOD_NAME}\",\n",
    "    fontsize=40,\n",
    ")\n",
    "ax.set_ylabel(\"EV reduction rate\", fontsize=30)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "ax.set_ylim(-10, 35)\n",
    "\n",
    "# グラフを表示\n",
    "plt.tight_layout()  # レイアウトを調整\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 誤差分散の箱ひげ図\n",
    "fig, axs = plt.subplots(1, 1, figsize=(8, 5))  # サブプロットを作成\n",
    "\n",
    "ax = axs\n",
    "allo_data = list(all_error_variance_dict.values())  # allo1, allo2, allo3 のデータリスト\n",
    "allo_labels = list(all_error_variance_dict.keys())  # alloのラベルリスト\n",
    "ax.boxplot(allo_data, labels=allo_labels)\n",
    "# グラフタイトルを設定\n",
    "ax.set_title(f\"{CLUSTERING_METHOD_NAME}\", fontsize=40)\n",
    "ax.set_ylabel(\"EV\", fontsize=30)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "ax.set_ylim(0, 50000)\n",
    "\n",
    "# グラフを表示\n",
    "plt.tight_layout()  # レイアウトを調整\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 全実験の平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ作成\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=(8, 5),\n",
    ")  # サブプロットを作成\n",
    "\n",
    "ax = axs\n",
    "allo_data = list(\n",
    "    mean_reduction_rate_dict.values()\n",
    ")  # allo1, allo2, allo3 のデータリスト\n",
    "allo_labels = list(mean_reduction_rate_dict.keys())  # alloのラベルリスト\n",
    "bars = ax.bar(allo_labels, allo_data)\n",
    "# 各バーの上に値を表示\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()  # 各バーの高さ（値）\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        round(yval, 2),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=30,\n",
    "    )  # 値をバーの上に表示\n",
    "    # plt.ylabel(\"誤差分散削減率 (%)\")\n",
    "ax.set_ylabel(\"EV reduction rate\", fontsize=30)\n",
    "ax.set_ylim(0, 40)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "ax.set_title(\n",
    "    f\"{CLUSTERING_METHOD_NAME}\", fontsize=40\n",
    ")  # (number of clusters: {clusters})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# グラフ作成\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=(8, 5),\n",
    ")  # サブプロットを作成\n",
    "\n",
    "ax = axs\n",
    "allo_data = list(\n",
    "    mean_error_variance_dict.values()\n",
    ")  # allo1, allo2, allo3 のデータリスト\n",
    "allo_labels = list(mean_error_variance_dict.keys())  # alloのラベルリスト\n",
    "\n",
    "# bars = ax.bar(allo_labels, allo_data, color=\"tab:red\")\n",
    "bars = ax.bar(\n",
    "    allo_labels,\n",
    "    allo_data,\n",
    "    color=[\"blue\" if label == \"Random\" else \"tab:red\" for label in allo_labels],\n",
    ")\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()  # 各バーの高さ（値）\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        round(yval, 2),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=30,\n",
    "    )  # 値をバーの上に表示\n",
    "ax.set_ylabel(\"EV\", fontsize=30)\n",
    "ax.set_ylim(0, 300000)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "ax.set_title(f\"{CLUSTERING_METHOD_NAME}\", fontsize=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
