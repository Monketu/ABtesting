{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# １．ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn関連\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "# 抽象基底クラス (ABC)\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# タイピングのサポート\n",
    "from typing import Optional\n",
    "\n",
    "# 可視化の設定\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import itertools\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# 計算時間\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# ２．実験設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 実験設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 実験データの設定 ###\n",
    "TARGET = \"PM_US Post\"\n",
    "THRESHOLD = 2  # 外れ値除外の閾値\n",
    "\n",
    "### 実験設定 ###\n",
    "SAMPLE_SIZE = 10000  # 標本サイズ\n",
    "N_TRIALS = 10000  # 試行回数（標本平均を求める回数）\n",
    "m_VALUE = 2  # 最適標本配分における各クラスタの最小標本数\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "SEED = 0\n",
    "\n",
    "### 実験方法 ###\n",
    "# クラスタリング\n",
    "CLUSTERING_METHOD = \"kmeans\"  # \"gmm\" or \"kmeans\" or \"xmeans\"\n",
    "N_CLUSTERS = 8  # クラスタ数\n",
    "K_MIN = 2\n",
    "K_MAX = 10\n",
    "\n",
    "# 特徴量選択\n",
    "ALLOCATION_METHODS = [\n",
    "    \"Proportional\",\n",
    "    \"Post\",\n",
    "    \"Optimal\",\n",
    "]\n",
    "SELECT_MAXIMUM_FEATURES = \"yes\"  # \"yes\" or \"no\"（特徴量数が MAXIMUM_FEATURES_TO_SELECT になるまで選ぶかいなか）\n",
    "MAXIMUM_FEATURES_TO_SELECT = 10  # 選択される最大の特徴量(特徴量選択ありの場合)\n",
    "\n",
    "\n",
    "### 可視化 ###\n",
    "TITLE_SIZE = 20\n",
    "LABEL_SIZE = 15\n",
    "TICK_SIZE = 12.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 手法の名前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTERING_METHOD == \"kmeans\":\n",
    "    METHOD_NAME = \"SFS-Kmeans-Var\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# ３．データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Beijing = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\BeijingPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Chengdu = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ChengduPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Guangzhou = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\GuangzhouPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Shanghai = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ShanghaiPM20100101_20151231.csv\"\n",
    "# )\n",
    "# df_Shenyang = pd.read_csv(\n",
    "#     R\"C:\\Users\\HaruMomozu\\Documents\\中国気象データ\\ShenyangPM20100101_20151231.csv\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing = pd.read_csv(\n",
    "    R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\BeijingPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Chengdu = pd.read_csv(\n",
    "    R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ChengduPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Guangzhou = pd.read_csv(\n",
    "    R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\GuangzhouPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Shanghai = pd.read_csv(\n",
    "    R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ShanghaiPM20100101_20151231.csv\"\n",
    ")\n",
    "df_Shenyang = pd.read_csv(\n",
    "    R\"C:\\Users\\tsuki\\Documents\\研究室\\研究\\ABテスト実装\\オンラインデータ\\中国気象データ\\ShenyangPM20100101_20151231.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing[\"city\"] = \"Beijing\"\n",
    "df_Chengdu[\"city\"] = \"Chengdu\"\n",
    "df_Guangzhou[\"city\"] = \"Guangzhou\"\n",
    "df_Shanghai[\"city\"] = \"Shanghai\"\n",
    "df_Shenyang[\"city\"] = \"Shenyang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(\n",
    "    [df_Beijing, df_Chengdu, df_Guangzhou, df_Shanghai, df_Shenyang],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "features_list_con = [\n",
    "    \"DEWP\",\n",
    "    \"TEMP\",\n",
    "    \"HUMI\",\n",
    "    \"PRES\",\n",
    "    \"Iws\",\n",
    "    \"precipitation\",\n",
    "    \"Iprec\",\n",
    "]\n",
    "features_list_cat = [\"city\", \"season\", \"cbwd\"]\n",
    "\n",
    "features_list = features_list_con + features_list_cat\n",
    "variables_list = [TARGET] + features_list_con + features_list_cat\n",
    "\n",
    "print(variables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_encoded = pd.get_dummies(df_all, columns=features_list_cat)\n",
    "print(df_all_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_dict = {}\n",
    "cat_columns_list = []\n",
    "for f in features_list_cat:\n",
    "    col_list = [col for col in df_all_encoded.columns if col.startswith(f)]\n",
    "    cat_columns_dict[f] = col_list\n",
    "    cat_columns_list = cat_columns_list + col_list\n",
    "\n",
    "columns_list_features = features_list_con + cat_columns_list\n",
    "columns_list = [TARGET] + features_list_con + cat_columns_list\n",
    "\n",
    "print(\"cat_columns_dict:\", cat_columns_dict)\n",
    "print(\"cat_columns_list:\", cat_columns_list)\n",
    "print(\"columns_list_features:\", columns_list_features)\n",
    "print(\"columns_list:\", columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = df_all_encoded[df_all[\"year\"] == 2014][columns_list]\n",
    "df_2015 = df_all_encoded[df_all[\"year\"] == 2015][columns_list]\n",
    "print(df_2014)\n",
    "print(df_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 欠損値除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # 各列に対して、pd.to_numericを使用して数値に変換（エラーがあればNaNにする）\n",
    "    df_numeric = data.apply(lambda col: pd.to_numeric(col, errors=\"coerce\"))\n",
    "\n",
    "    # 数値に変換できなかった行を抽出（NaNを含む行）\n",
    "    df_excluded = data[df_numeric.isna().any(axis=1)]\n",
    "\n",
    "    # NaNを含む行を削除\n",
    "    df_clean = df_numeric.dropna()\n",
    "\n",
    "    return df_excluded, df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_2014_clean = remove_nan(df_2014)\n",
    "_, df_2015_clean = remove_nan(df_2015)\n",
    "\n",
    "print(\"全データ数（訓練）：\", len(df_2014))\n",
    "print(\"全データ数（テスト）：\", len(df_2015))\n",
    "print(\"欠損値除去後のデータ数（訓練）：\", len(df_2014_clean))\n",
    "print(\"欠損値除去後のデータ数（テスト）：\", len(df_2015_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 外れ値除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(\n",
    "    data: pd.DataFrame, metric: str, threshold: float\n",
    ") -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = remove_outliers_zscore(\n",
    "    data=df_2014_clean, metric=TARGET, threshold=THRESHOLD\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)  # 行を詰める\n",
    "df_test = remove_outliers_zscore(data=df_2015_clean, metric=TARGET, threshold=THRESHOLD)\n",
    "df_test = df_test.reset_index(drop=True)  # 行を詰める\n",
    "\n",
    "print(\"欠損値と外れ値除外後のデータ数（訓練）:\", len(df_train))\n",
    "print(\"欠損値と外れ値除外後のデータ数（テスト）:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 X, yに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[columns_list_features].to_numpy()\n",
    "y_train = df_train[TARGET].to_numpy()\n",
    "\n",
    "X_test = df_test[columns_list_features].to_numpy()\n",
    "y_test = df_test[TARGET].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# 訓練データに基づいてfit\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# ４．特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DICT = {}\n",
    "SELECTED_FEATURES_DICT = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocation_in_Wrapper Class で特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Allocation_in_Wrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        maximum_features_to_select: int,\n",
    "        n_clusters: int,\n",
    "        clustering_method: str = \"kmeans\",\n",
    "        random_state: int = 0,\n",
    "        select_maximum_features: str = \"yes\",\n",
    "        k_min: int = 2,\n",
    "        k_max: int = 10,\n",
    "        allocation_method: str = \"Proportional\",\n",
    "        sample_size: int = 10,\n",
    "        n_trials: int = 100,\n",
    "        m_value=2,\n",
    "        M: Optional[NDArray] = None,\n",
    "    ):\n",
    "        self.maximum_features_to_select = maximum_features_to_select\n",
    "        self.n_clusters = n_clusters\n",
    "        self.clustering_method = clustering_method\n",
    "        self.random_state = random_state\n",
    "        self.select_maximum_features = select_maximum_features\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.allocation_method = allocation_method\n",
    "        self.sample_size = sample_size\n",
    "        self.n_trials = n_trials\n",
    "        self.m_value = m_value\n",
    "        self.M = M\n",
    "\n",
    "    def fss(self, X: pd.DataFrame, y: pd.DataFrame) -> \"Allocation_in_Wrapper\":\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_all_features = X.shape[1]  # 総特徴量数\n",
    "\n",
    "        features_score_dict = {}\n",
    "\n",
    "        # 選ばれた特徴量と残っている特徴量の初期化\n",
    "        current_features = []\n",
    "        remaining_features = list(range(n_all_features))\n",
    "\n",
    "        if self.select_maximum_features == \"no\":\n",
    "            best_score = -np.inf\n",
    "\n",
    "        while len(current_features) < self.maximum_features_to_select:\n",
    "            best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "            if self.select_maximum_features == \"yes\":\n",
    "                best_score = -np.inf\n",
    "\n",
    "            for feature in remaining_features:\n",
    "                # temp_features = current_features + [\n",
    "                #     feature\n",
    "                # ]  # 特徴量をひとつ加え、score計算\n",
    "                \n",
    "                score, n_clusters = self.crit(X[:, temp_columns], y)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "                    best_N_cluster_size = self.N_cluster_size\n",
    "                    best_n_clusters = n_clusters\n",
    "                    best_n_cluster_size = self.n_cluster_size\n",
    "\n",
    "            if best_feature is not None:\n",
    "                current_features.append(best_feature)\n",
    "                remaining_features.remove(best_feature)\n",
    "                num_of_features = len(current_features)\n",
    "                print(\n",
    "                    \"num_of_features:\",\n",
    "                    num_of_features,\n",
    "                    \"current_features:\",\n",
    "                    current_features,\n",
    "                    \", score:\",\n",
    "                    best_score,\n",
    "                    \"best_n_clusters:\",\n",
    "                    best_n_clusters,\n",
    "                    \"best_N_cluster_size:\",\n",
    "                    best_N_cluster_size,\n",
    "                    \"best_n_cluster_size:\",\n",
    "                    best_n_cluster_size,\n",
    "                )\n",
    "\n",
    "                features_score_dict[str(num_of_features)] = best_score  # 確認用\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        self.selected_features_index = current_features\n",
    "        self.features_score_dict = features_score_dict\n",
    "\n",
    "        return self\n",
    "\n",
    "    def crit(self, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "        # クラスタリング手法がGMMの場合\n",
    "        if self.clustering_method == \"gmm\":\n",
    "            model = GaussianMixture(\n",
    "                n_components=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                init_params=\"kmeans\",\n",
    "            )\n",
    "        # クラスタリング手法がKMEANSの場合\n",
    "        if self.clustering_method == \"kmeans\":\n",
    "            model = KMeans(\n",
    "                n_clusters=self.n_clusters,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "\n",
    "        model.fit(X)\n",
    "        self.N_cluster_label = model.predict(X)\n",
    "        self.N_cluster_size = np.bincount(self.N_cluster_label)\n",
    "        n_clusters = len(np.unique(self.N_cluster_label))\n",
    "\n",
    "        if self.allocation_method == \"Proportional\":\n",
    "            var = self.cauculate_var_proportional(X, y)\n",
    "        if self.allocation_method == \"Post\":\n",
    "            var = self.cauculate_var_proportional(X, y)\n",
    "        if self.allocation_method == \"Optimal\":\n",
    "            var = self.cauculate_var_optimal(X, y)\n",
    "        score = -var\n",
    "\n",
    "        return score, n_clusters\n",
    "\n",
    "    def cauculate_var_proportional(self, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "        self.n_cluster_size = self.proportional(X, y)\n",
    "        H = np.where(self.n_cluster_size > 0)[0]\n",
    "        N = np.sum(np.isin(self.N_cluster_label, H))\n",
    "        S = np.array([np.var(y[self.N_cluster_label == h]) for h in H])\n",
    "        n_h = self.n_cluster_size[H]\n",
    "        N_h = self.N_cluster_size[H]\n",
    "        var = (1 / int(N) ** 2) * (np.sum((N_h**2 * S) / n_h) - np.sum(N_h * S))\n",
    "        return var\n",
    "\n",
    "    def cauculate_var_optimal(self, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "        self.n_cluster_size, H = self.optimal(X, y)\n",
    "        N = np.sum(np.isin(self.N_cluster_label, H))\n",
    "        S = np.array([np.var(y[self.N_cluster_label == h]) for h in H])\n",
    "        n_h = self.n_cluster_size\n",
    "        N_h = self.N_cluster_size[H]\n",
    "        var = (1 / int(N) ** 2) * ((np.sum((N_h**2 * S) / n_h)) - np.sum(N_h * S))\n",
    "\n",
    "        return var\n",
    "\n",
    "    def proportional(self, X: pd.DataFrame, y: pd.DataFrame) -> NDArray:\n",
    "        n_cluster_size: NDArray = np.round(\n",
    "            self.N_cluster_size / self.N_cluster_size.sum() * self.sample_size\n",
    "        ).astype(int)\n",
    "\n",
    "        if n_cluster_size.sum() > self.sample_size:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n_cluster_size[np.argmax(n_cluster_size)] -= (\n",
    "                n_cluster_size.sum() - self.sample_size\n",
    "            )\n",
    "        if n_cluster_size.sum() < self.sample_size:\n",
    "            # nの合計がn_samplesより小さい場合は一番標本数が多いクラスタにたす\n",
    "            n_cluster_size[np.argmax(n_cluster_size)] += (\n",
    "                -n_cluster_size.sum() + self.sample_size\n",
    "            )\n",
    "        return n_cluster_size\n",
    "\n",
    "    def optimal(self, X: pd.DataFrame, y: pd.DataFrame) -> NDArray:\n",
    "        cluster_size_all = np.bincount(self.N_cluster_label)\n",
    "        cluster_size_for_optimal = []\n",
    "        labels_for_optimal = []\n",
    "        for i in range(len(cluster_size_all)):\n",
    "            if cluster_size_all[i] >= 2:\n",
    "                cluster_size_for_optimal.append(cluster_size_all[i])\n",
    "                labels_for_optimal.append(i)\n",
    "        n_clusters_for_optimal = len(cluster_size_for_optimal)\n",
    "        cluster_size_for_optimal = np.array(cluster_size_for_optimal)\n",
    "\n",
    "        self.m = np.full(n_clusters_for_optimal, self.m_value)\n",
    "\n",
    "        S = np.array([np.var(y[self.N_cluster_label == h]) for h in labels_for_optimal])\n",
    "        d = (cluster_size_for_optimal**2) * S\n",
    "\n",
    "        n_cluster_size = self.m.copy()  # 初期値\n",
    "        M = self.M.copy() if self.M is not None else cluster_size_for_optimal.copy()\n",
    "        I = np.arange(n_clusters_for_optimal)  # noqa #クラスタのインデックス\n",
    "\n",
    "        while (n_cluster_size.sum() != self.sample_size) and len(I) != 0:\n",
    "            delta = np.zeros(n_clusters_for_optimal)\n",
    "            delta[I] = (d / (n_cluster_size + 1) - d / n_cluster_size)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n_cluster_size[h_star] + 1 <= M[h_star]:\n",
    "                n_cluster_size[h_star] = n_cluster_size[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        # 制約チェック\n",
    "        assert (\n",
    "            n_cluster_size.sum() <= self.sample_size\n",
    "        ), f\"Total sample size is over than {self.sample_size}\"\n",
    "        assert np.all(\n",
    "            n_cluster_size >= self.m\n",
    "        ), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n_cluster_size <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\"\n",
    "\n",
    "        return n_cluster_size, labels_for_optimal\n",
    "\n",
    "    def get_selected_features_index(self):\n",
    "        return self.selected_features_index  # 選択された特徴量のインデックス\n",
    "\n",
    "    def get_features_score_dict(self):\n",
    "        return self.features_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_allocation_in_wrapper(\n",
    "    instance: \"Allocation_in_Wrapper\", X: NDArray, y: NDArray\n",
    ") -> tuple[list, dict[int, float]]:\n",
    "    instance.fss(X, y)\n",
    "    selected_features_index = instance.get_selected_features_index()\n",
    "    features_score_dict = instance.get_features_score_dict()\n",
    "    selected_features_index = np.array(selected_features_index)\n",
    "\n",
    "    return selected_features_index, features_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インスタンスのリスト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    instances.append(\n",
    "        (\n",
    "            allocation_method,\n",
    "            Allocation_in_Wrapper(\n",
    "                maximum_features_to_select=MAXIMUM_FEATURES_TO_SELECT,\n",
    "                n_clusters=N_CLUSTERS,\n",
    "                clustering_method=CLUSTERING_METHOD,\n",
    "                random_state=RANDOM_STATE,\n",
    "                select_maximum_features=SELECT_MAXIMUM_FEATURES,\n",
    "                k_min=K_MIN,\n",
    "                k_max=K_MAX,\n",
    "                allocation_method=allocation_method,\n",
    "                sample_size=SAMPLE_SIZE,\n",
    "                n_trials=N_TRIALS,\n",
    "                m_value=m_VALUE,\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocaiton in Wrapper の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_score_dict_dict = {}\n",
    "\n",
    "# 各インスタンスに対して処理を実行\n",
    "for allocation_method, instance in instances:\n",
    "    print(\"[\", allocation_method, \"]\")\n",
    "    start_time = time.time()\n",
    "    selected_features_index, features_score_dict = process_allocation_in_wrapper(\n",
    "        instance, X_train_std, y_train\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    SELECTED_FEATURES_DICT[allocation_method] = selected_features_index\n",
    "    TIME_DICT[allocation_method] = end_time - start_time\n",
    "\n",
    "    features_score_dict_dict[allocation_method] = features_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_score_dict_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 特徴量選択の結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スコア（-分散）の推移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for allocation_method, instance in instances:\n",
    "    # 棒グラフを作成\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(\n",
    "        features_score_dict_dict[allocation_method].keys(),\n",
    "        features_score_dict_dict[allocation_method].values(),\n",
    "    )\n",
    "    plt.title(f\"{allocation_method}\", fontsize=TITLE_SIZE)\n",
    "    plt.xlabel(\"Number of features\", fontsize=LABEL_SIZE)\n",
    "    plt.ylabel(\"Evaluation value\", fontsize=LABEL_SIZE)\n",
    "    # plt.ylim(,)\n",
    "    plt.tick_params(axis=\"both\", labelsize=TICK_SIZE)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散削減率の推移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_var_random(y: NDArray, seed: int, n_trials: int, sample_size) -> float:\n",
    "    np.random.seed(seed)\n",
    "    y_hats = []\n",
    "    for i in range(n_trials):\n",
    "        sample = np.random.choice(y, sample_size, replace=False)\n",
    "        y_hat_random = sample.mean()\n",
    "        y_hats.append(y_hat_random)\n",
    "    var_random = np.array(y_hats).var()\n",
    "\n",
    "    return var_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_RANDOM_TRAIN = cauculate_var_random(\n",
    "    y=y_train, seed=SEED, n_trials=N_TRIALS, sample_size=SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_reduction_rate_dict_dict = {}\n",
    "for allocation_method, score_dict in features_score_dict_dict.items():\n",
    "    features_reduction_rate_dict_dict[allocation_method] = {}\n",
    "    for n_features, score in score_dict.items():\n",
    "        reduction_rate = (1 - (-score / VAR_RANDOM_TRAIN)) * 100\n",
    "        features_reduction_rate_dict_dict[allocation_method][n_features] = (\n",
    "            reduction_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for allocation_method, instance in instances:\n",
    "    # 棒グラフを作成\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(\n",
    "        features_reduction_rate_dict_dict[allocation_method].keys(),\n",
    "        features_reduction_rate_dict_dict[allocation_method].values(),\n",
    "    )\n",
    "    plt.title(f\"{allocation_method}\", fontsize=TITLE_SIZE)\n",
    "    plt.xlabel(\"Number of features\", fontsize=LABEL_SIZE)\n",
    "    plt.ylabel(\"Var reduction rate\", fontsize=LABEL_SIZE)\n",
    "    plt.ylim(0, 40)\n",
    "    plt.tick_params(axis=\"both\", labelsize=TICK_SIZE)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量選択の結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"【{allocation_method}】\")\n",
    "    print(\"選択された特徴量：\", SELECTED_FEATURES_DICT[allocation_method])\n",
    "    print(f\"特徴量選択にかかった時間：{round(TIME_DICT[allocation_method], 3)} s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ５．クラスタリングと標本配分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bic(X: NDArray, kmeans: KMeans) -> float:\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "    n_clusters = len(clusters)\n",
    "    n = len(X)\n",
    "    m = X.shape[1]\n",
    "    variance = np.sum(\n",
    "        [\n",
    "            np.linalg.norm(X[labels == i] - clusters[i], axis=1).sum()\n",
    "            for i in range(n_clusters)\n",
    "        ]\n",
    "    )\n",
    "    bic = np.log(n) * n_clusters * m + n * np.log(variance / n)\n",
    "    return bic\n",
    "\n",
    "\n",
    "def xmeans(X: NDArray, k_min: int, k_max: int) -> KMeans:\n",
    "    kmeans = KMeans(n_clusters=k_min, random_state=RANDOM_STATE)\n",
    "    kmeans.fit(X)\n",
    "    best_bic = compute_bic(X, kmeans)\n",
    "    best_kmeans = kmeans\n",
    "    for k in range(k_min + 1, k_max + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
    "        kmeans.fit(X)\n",
    "        current_bic = compute_bic(X, kmeans)\n",
    "        if current_bic < best_bic:\n",
    "            best_bic = current_bic\n",
    "            best_kmeans = kmeans\n",
    "    return best_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(\n",
    "    X_train: NDArray,\n",
    "    X_test: NDArray,\n",
    "    allocation_methods: list,\n",
    "    clustering_method: str,\n",
    "    n_clusters: int,\n",
    "    random_state: int,\n",
    "    k_min: int,\n",
    "    k_max: int,\n",
    "    selected_features_dict: dict,\n",
    ") -> tuple[dict, dict, dict, dict]:\n",
    "    cluster_label_dict_train = {}\n",
    "    cluster_size_dict_train = {}\n",
    "    cluster_label_dict_test = {}\n",
    "    cluster_size_dict_test = {}\n",
    "\n",
    "    for allocation_method in allocation_methods:\n",
    "        if clustering_method == \"kmeans\":\n",
    "            model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "        if clustering_method == \"gmm\":\n",
    "            model = GaussianMixture(\n",
    "                n_components=n_clusters, random_state=random_state, init_params=\"kmeans\"\n",
    "            )\n",
    "        if clustering_method == \"xmeans\":\n",
    "            model = xmeans(X_train, k_min=k_min, k_max=k_max)\n",
    "\n",
    "        cluster_label_train = model.fit_predict(\n",
    "            X_train[:, selected_features_dict[allocation_method]]\n",
    "        )\n",
    "        cluster_size_train = np.bincount(cluster_label_train)\n",
    "        cluster_label_test = model.predict(\n",
    "            X_test[:, selected_features_dict[allocation_method]]\n",
    "        )\n",
    "        cluster_size_test = np.bincount(cluster_label_test, minlength=n_clusters)\n",
    "\n",
    "        cluster_label_dict_train[allocation_method] = cluster_label_train\n",
    "        cluster_size_dict_train[allocation_method] = cluster_size_train\n",
    "        cluster_label_dict_test[allocation_method] = cluster_label_test\n",
    "        cluster_size_dict_test[allocation_method] = cluster_size_test\n",
    "\n",
    "    return (\n",
    "        cluster_label_dict_train,\n",
    "        cluster_size_dict_train,\n",
    "        cluster_label_dict_test,\n",
    "        cluster_size_dict_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリングの実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    CLUSTER_LABEL_DICT_TRAIN,\n",
    "    CLUSTER_SIZE_DICT_TRAIN,\n",
    "    CLUSTER_LABEL_DICT_TEST,\n",
    "    CLUSTER_SIZE_DICT_TEST,\n",
    ") = clustering(\n",
    "    X_train=X_train_std,\n",
    "    X_test=X_test_std,\n",
    "    allocation_methods=ALLOCATION_METHODS,\n",
    "    clustering_method=CLUSTERING_METHOD,\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    k_min=K_MIN,\n",
    "    k_max=K_MAX,\n",
    "    selected_features_dict=SELECTED_FEATURES_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_cluster_size_dict_for_allocation(\n",
    "    cluster_size_dict: dict[str, NDArray],\n",
    ") -> dict[str, list[int]]:\n",
    "    cluster_size_dict_for_allocation = {}\n",
    "    for allo, cluster_size in cluster_size_dict.items():\n",
    "        if allo == \"Optimal\":\n",
    "            min_n_h = 2\n",
    "        else:\n",
    "            min_n_h = 1\n",
    "        cluster_size_list = []\n",
    "        for i in range(len(cluster_size)):\n",
    "            if cluster_size[i] >= min_n_h:\n",
    "                cluster_size_list.append(cluster_size[i])\n",
    "        cluster_size_dict_for_allocation[allo] = cluster_size_list\n",
    "    return cluster_size_dict_for_allocation\n",
    "\n",
    "\n",
    "def cauculate_unique_label_dict_for_allocation(\n",
    "    cluster_size_dict: dict[str, NDArray],\n",
    ") -> dict[str, list[int]]:\n",
    "    unique_label_dict_for_allocation = {}\n",
    "    for allo, cluster_size in cluster_size_dict.items():\n",
    "        if allo == \"Optimal\":\n",
    "            min_n_h = 2\n",
    "        else:\n",
    "            min_n_h = 1\n",
    "        labels = []\n",
    "        for i in range(len(cluster_size)):\n",
    "            if cluster_size[i] >= min_n_h:\n",
    "                labels.append(i)\n",
    "        unique_label_dict_for_allocation[allo] = labels\n",
    "    return unique_label_dict_for_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SIZE_DICT_FOR_ALLOCATION_TRAIN = cauculate_cluster_size_dict_for_allocation(\n",
    "    cluster_size_dict=CLUSTER_SIZE_DICT_TRAIN\n",
    ")\n",
    "UNIQUE_LABEL_DICT_FOR_ALLOCATION_TRAIN = cauculate_unique_label_dict_for_allocation(\n",
    "    cluster_size_dict=CLUSTER_SIZE_DICT_TRAIN\n",
    ")\n",
    "CLUSTER_SIZE_DICT_FOR_ALLOCATION_TEST = cauculate_cluster_size_dict_for_allocation(\n",
    "    cluster_size_dict=CLUSTER_SIZE_DICT_TEST\n",
    ")\n",
    "UNIQUE_LABEL_DICT_FOR_ALLOCATION_TEST = cauculate_unique_label_dict_for_allocation(\n",
    "    cluster_size_dict=CLUSTER_SIZE_DICT_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリング結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"【訓練データ】\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method}：{CLUSTER_SIZE_DICT_TRAIN[allocation_method]}\")\n",
    "    print(f\"標本配分用：{CLUSTER_SIZE_DICT_FOR_ALLOCATION_TRAIN[allocation_method]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"【テストデータ】\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method}：{CLUSTER_SIZE_DICT_TEST[allocation_method]}\")\n",
    "    print(f\"標本配分用：{CLUSTER_SIZE_DICT_FOR_ALLOCATION_TEST[allocation_method]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 標本配分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基底クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "    # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: int,\n",
    "        random_state: int,\n",
    "        allocation_method: str,\n",
    "        cluster_size_dict_for_allocation: dict,\n",
    "    ):\n",
    "        self.sample_size = sample_size\n",
    "        self.random_state = random_state\n",
    "        self.allocation_method = allocation_method\n",
    "        self.N = np.array(cluster_size_dict_for_allocation[self.allocation_method])\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"標本配分を解く\n",
    "\n",
    "        Args:\n",
    "            X (NDArray): データ (N x M)\n",
    "            y (NDArray): 目的変数 (N)\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "        Note:\n",
    "            M: 特徴量数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 比例配分（Proportional）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proportional(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "        n: NDArray = np.round(self.N / self.N.sum() * self.sample_size).astype(int)\n",
    "\n",
    "        if n.sum() > self.sample_size:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n[np.argmax(n)] -= n.sum() - self.sample_size\n",
    "\n",
    "        if n.sum() < self.sample_size:\n",
    "            # nの合計がn_samplesより小さい場合は一番標本数が多いクラスタにたす\n",
    "            n[np.argmax(n)] += -n.sum() + self.sample_size\n",
    "\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② 事後層化（Post）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.sample_size])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ 最適標本配分（Optimal）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimal(BaseAllocation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: int,\n",
    "        random_state: int,\n",
    "        allocation_method: str,\n",
    "        cluster_label_dict: dict,\n",
    "        cluster_size_dict_for_allocation: dict,\n",
    "        unique_label_dict_for_allocation: dict,\n",
    "        m_value: int,  # 標本サイズ下限\n",
    "        M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sample_size,\n",
    "            random_state,\n",
    "            allocation_method,\n",
    "            cluster_size_dict_for_allocation,\n",
    "        )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "        self.cluster_label = cluster_label_dict[self.allocation_method]\n",
    "        self.N_for_optimal = np.array(cluster_size_dict_for_allocation[\"Optimal\"])\n",
    "        self.labels = unique_label_dict_for_allocation[\"Optimal\"]\n",
    "        self.m_value = m_value  # 各クラスタの最小標本サイズ (H, )\n",
    "        self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array(\n",
    "            [np.var(y[self.cluster_label == h]) for h in self.labels]\n",
    "        )  # 層hのデータが１つのときはS[h]は0になる\n",
    "        d = (self.N_for_optimal**2) * S  # (H, )\n",
    "\n",
    "        self.m = np.full(len(self.labels), self.m_value)\n",
    "        n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "        # 制約チェック\n",
    "        self._check_constraints(n)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "        M = self.M.copy() if self.M is not None else self.N_for_optimal.copy()\n",
    "        I = np.arange(len(self.labels))  # noqa #クラスタのインデックス配列\n",
    "        while (n.sum() != self.sample_size) and len(I) != 0:\n",
    "            delta = np.zeros(len(self.labels))\n",
    "            delta[I] = (d / (n + 1) - d / n)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n[h_star] + 1 <= M[h_star]:\n",
    "                n[h_star] = n[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _check_constraints(self, n: NDArray):\n",
    "        assert (\n",
    "            n.sum() <= self.sample_size\n",
    "        ), f\"Total sample size is over than {self.sample_size}\"\n",
    "        assert np.all(\n",
    "            n >= self.m_value\n",
    "        ), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ポリシーの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policies(\n",
    "    sample_size: int,\n",
    "    random_state: int,\n",
    "    m_value: int,\n",
    "    cluster_label_dict: dict,\n",
    "    cluster_size_dict_for_allocation: list,\n",
    "    unique_label_dict_for_allocation: list,\n",
    "    allocation_methods: list,\n",
    ") -> list[BaseAllocation]:\n",
    "    policies: list[BaseAllocation] = []\n",
    "    if \"Proportional\" in allocation_methods:\n",
    "        policies.append(\n",
    "            Proportional(\n",
    "                sample_size=sample_size,\n",
    "                random_state=random_state,\n",
    "                allocation_method=\"Proportional\",\n",
    "                cluster_size_dict_for_allocation=cluster_size_dict_for_allocation,\n",
    "            )\n",
    "        )\n",
    "    if \"Post\" in allocation_methods:\n",
    "        policies.append(\n",
    "            Post(\n",
    "                sample_size=sample_size,\n",
    "                random_state=random_state,\n",
    "                allocation_method=\"Post\",\n",
    "                cluster_size_dict_for_allocation=cluster_size_dict_for_allocation,\n",
    "            )\n",
    "        )\n",
    "    if \"Optimal\" in allocation_methods:\n",
    "        policies.append(\n",
    "            Optimal(\n",
    "                sample_size=sample_size,\n",
    "                random_state=random_state,\n",
    "                allocation_method=\"Optimal\",\n",
    "                cluster_label_dict=cluster_label_dict,\n",
    "                cluster_size_dict_for_allocation=cluster_size_dict_for_allocation,\n",
    "                unique_label_dict_for_allocation=unique_label_dict_for_allocation,\n",
    "                m_value=m_value,\n",
    "                M=None,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_train = create_policies(\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cluster_label_dict=CLUSTER_LABEL_DICT_TRAIN,\n",
    "    cluster_size_dict_for_allocation=CLUSTER_SIZE_DICT_FOR_ALLOCATION_TRAIN,\n",
    "    unique_label_dict_for_allocation=UNIQUE_LABEL_DICT_FOR_ALLOCATION_TRAIN,\n",
    "    m_value=m_VALUE,\n",
    "    allocation_methods=ALLOCATION_METHODS,\n",
    ")\n",
    "\n",
    "policies_test = create_policies(\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cluster_label_dict=CLUSTER_LABEL_DICT_TEST,\n",
    "    cluster_size_dict_for_allocation=CLUSTER_SIZE_DICT_FOR_ALLOCATION_TEST,\n",
    "    unique_label_dict_for_allocation=UNIQUE_LABEL_DICT_FOR_ALLOCATION_TEST,\n",
    "    m_value=m_VALUE,\n",
    "    allocation_methods=ALLOCATION_METHODS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標本配分の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_allocation_dict(\n",
    "    X: NDArray, y: NDArray, policies: list\n",
    ") -> dict[str, NDArray]:\n",
    "    allocation_dict = {}\n",
    "    for policy in policies:\n",
    "        n = policy.solve(X, y)\n",
    "        allocation_dict[policy.__class__.__name__] = n\n",
    "\n",
    "    return allocation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOCATION_DICT_TRAIN = cauculate_allocation_dict(\n",
    "    X=X_train_std, y=y_train, policies=policies_train\n",
    ")\n",
    "ALLOCATION_DICT_TEST = cauculate_allocation_dict(\n",
    "    X=X_test_std, y=y_test, policies=policies_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標本配分の結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"訓練データ\")\n",
    "print(f\"【{METHOD_NAME}】\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method} : {ALLOCATION_DICT_TRAIN[allocation_method]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"テストデータ\")\n",
    "print(f\"【{METHOD_NAME}】\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method} : {ALLOCATION_DICT_TEST[allocation_method]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# ６．評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 標本平均の分散を計算（単純無作為抽出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_RANDOM_TEST = cauculate_var_random(\n",
    "    y=y_test, seed=SEED, n_trials=N_TRIALS, sample_size=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "print(f\"訓練データでの標本平均の分散 (Random) : {VAR_RANDOM_TRAIN}\")\n",
    "print(f\"テストデータでの標本平均の分散 (Random) : {VAR_RANDOM_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 標本平均の分散を計算（層化抽出）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標本平均を計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_y_mean_proportional(\n",
    "    n: NDArray,\n",
    "    cluster_label: NDArray,\n",
    "    cluster_size_for_proportional: list,\n",
    "    unique_label_for_proportional: list,\n",
    "    y: NDArray,\n",
    ") -> float:\n",
    "    N = np.array(cluster_size_for_proportional)\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    i = -1\n",
    "    for h in unique_label_for_proportional:  # n.shape[0]:層の数\n",
    "        i += 1\n",
    "        if n[i] > 0:\n",
    "            sample: NDArray = np.random.choice(\n",
    "                y[cluster_label == h], n[i], replace=False\n",
    "            )\n",
    "            y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "            y_hat += y_sample_mean * weights[i]\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_optimal(\n",
    "    n: NDArray,\n",
    "    cluster_label: NDArray,\n",
    "    cluster_size_for_optimal: list,\n",
    "    unique_label_for_optimal: list,\n",
    "    y: NDArray,\n",
    ") -> float:\n",
    "    N = np.array(cluster_size_for_optimal)\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    i = -1\n",
    "    for h in unique_label_for_optimal:  # n.shape[0]:層の数\n",
    "        i += 1\n",
    "        sample: NDArray = np.random.choice(y[cluster_label == h], n[i], replace=False)\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[i]\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_post(\n",
    "    n: NDArray,\n",
    "    cluster_label: NDArray,\n",
    "    cluster_size_for_post: list,\n",
    "    unique_label_for_post: list,\n",
    "    y: NDArray,\n",
    ") -> float:\n",
    "    N = np.array(cluster_size_for_post)\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "\n",
    "    all_indices = np.arange(len(y))  # 全データのインデックス\n",
    "    sample_indices = np.random.choice(all_indices, n[0], replace=False)\n",
    "    sample_labels = np.array([cluster_label[i] for i in sample_indices])\n",
    "\n",
    "    for h in np.unique(sample_labels):\n",
    "        index = np.where(sample_labels == h)[0]\n",
    "        sample = y[sample_indices[index]]\n",
    "        y_sample_mean = sample.mean()\n",
    "        y_hat += y_sample_mean * weights[unique_label_for_post.index(h)]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散を計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_var_stratified_dict(\n",
    "    y: NDArray,\n",
    "    seed: int,\n",
    "    n_trials: int,\n",
    "    allocation_dict: dict,\n",
    "    cluster_label_dict: dict,\n",
    "    cluster_size_dict_for_allocation: dict,\n",
    "    unique_label_dict_for_allocation: dict,\n",
    ") -> dict[str, float]:\n",
    "    np.random.seed(seed)\n",
    "    var_stratified_dict = {}\n",
    "\n",
    "    for allocation_method, allocation in allocation_dict.items():\n",
    "        y_hats = []\n",
    "        for i in range(n_trials):\n",
    "            if allocation_method == \"Post\":\n",
    "                y_hat = estimate_y_mean_post(\n",
    "                    n=allocation,\n",
    "                    cluster_label=cluster_label_dict[allocation_method],\n",
    "                    cluster_size_for_post=cluster_size_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    unique_label_for_post=unique_label_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    y=y,\n",
    "                )\n",
    "            elif allocation_method == \"Proportional\":\n",
    "                y_hat = estimate_y_mean_proportional(\n",
    "                    n=allocation,\n",
    "                    cluster_label=cluster_label_dict[allocation_method],\n",
    "                    cluster_size_for_proportional=cluster_size_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    unique_label_for_proportional=unique_label_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    y=y,\n",
    "                )\n",
    "            elif allocation_method == \"Optimal\":\n",
    "                y_hat = estimate_y_mean_optimal(\n",
    "                    n=allocation,\n",
    "                    cluster_label=cluster_label_dict[allocation_method],\n",
    "                    cluster_size_for_optimal=cluster_size_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    unique_label_for_optimal=unique_label_dict_for_allocation[\n",
    "                        allocation_method\n",
    "                    ],\n",
    "                    y=y,\n",
    "                )\n",
    "            y_hats.append(y_hat)\n",
    "        y_hats = np.array(y_hats)\n",
    "        var = y_hats.var()\n",
    "        var_stratified_dict[allocation_method] = var\n",
    "\n",
    "    return var_stratified_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_STRATIFIED_DICT_TRAIN = cauculate_var_stratified_dict(\n",
    "    y=y_train,\n",
    "    seed=SEED,\n",
    "    n_trials=N_TRIALS,\n",
    "    allocation_dict=ALLOCATION_DICT_TRAIN,\n",
    "    cluster_label_dict=CLUSTER_LABEL_DICT_TRAIN,\n",
    "    cluster_size_dict_for_allocation=CLUSTER_SIZE_DICT_FOR_ALLOCATION_TRAIN,\n",
    "    unique_label_dict_for_allocation=UNIQUE_LABEL_DICT_FOR_ALLOCATION_TRAIN,\n",
    ")\n",
    "VAR_STRATIFIED_DICT_TEST = cauculate_var_stratified_dict(\n",
    "    y=y_test,\n",
    "    seed=SEED,\n",
    "    n_trials=N_TRIALS,\n",
    "    allocation_dict=ALLOCATION_DICT_TEST,\n",
    "    cluster_label_dict=CLUSTER_LABEL_DICT_TEST,\n",
    "    cluster_size_dict_for_allocation=CLUSTER_SIZE_DICT_FOR_ALLOCATION_TEST,\n",
    "    unique_label_dict_for_allocation=UNIQUE_LABEL_DICT_FOR_ALLOCATION_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"【{METHOD_NAME}】\")\n",
    "print(\"訓練データでの標本平均の分散\")\n",
    "print(VAR_STRATIFIED_DICT_TRAIN)\n",
    "print(\"テストデータでの標本平均の分散\")\n",
    "print(VAR_STRATIFIED_DICT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 分散削減率の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauculate_reduction_rate_dict(\n",
    "    var_stratified_dict: dict, var_random: float\n",
    ") -> dict[str, float]:\n",
    "    reduction_rate_dict = {}\n",
    "    for allocation_method, var in var_stratified_dict.items():\n",
    "        reduction_rate = (1 - var / var_random) * 100\n",
    "        reduction_rate_dict[allocation_method] = reduction_rate\n",
    "\n",
    "    return reduction_rate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCTION_RATE_DICT_TRAIN = cauculate_reduction_rate_dict(\n",
    "    VAR_STRATIFIED_DICT_TRAIN, VAR_RANDOM_TRAIN\n",
    ")\n",
    "\n",
    "REDUCTION_RATE_DICT_TEST = cauculate_reduction_rate_dict(\n",
    "    VAR_STRATIFIED_DICT_TEST, VAR_RANDOM_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"【{METHOD_NAME}】\")\n",
    "print(\"訓練データでの標本平均の分散の削減率\")\n",
    "print(REDUCTION_RATE_DICT_TRAIN)\n",
    "print(\"テストデータでの標本平均の分散の削減率\")\n",
    "print(REDUCTION_RATE_DICT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 結果のプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロットするための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars(\n",
    "    dicts, var_random, target, method_name, title_size, label_size, tick_size\n",
    ") -> plt.Figure:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 2.5))\n",
    "    ylabels = [\"Var reduction rate\", \"Var\"]\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        data = list(dicts[i].values())\n",
    "        labels = list(dicts[i].keys())\n",
    "        bars = ax.bar(labels, data)\n",
    "        if i == 0:\n",
    "            for bar in bars:\n",
    "                yval = bar.get_height()  # 各バーの高さ（値）\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    yval,\n",
    "                    round(yval, 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=label_size,\n",
    "                )  # 値をバーの上に表示\n",
    "        if i == 1:\n",
    "            ax.bar(\"Random\", var_random, color=\"blue\")\n",
    "        ax.set_title(method_name, fontsize=title_size)\n",
    "        ax.set_ylabel(ylabels[i], fontsize=label_size)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=tick_size)\n",
    "        ax.set_ylim(0, 50)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練データの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(\n",
    "    dicts=[\n",
    "        REDUCTION_RATE_DICT_TRAIN,\n",
    "        VAR_STRATIFIED_DICT_TRAIN,\n",
    "    ],\n",
    "    var_random=VAR_RANDOM_TRAIN,\n",
    "    target=TARGET,\n",
    "    method_name=METHOD_NAME,\n",
    "    title_size=TITLE_SIZE,\n",
    "    label_size=LABEL_SIZE,\n",
    "    tick_size=TICK_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(\n",
    "    dicts=[\n",
    "        REDUCTION_RATE_DICT_TEST,\n",
    "        VAR_STRATIFIED_DICT_TEST,\n",
    "    ],\n",
    "    var_random=VAR_RANDOM_TEST,\n",
    "    target=TARGET,\n",
    "    method_name=METHOD_NAME,\n",
    "    title_size=TITLE_SIZE,\n",
    "    label_size=LABEL_SIZE,\n",
    "    tick_size=TICK_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ\n",
    "print(\"全データ数（訓練）：\", len(df_2014))\n",
    "print(\"全データ数（テスト）：\", len(df_2015))\n",
    "print(\"欠損値除去後のデータ数（訓練）：\", len(df_2014_clean))\n",
    "print(\"欠損値除去後のデータ数（テスト）：\", len(df_2015_clean))\n",
    "print(\"欠損値と外れ値除外後のデータ数（訓練）:\", len(df_train))\n",
    "print(\"欠損値と外れ値除外後のデータ数（テスト）:\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量選択\n",
    "print(f\"【{METHOD_NAME}】\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method}\")\n",
    "    print(\"選択された特徴量：\", SELECTED_FEATURES_DICT[allocation_method])\n",
    "    print(f\"特徴量選択にかかった時間：{round(TIME_DICT[allocation_method], 3)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"【{METHOD_NAME}】\")\n",
    "print(\"訓練データ\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method}：{CLUSTER_SIZE_DICT_TRAIN[allocation_method]}\")\n",
    "\n",
    "print(\"テストデータ\")\n",
    "for allocation_method in ALLOCATION_METHODS:\n",
    "    print(f\"{allocation_method}：{CLUSTER_SIZE_DICT_TEST[allocation_method]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"var_random_train =\", VAR_RANDOM_TRAIN)\n",
    "print(\"var_random_test =\", VAR_RANDOM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"var_once_dict_train =\", VAR_STRATIFIED_DICT_TRAIN)\n",
    "print(\"var_once_dict_test =\", VAR_STRATIFIED_DICT_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
