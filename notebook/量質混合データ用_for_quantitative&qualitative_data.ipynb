{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # データの前処理 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量質混合データ用\n",
    "[Methods]\n",
    "- Clustering methods : KPrototype, FSS-KPrototype\n",
    "- Allocation methods : random, proportional allocation, post stratification, optimal allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 2  # クラスタ数\n",
    "n_features_to_select = 7  # 選択される最大の特徴量数\n",
    "\n",
    "N_SAMPLES = 100  # 標本サイズ\n",
    "H = clusters\n",
    "N_TRIALS = 1000  # 試行回数\n",
    "m_VALUE = 2  # 各クラスタの最小標本数(最適標本配分)\n",
    "RANDOM_STATE = 0  # 乱数シード\n",
    "CRITERION_LIST = [\"none\"]  # \"ml\", \"tr\"は後で入れる\n",
    "CLUSTERING_METHOD_LIST = [\"kprototype\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### ライブラリのインポート & その他の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 8\n"
     ]
    }
   ],
   "source": [
    "# 基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn関連\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "# kmodes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "# 抽象基底クラス (ABC)\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# タイピングのサポート\n",
    "from typing import Optional\n",
    "\n",
    "# シード設定\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(8)\n",
    "\n",
    "# 可視化の設定\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### データの前処理\n",
    "外れ値除去 ⇒ 量的データのみ標準化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 外れ値を除去する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外れ値の除去\n",
    "def remove_outliers_zscore(\n",
    "    data: pd.DataFrame, metric: str, threshold: float = 2\n",
    ") -> pd.DataFrame:\n",
    "    z_scores = np.abs(stats.zscore(data[metric]))\n",
    "    data = data[(z_scores < threshold)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df1 : 健康栄養調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外れ値除外前 (2278, 8)\n",
      "外れ値除外後 (2170, 8)\n",
      "量的データ標準化後 (2170, 8)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\n",
    "    R\"C:\\Users\\HaruMomozu\\Documents\\オンラインデータ\\NHANES_age_prediction.csv\"\n",
    ")\n",
    "obj1 = \"BMXBMI\"\n",
    "features_list1 = [\n",
    "    \"RIDAGEYR\",  # 年齢（連続変数）\n",
    "    \"RIAGENDR\",  # 性別（1:Male, 2:Female)\n",
    "    \"PAQ605\",  # 運動有無(1:日常的に運動する, 2:運動しない)\n",
    "    \"LBXGLU\",  # 断食後の血糖値（連続変数）\n",
    "    \"DIQ010\",  # 糖尿病の有無(0:なし、1:あり)\n",
    "    \"LBXGLT\",  # 口内の健康状態（連続変数）\n",
    "    \"LBXIN\",  # 血中インスリン濃度（連続変数）\n",
    "]\n",
    "\n",
    "df1 = df1.drop(columns=[\"SEQN\", \"age_group\"])\n",
    "print(\"外れ値除外前\", df1.shape)\n",
    "\n",
    "df1 = remove_outliers_zscore(df1, obj1)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "print(\"外れ値除外後\", df1.shape)\n",
    "\n",
    "# 量的データと質的データに分割する\n",
    "numerical_features = [\"RIDAGEYR\", \"LBXGLU\", \"LBXGLT\", \"LBXIN\"]\n",
    "catagorical_features = [\"RIAGENDR\", \"PAQ605\", \"DIQ010\"]\n",
    "X_numerical = df1[numerical_features]\n",
    "X_catagorical = df1[catagorical_features]\n",
    "\n",
    "# 量的データは標準化\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(X_numerical)\n",
    "X_numerical_scaled = pd.DataFrame(features_scaled, columns=numerical_features)\n",
    "\n",
    "X = pd.concat([X_numerical_scaled, X_catagorical], axis=1)\n",
    "y = df1[obj1]\n",
    "\n",
    "df1 = pd.concat([X, y], axis=1)\n",
    "print(\"量的データ標準化後\", df1.shape)\n",
    "\n",
    "X_scaled = df1[features_list1]\n",
    "y = df1[obj1]\n",
    "catColumnsPos = [X_scaled.columns.get_loc(col) for col in catagorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Wrapper 法でクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Wrapper(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_features_to_select: int,\n",
    "#         n_clusters: int,\n",
    "#         criterion: str = \"ml\",\n",
    "#         clustering_method: str = \"kprototype\",\n",
    "#         random_state: int = 0,\n",
    "#     ):\n",
    "#         self.n_features_to_select = n_features_to_select  # 特徴量数\n",
    "#         self.n_clusters = n_clusters  # クラスタ数\n",
    "#         self.criterion = criterion  # 特徴量選択基準\n",
    "#         self.clustering_method = clustering_method  # クラスタリング手法\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def fss(self, X: pd.DataFrame, y: pd.DataFrame) -> \"Wrapper\":\n",
    "#         X, y = check_X_y(X, y)\n",
    "\n",
    "#         n_features = X.shape[1]  # 総特徴量数\n",
    "#         self.selected_features_ = []  # ここに選択した特徴量を入れる\n",
    "\n",
    "#         # 選ばれた特徴量と残っている特徴量の初期化\n",
    "#         current_features = []\n",
    "#         remaining_features = list(range(n_features))\n",
    "#         best_score = -np.inf\n",
    "\n",
    "#         while len(current_features) < self.n_features_to_select:\n",
    "#             best_feature = None  # 選ぶ特徴量の初期化\n",
    "\n",
    "#             for feature in remaining_features:\n",
    "#                 temp_features = tuple(\n",
    "#                     current_features + [feature]\n",
    "#                 )  # 特徴量をひとつ加え、score計算\n",
    "#                 score = self.crit(X[:, temp_features], temp_features)\n",
    "\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_feature = feature\n",
    "\n",
    "#             if best_feature is not None:\n",
    "#                 current_features.append(\n",
    "#                     best_feature\n",
    "#                 )  # best feature をcurrent features に追加\n",
    "#                 remaining_features.remove(\n",
    "#                     best_feature\n",
    "#                 )  # best feature をremaining features から取り除く\n",
    "#                 self.selected_features_ = current_features\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "#         # 選ばれた特徴量サブセットでクラスタリング\n",
    "#         final_features = X[:, self.selected_features_]\n",
    "#         if self.clustering_method == \"kprototype\":\n",
    "#             catColumnsPos_final = [\n",
    "#                 i for i in self.selected_features_ if i in catColumnsPos\n",
    "#             ]\n",
    "#             if catColumnsPos_final == []:\n",
    "#                 self.final_model_ = KMeans(\n",
    "#                     n_clusters=self.n_clusters,\n",
    "#                     random_state=self.random_state,\n",
    "#                 )\n",
    "#                 self.final_model_.fit(final_features)\n",
    "#                 self.final_cluster_assignments_ = self.final_model_.predict(\n",
    "#                     final_features\n",
    "#                 )\n",
    "#             if sorted(catColumnsPos_final) == sorted(self.selected_features_):\n",
    "#                 self.final_model_ = KModes(\n",
    "#                     n_jobs=-1,\n",
    "#                     n_clusters=self.n_clusters,\n",
    "#                     random_state=self.random_state,\n",
    "#                     init=\"Huang\",\n",
    "#                 )\n",
    "#                 self.final_model_.fit(final_features)\n",
    "#                 self.final_cluster_assignments_ = self.final_model_.predict(\n",
    "#                     final_features\n",
    "#                 )\n",
    "#             elif catColumnsPos_final != []:\n",
    "#                 self.final_model_ = KPrototypes(\n",
    "#                     n_jobs=-1,\n",
    "#                     n_clusters=clusters,\n",
    "#                     init=\"Huang\",\n",
    "#                     random_state=self.random_state,\n",
    "#                 )\n",
    "#                 self.final_model_.fit(final_features, categorical=catColumnsPos_final)\n",
    "#                 self.final_cluster_assignments_ = self.final_model_.predict(\n",
    "#                     final_features\n",
    "#                 )\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def crit(self, X: pd.DataFrame, temp_features: list[int]) -> float:\n",
    "#         if self.clustering_method == \"kprototype\":\n",
    "#             temp_features_list = list(temp_features)\n",
    "#             catColumnsPos_temp = [i for i in temp_features_list if i in catColumnsPos]\n",
    "#             print(catColumnsPos_temp)\n",
    "\n",
    "#             if catColumnsPos_temp == []:\n",
    "#                 kmeans = KMeans(\n",
    "#                     n_clusters=self.n_clusters,\n",
    "#                     random_state=self.random_state,\n",
    "#                 )\n",
    "#                 kmeans.fit(X)\n",
    "#                 labels = kmeans.predict(X)\n",
    "#                 model = kmeans\n",
    "\n",
    "#             if sorted(catColumnsPos_temp) == sorted(temp_features_list):\n",
    "#                 kmodes = KModes(\n",
    "#                     n_jobs=-1,\n",
    "#                     n_clusters=self.n_clusters,\n",
    "#                     random_state=self.random_state,\n",
    "#                     init=\"Huang\",\n",
    "#                 )\n",
    "#                 kmodes.fit(X)\n",
    "#                 labels = kmodes.predict(X)\n",
    "#                 model = kmodes\n",
    "\n",
    "#             elif catColumnsPos_temp != []:\n",
    "#                 kprototype = KPrototypes(\n",
    "#                     n_jobs=-1,\n",
    "#                     n_clusters=clusters,\n",
    "#                     init=\"Huang\",\n",
    "#                     random_state=self.random_state,\n",
    "#                 )\n",
    "#                 kprototype.fit(X, categorical=catColumnsPos_temp)\n",
    "#                 labels = kprototype.predict(X)\n",
    "#                 model = kprototype\n",
    "\n",
    "#             if self.criterion == \"tr\": #############ここをなおす\n",
    "#                 labels = model.labels_\n",
    "#                 cluster_centers = model.cluster_centers_\n",
    "\n",
    "#                 sw_i_list = []\n",
    "#                 for i in range(self.n_clusters):\n",
    "#                     cluster_points = X[labels == i]\n",
    "\n",
    "#                     if cluster_points.shape[0] <= 2:\n",
    "#                         # データポイントが1つの場合はゼロ行列を使用\n",
    "#                         sw_i = np.zeros((X.shape[1], X.shape[1])) + 1e-7\n",
    "#                     else:\n",
    "#                         sw_i = (np.cov(cluster_points, rowvar=False) + 1e-7) * np.sum(\n",
    "#                             labels\n",
    "#                             == i  # データ数を重みに使う代わりにデータの割合を使う\n",
    "#                         )\n",
    "#                         if np.isscalar(sw_i):  # スカラー値のとき\n",
    "#                             sw_i = np.array([[sw_i]])\n",
    "#                     sw_i_list.append(sw_i)\n",
    "\n",
    "#                 # 全クラスターの S_W を合計\n",
    "#                 S_W = np.sum(sw_i_list, axis=0)\n",
    "\n",
    "#                 # クラスター間散布行列 S_B を計算\n",
    "#                 overall_mean = np.mean(X, axis=0)\n",
    "#                 S_B = sum(\n",
    "#                     (np.sum(labels == i) / X.shape[0])  # 割合にする\n",
    "#                     * np.outer(\n",
    "#                         cluster_centers[i] - overall_mean,\n",
    "#                         cluster_centers[i] - overall_mean,\n",
    "#                     )\n",
    "#                     # *(cluster_centers[i] - overall_mean) @ (cluster_centers[i] - overall_mean).T\n",
    "#                     for i in range(self.n_clusters)\n",
    "#                 )\n",
    "\n",
    "#                 # 散乱分離性を計算\n",
    "#                 score = np.trace(np.linalg.solve(S_W, S_B))\n",
    "\n",
    "#             elif self.criterion == \"ml\":\n",
    "#                 score = -model.score(X)\n",
    "\n",
    "#         return score\n",
    "\n",
    "#     def get_feature_index_out(self) -> NDArray:\n",
    "#         return np.array(self.selected_features_)  # 選択された特徴量のインデックス\n",
    "\n",
    "#     def get_final_cluster_assignments(self) -> NDArray:\n",
    "#         return self.final_cluster_assignments_  # 最終的なクラスタリング結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper class でクラスタリングしたとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wrapper法を実装するための関数\n",
    "# def process_wrapper(\n",
    "#     name: str, instance: \"Wrapper\", X_scaled: pd.DataFrame, y: pd.DataFrame\n",
    "# ):\n",
    "#     instance.fss(X_scaled, y)\n",
    "#     selected_features_index = instance.get_feature_index_out()\n",
    "#     cluster_label = instance.get_final_cluster_assignments()\n",
    "#     cluster_size = np.unique(cluster_label, return_counts=True)[1]\n",
    "\n",
    "#     return selected_features_index, cluster_label, cluster_size\n",
    "\n",
    "\n",
    "# # Wrapperインスタンスのリスト\n",
    "# instances = [\n",
    "#     (\n",
    "#         \"fsskprototype_tr\",\n",
    "#         Wrapper(\n",
    "#             n_features_to_select=n_features_to_select,\n",
    "#             n_clusters=clusters,\n",
    "#             criterion=\"tr\",\n",
    "#             clustering_method=\"kprototype\",\n",
    "#             random_state=0,\n",
    "#         ),\n",
    "#     ),\n",
    "#     (\n",
    "#         \"fsskprototype_ml\",\n",
    "#         Wrapper(\n",
    "#             n_features_to_select=n_features_to_select,\n",
    "#             n_clusters=clusters,\n",
    "#             criterion=\"ml\",\n",
    "#             clustering_method=\"kprototype\",\n",
    "#             random_state=0,\n",
    "#         ),\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# # 辞書の初期化\n",
    "# selected_features_index_dict = {}\n",
    "# cluster_label_dict = {}\n",
    "# cluster_size_dict = {}\n",
    "\n",
    "# # 各インスタンスに対して処理を実行\n",
    "# for name, instance in instances:\n",
    "#     selected_features_index, cluster_label, cluster_size = process_wrapper(\n",
    "#         name, instance, X_scaled, y\n",
    "#     )\n",
    "#     selected_features_index_dict[name] = selected_features_index\n",
    "#     cluster_label_dict[name] = cluster_label\n",
    "#     cluster_size_dict[name] = cluster_size\n",
    "#     print(\"[\", name, \"]\")\n",
    "#     print(\"選択された特徴量のインデックス : \", selected_features_index)\n",
    "#     print(\"選択された特徴量の数 : \", len(selected_features_index))\n",
    "#     print(\"各層のクラスタサイズ : \", cluster_size)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KPrototypesでクラスタリングしたとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ kprototype ]\n",
      "各層のクラスタサイズ :  [1341  829]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_label_dict = {}\n",
    "cluster_size_dict = {}\n",
    "\n",
    "kprototype = KPrototypes(n_jobs=-1, n_clusters=clusters, init=\"Huang\", random_state=0)\n",
    "kprototype_cluster = kprototype.fit_predict(X_scaled, categorical=catColumnsPos)\n",
    "\n",
    "kprototype_size = np.bincount(kprototype_cluster, minlength=clusters)\n",
    "\n",
    "cluster_label_dict[\"kprototype\"] = kprototype_cluster\n",
    "cluster_size_dict[\"kprototype\"] = kprototype_size\n",
    "\n",
    "methods = [\"kprototype\"]\n",
    "for name in methods:\n",
    "    print(\"[\", name, \"]\")\n",
    "    print(\"各層のクラスタサイズ : \", cluster_size_dict[name])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "### 標本配分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基底クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAllocation(metaclass=ABCMeta):  # 抽象基底クラス（ABC）\n",
    "    # 初期化クラス（n_samples(標本サイズ), H(クラスタ数)）\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        random_state: int,\n",
    "        criterion: str,\n",
    "        clustering_method: str,\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.H = H\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.clustering_method = clustering_method\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"標本配分を解く\n",
    "\n",
    "        Args:\n",
    "            X (NDArray): データ (N x M)\n",
    "            y (NDArray): 目的変数 (N)\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            NDArray: 各クラスタの標本数 (H, )\n",
    "\n",
    "        Note:\n",
    "            M: 特徴量数\n",
    "            H: クラスタ数\n",
    "        \"\"\"\n",
    "        # 具象クラスがsolveメゾッドを実装しない場合はNotImpleamentedErrorが発生\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        if self.criterion == \"tr\" and self.clustering_method == \"kprototype\":\n",
    "            cluster_label = cluster_label_dict[\"fsskprototype_tr\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskprototype_tr\"]\n",
    "        if self.criterion == \"ml\" and self.clustering_method == \"kprototype\":\n",
    "            cluster_label = cluster_label_dict[\"fsskprototype_ml\"]\n",
    "            cluster_size = cluster_size_dict[\"fsskprototype_ml\"]\n",
    "        if self.criterion == \"none\" and self.clustering_method == \"kprototype\":\n",
    "            cluster_label = cluster_label_dict[\"kprototype\"]\n",
    "            cluster_size = cluster_size_dict[\"kprototype\"]\n",
    "\n",
    "        # インスタンス変数として設定\n",
    "        self.cluster_label = cluster_label\n",
    "        self.N = cluster_size\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 単純無作為抽出のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAllocation(BaseAllocation):\n",
    "    # 抽象メゾッドを具象化\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]\n",
    "\n",
    "    def clustering(self, X: NDArray) -> tuple[NDArray, NDArray]:\n",
    "        # cluster_labelのすべての要素は0（すべてのデータを同じクラスタに属させている）\n",
    "        cluster_label = np.zeros(\n",
    "            X.shape[0]\n",
    "        )  # cluster_label = [0,0,0,,...(要素数：データ数）]\n",
    "        # クラスタサイズ＝データ数\n",
    "        cluster_size = np.array([len(cluster_label)])  # cluster_size=[データ数]\n",
    "        return cluster_label, cluster_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 比例配分のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalAllocation(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"各クラスタ数に比例した標本数で分割する\"\"\"\n",
    "        n: NDArray = np.round(self.N / self.N.sum() * self.n_samples).astype(int)\n",
    "\n",
    "        if n.sum() > self.n_samples:\n",
    "            # nの合計がn_samplesより大きい場合は一番標本数が多いクラスタから削る\n",
    "            n[np.argmax(n)] -= n.sum() - self.n_samples\n",
    "\n",
    "        if n.sum() < self.n_samples:\n",
    "            # nの合計がn_samplesより小さい場合は一番標本数が多いクラスタにたす\n",
    "            n[np.argmax(n)] += -n.sum() + self.n_samples\n",
    "\n",
    "        # for i in range(\n",
    "        #     len(n)\n",
    "        # ):  # nの要素でm_VALUEより小さいものがあれば要素数が最も大きい層から持ってくる\n",
    "        #     if n[i] < m_VALUE:\n",
    "        #         delta = m_VALUE - n[i]\n",
    "        #         n[i] = m_VALUE\n",
    "        #         n[np.argmax(n)] -= delta\n",
    "\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 事後層化のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostStratification(BaseAllocation):\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"ランダムにn_samplesの標本を選択する\"\"\"\n",
    "        n = np.array([self.n_samples])\n",
    "\n",
    "        return n  # （例）n=[標本サイズ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最適標本配分のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAllocation(BaseAllocation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        H: int,\n",
    "        m: NDArray,  # 標本サイズ下限\n",
    "        M: Optional[NDArray] = None,  # 標本サイズ上限 #Optional(Noneである可能性がある)\n",
    "        random_state: int = 0,\n",
    "        criterion: str = \"none\",\n",
    "        clustering_method: str = \"kprototype\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_samples, H, random_state, criterion, clustering_method\n",
    "        )  # 基底クラスBaseAllocation（スーパークラス）の初期化メゾッドを呼び出す\n",
    "        self.m = m  # 各クラスタの最小標本サイズ (H, )\n",
    "        self.M = M  # 各クラスタの最大標本サイズ (H, ), (指定しない場合はクラスタサイズ)\n",
    "\n",
    "    def solve(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        # S:クラスタ毎の目的変数のvarianceを要素とする配列 (H, )\n",
    "        S = np.array([np.var(y[self.cluster_label == h]) for h in range(self.H)])\n",
    "        print(\"self.N\", self.N)\n",
    "        print(\"S\", S)\n",
    "        d = (self.N**2) * S  # (H, )\n",
    "        print(\"d\", d)\n",
    "        n = self._simple_greedy(n=self.m.copy(), d=d)\n",
    "\n",
    "        # 制約チェック\n",
    "        self._check_constraints(n)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _simple_greedy(self, n: NDArray, d: NDArray) -> NDArray:\n",
    "        M = self.M.copy() if self.M is not None else self.N.copy()\n",
    "        I = np.arange(self.H)  # noqa #クラスタのインデックス\n",
    "        while (n.sum() != self.n_samples) and len(I) != 0:\n",
    "            delta = np.zeros(self.H)\n",
    "            delta[I] = (d / (n + 1) - d / n)[I]\n",
    "            h_star = np.argmin(delta[I])\n",
    "            h_star = I[h_star]\n",
    "\n",
    "            if n[h_star] + 1 <= M[h_star]:\n",
    "                n[h_star] = n[h_star] + 1\n",
    "            else:\n",
    "                # Iの要素h_starを削除\n",
    "                I_ = I.tolist()\n",
    "                I_ = [i for i in I_ if i != h_star]\n",
    "                I = np.array(I_)  # noqa\n",
    "\n",
    "        return n\n",
    "\n",
    "    def _check_constraints(self, n: NDArray):\n",
    "        assert (\n",
    "            n.sum() <= self.n_samples\n",
    "        ), f\"Total sample size is over than {self.n_samples}\"\n",
    "        assert np.all(n >= self.m), \"Minimum sample size constraint is not satisfied\"\n",
    "        if self.M is not None:\n",
    "            assert np.all(\n",
    "                n <= self.M\n",
    "            ), \"Maximum sample size constraint is not satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 母平均の推定値を計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_y_mean(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    \"\"\"実際にサンプリングを行って目的変数の平均を推定\n",
    "\n",
    "    Args:\n",
    "        n (NDArray): 各クラスタの標本数 (H, )\n",
    "        cluster_label (NDArray): クラスタラベル (N, )\n",
    "        y (NDArray): 目的変数 (N, )\n",
    "\n",
    "    Returns:\n",
    "        NDArray: 推定された目的変数の平均\n",
    "\n",
    "    Note:\n",
    "        N: データ数\n",
    "        H: クラスタ数\n",
    "    \"\"\"\n",
    "    # cluster_labelからユニークなクラスタラベルを取得し、母集団の各クラスタのサイズNを取得\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]  # クラスタサイズ (H, )\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    for h in range(n.shape[0]):  # n.shape[0]:層の数\n",
    "        y_cluster = y[cluster_label == h]\n",
    "        # クラスタ内でランダム n_h サンプリング\n",
    "        sample: NDArray = np.random.choice(y_cluster, n[h], replace=False)\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def estimate_y_mean_post(n: NDArray, cluster_label: NDArray, y: NDArray) -> NDArray:\n",
    "    N = np.unique(cluster_label, return_counts=True)[1]\n",
    "    weights = N / N.sum()\n",
    "    y_hat = 0\n",
    "    indices = np.arange(N.sum())\n",
    "    y_array = np.array(y.tolist())\n",
    "    n_indices = np.random.choice(indices, n[0], replace=False)\n",
    "    n_label = np.array([cluster_label[i] for i in n_indices])\n",
    "    n_new = np.unique(n_label)\n",
    "    # n_new = np.unique(n_label, return_counts=True)[1]\n",
    "    # for h in range(n_new.shape[0]):\n",
    "    for h in n_new:\n",
    "        index = np.where(n_label == h)[0]\n",
    "        sample = y_array[n_indices[index]]\n",
    "        y_sample_mean = sample.mean()  # サンプリングした標本の平均\n",
    "        y_hat += y_sample_mean * weights[h]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各ポリシーの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ポリシーの生成を行う関数\n",
    "def create_policies(\n",
    "    criterion_list: list[str],\n",
    "    clustering_method_list: list[str],\n",
    "    n_samples: int,\n",
    "    H: int,\n",
    "    random_state: int,\n",
    "    m_value: int,\n",
    ") -> dict[list[str] : list[BaseAllocation]]:\n",
    "    policies_dict = {}\n",
    "    for criterion in criterion_list:\n",
    "        for clustering_method in clustering_method_list:\n",
    "            policies: list[BaseAllocation] = [\n",
    "                RandomAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                ProportionalAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                PostStratification(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "                OptimalAllocation(\n",
    "                    n_samples=n_samples,\n",
    "                    H=H,\n",
    "                    random_state=random_state,\n",
    "                    m=np.full(H, m_value),\n",
    "                    M=None,\n",
    "                    criterion=criterion,\n",
    "                    clustering_method=clustering_method,\n",
    "                ),\n",
    "            ]\n",
    "            policies_dict[(criterion, clustering_method)] = policies\n",
    "    return policies_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### 母平均の推定と分散の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 母平均の推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.N [1341  829]\n",
      "S [32.41994805 26.59850681]\n",
      "d [58300176.6  18279584.42]\n"
     ]
    }
   ],
   "source": [
    "policies_dict = create_policies(\n",
    "    CRITERION_LIST, CLUSTERING_METHOD_LIST, N_SAMPLES, H, RANDOM_STATE, m_VALUE\n",
    ")\n",
    "allocations_dict = {}\n",
    "for method, policies in policies_dict.items():\n",
    "    # それぞれの戦略で各クラスタの標本数を求解\n",
    "    allocations: list[dict] = []  # 各戦略の実行結果が辞書形式で追加される\n",
    "    for policy in policies:\n",
    "        # policyを用いてXをクラスタリング\n",
    "        cluster_label, _ = policy.clustering(X_scaled)\n",
    "        n = policy.solve(X_scaled, y)\n",
    "        allocations.append(\n",
    "            {\n",
    "                \"policy\": policy.__class__.__name__,\n",
    "                \"n\": n,\n",
    "                \"cluster_label\": cluster_label,\n",
    "            }\n",
    "        )\n",
    "    allocations_dict[method] = allocations\n",
    "\n",
    "# 各戦略の標本数に基づいて目的変数の平均を推定\n",
    "y_hats_dict = {}\n",
    "for method, allocations in allocations_dict.items():\n",
    "    y_hats = []\n",
    "    for random_state in range(N_TRIALS):\n",
    "        for allocation in allocations:\n",
    "            if allocation[\"policy\"] == \"PostStratification\":\n",
    "                y_hat = estimate_y_mean_post(\n",
    "                    allocation[\"n\"], allocation[\"cluster_label\"], y\n",
    "                )\n",
    "            else:\n",
    "                y_hat = estimate_y_mean(allocation[\"n\"], allocation[\"cluster_label\"], y)\n",
    "            y_hats.append(\n",
    "                {\n",
    "                    \"policy\": allocation[\"policy\"],\n",
    "                    \"y_hat\": y_hat,\n",
    "                    \"random_state\": random_state,\n",
    "                }\n",
    "            )\n",
    "    y_hats_dict[method] = y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(clustering_method: kprototype , criterion: none )\n",
      "[各手法の誤差分散削減率]\n",
      "policy\n",
      "RandomAllocation               NaN\n",
      "ProportionalAllocation    7.602240\n",
      "PostStratification        3.045889\n",
      "OptimalAllocation         3.118414\n",
      "Name: error, dtype: float64\n",
      "[各手法の分散]\n",
      "RandomAllocation 0.3012840224224224\n",
      "ProportionalAllocation 0.2783796871452557\n",
      "PostStratification 0.2921072449505667\n",
      "OptimalAllocation 0.2918887386988166\n"
     ]
    }
   ],
   "source": [
    "for method, y_hats in y_hats_dict.items():\n",
    "    y_hat_df = pd.DataFrame(y_hats)\n",
    "    y_hat_df[\"error\"] = (\n",
    "        y_hat_df[\"y_hat\"] - y.mean()\n",
    "    )  # 真の平均からの誤差をerrorカラムに追加\n",
    "\n",
    "    # random_allocationの誤差分散\n",
    "    random_allocation_std = y_hat_df[y_hat_df[\"policy\"] == \"RandomAllocation\"][\n",
    "        \"error\"\n",
    "    ].var()\n",
    "    # random_allocation以外の誤差分散\n",
    "    non_random_allocation_std = (\n",
    "        y_hat_df[y_hat_df[\"policy\"] != \"RandomAllocation\"]\n",
    "        .groupby(\"policy\")[\"error\"]\n",
    "        .var()\n",
    "    )\n",
    "\n",
    "    # 削減率\n",
    "    reduction_rate = (1 - non_random_allocation_std / random_allocation_std) * 100\n",
    "\n",
    "    ## policyの順番をpoliciesの順番に調整\n",
    "    reduction_rate = reduction_rate.reindex(\n",
    "        [policy.__class__.__name__ for policy in policies]\n",
    "    )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"(clustering_method:\", method[1], \", criterion:\", method[0], \")\")\n",
    "    print(\"[各手法の誤差分散削減率]\")\n",
    "    print(reduction_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var\n",
      "Random Allocation: 0.2896795665575576\n",
      "PropotionalAllocation: 0.2945425108992866\n",
      "PostStratification: 0.2747263459798568\n",
      "OptimalAllocation: 0.2656059486261859\n"
     ]
    }
   ],
   "source": [
    "RandomAllocation_df = y_hat_df[y_hat_df[\"policy\"] == \"RandomAllocation\"]\n",
    "var_for_RandomAllocation = RandomAllocation_df[\"error\"].var()\n",
    "ProportionalAllocation_df = y_hat_df[y_hat_df[\"policy\"] == \"ProportionalAllocation\"]\n",
    "var_for_ProportionalAllocation = ProportionalAllocation_df[\"error\"].var()\n",
    "PostStratification_df = y_hat_df[y_hat_df[\"policy\"] == \"PostStratification\"]\n",
    "var_for_PostStratification = PostStratification_df[\"error\"].var()\n",
    "OptimalAllocation_df = y_hat_df[y_hat_df[\"policy\"] == \"OptimalAllocation\"]\n",
    "var_for_OptimalAllocation = OptimalAllocation_df[\"error\"].var()\n",
    "\n",
    "print(\"Var\")\n",
    "print(\"Random Allocation:\", var_for_RandomAllocation)\n",
    "print(\"PropotionalAllocation:\", var_for_ProportionalAllocation)\n",
    "print(\"PostStratification:\", var_for_PostStratification)\n",
    "print(\"OptimalAllocation:\", var_for_OptimalAllocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
